[{"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397822600007 ISSN: 0269-9648 eISSN: 1469-8951","Keywords":"stochastic dynamic programming KeyWords Plus:SELECTION","Categories":"Engineering; Operations Research & Management Science; Mathematics Web of Science Categories:Engineering, Industrial; Operations Research & Management Science; Statistics & Probability","Journal Information":"PROBABILITY IN THE ENGINEERING AND INFORMATIONAL SCIENCES Volume: 31 Issue: 2 Pages: 239-263 DOI: 10.1017/S0269964816000279 Published: APR 2017","Abstract":"The knowledge gradient (KG) policy was originally proposed for online ranking and selection problems but has recently been adapted for use in online decision-making in general and multi-armed bandit problems (MABs) in particular. We study its use in a class of exponential family MABs and identify weaknesses, including a propensity to take actions which are dominated with respect to both exploitation and exploration. We propose variants of KG which avoid such errors. These new policies include an index heuristic, which deploys a KG approach to develop an approximation to the Gittins index. A numerical study shows this policy to perform well over a range of MABs including those for which index policies are not optimal. While KG does not take dominated actions when bandits are Gaussian, it fails to be index consistent and appears not to enjoy a performance advantage over competitor policies when arms are correlated to compensate for its greater computational demands.","Authors":"Edwards, J (Edwards, James) ; Fearnhead, P (Fearnhead, Paul) ; Glazebrook, K (Glazebrook, Kevin) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Glazebrook, Kevin  http://orcid.org/0000-0002-5045-0718","Title":"ON THE IDENTIFICATION AND MITIGATION OF WEAKNESSES IN THE KNOWLEDGE GRADIENT POLICY FOR MULTI-ARMED BANDITS"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393243200014 ISSN: 1570-8705 eISSN: 1570-8713","Keywords":"Network coding; Cognitive radio networks; Multicast; Multi-armed bandits KeyWords Plus:SPECTRUM ACCESS; CONTROL CHANNEL; MAC","Categories":"Computer Science; Telecommunications Web of Science Categories:Computer Science, Information Systems; Telecommunications","Journal Information":"AD HOC NETWORKS Volume: 56 Pages: 186-201 DOI: 10.1016/j.adhoc.2016.12.009 Published: MAR 1 2017","Abstract":"In cognitive radio networks (CRNs), secondary users (SUs) may employ network coding to pursue higher throughput. However, as SUs must vacate the spectrum when it is accessed by primary users (PUS), the available transmission time of SUs is usually uncertain, i.e., SUs do not know how long the idle state can last. Meanwhile, existing network coding strategies generally adopt a block-based transmission scheme, which means that all packets in the same block can be decoded simultaneously only when enough coded packets are collected. Therefore, the gain brought by network coding may be dramatically decreased as the packet collection process may be interrupted due to the unexpected arrivals of PUs. In this paper, for the first time, we develop an efficient network coding strategy for SUs while considering the uncertain idle durations in CRNs. At its heart is that systematic network coding (SNC) is employed to opportunistically utilize the idle duration left by PUs. To handle the uncertainty of idle durations, we utilize confidential interval estimation to estimate the expected duration for stochastic idle durations, and multi-armed bandits to determine the duration sequentially for non-stochastic idle durations, respectively. Then, we propose a coding parameter selection algorithm for SNC by considering the complicated correlation among the receptions at different receivers. Simulation results show that, our proposed schemes outperform both traditional optimal block-based network coding and non-network coding schemes, and achieve competitive performance compared with the scheme with perfect idle duration information. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Qu, YB (Qu, Yuben) ; Dong, C (Dong, Chao) ; Tang, SJ (Tang, Shaojie) ; Chen, C (Chen, Chen) ; Dai, HP (Dai, Haipeng) ; Wang, H (Wang, Hai) ; Tian, C (Tian, Chang)","Title":"Opportunistic network coding for secondary users in cognitive radio networks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000391296100006 ISSN: 1053-587X eISSN: 1941-0476","Keywords":"Ensemble learning; meta-learning; online learning; regret; confidence bound; multi-armed bandits; contextual bandits; medical informatics KeyWords Plus:MULTIAGENT OPTIMIZATION; ALGORITHMS; CONSENSUS; REGRESSION; NETWORKS; EXPERT","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON SIGNAL PROCESSING Volume: 65 Issue: 4 Pages: 888-903 DOI: 10.1109/TSP.2016.2626250 Published: FEB 15 2017","Abstract":"Extracting actionable intelligence from distributed, heterogeneous, correlated, and high-dimensional data sources requires run-time processing and learning both locally and globally. In the last decade, a large number of meta-learning techniques have been proposed in which local learners make online predictions based on their locally collected data instances, and feed these predictions to an ensemble learner, which fuses them and issues a global prediction. However, most of these works do not provide performance guarantees or, when they do, these guarantees are asymptotic. None of these existing works provide confidence estimates about the issued predictions or rate of learning guarantees for the ensemble learner. In this paper, we provide a systematic ensemble learning method called Hedged Bandits, which comes with both long-run (asymptotic) and short-run (rate of learning) performance guarantees. Moreover, our approach yields performance guarantees with respect to the optimal local prediction strategy, and is also able to adapt its predictions in a data-driven manner. We illustrate the performance of Hedged Bandits in the context of medical informatics and show that it outperforms numerous online and offline ensemble learning methods.","Authors":"Tekin, C (Tekin, Cem) ; Yoon, J (Yoon, Jinsung) ; van der Schaar, M (van der Schaar, Mihaela)","Title":"Adaptive Ensemble Learning With Confidence Bounds"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395736700041 ISSN: 1089-7798 eISSN: 1558-2558","Keywords":"Contextual bandit; underwater cooperative communication; relay selection","Categories":"Telecommunications Web of Science Categories:Telecommunications","Journal Information":"IEEE COMMUNICATIONS LETTERS Volume: 21 Issue: 2 Pages: 382-385 DOI: 10.1109/LCOMM.2016.2625300 Published: FEB 2017","Abstract":"Cooperative relay transmission is an attractive architecture for underwater acoustic networks. However, designing relay selection policies in the harsh underwater environment is difficult. In this letter, we model relay selection as a contextual bandit problem-an important extension of multi-armed bandit. Through this way, we can achieve relay selection based on a bit of contextual communication environment information about relay nodes instead of instantaneous or statistical channel state information. Our proposed relay selection technique enables highly stable performance of the cooperative system in a complex and changeable underwater environment, and the process of relay selection can be simplified and easily facilitate efficient cooperative transmission. Simulation results illustrate the effectiveness and the robustness of this relay selection technique.","Authors":"Li, XB (Li, Xinbin) ; Liu, JJ (Liu, Jiajia) ; Yan, L (Yan, Lei) ; Han, S (Han, Song) ; Guan, XP (Guan, Xinping)","Title":"Relay Selection in Underwater Acoustic Cooperative Networks: A Contextual Bandit Approach"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396914700001 ISSN: 1229-2370 eISSN: 1976-5541","Keywords":"Cooperative communication; multi-armed bandit; power line communication (PLC); relay selection","Categories":"Computer Science; Telecommunications Web of Science Categories:Computer Science, Information Systems; Telecommunications","Journal Information":"JOURNAL OF COMMUNICATIONS AND NETWORKS Volume: 19 Issue: 1 Pages: 1-9 DOI: 10.1109/JCN.2017.000003 Published: FEB 2017","Abstract":"Power line communication (PLC) exploits the existence of installed infrastructure of power delivery system, in order to transmit data over power lines. In PLC networks, different nodes of the network are interconnected via power delivery transmission lines, and the data signal is flowing between them. However, the attenuation and the harsh environment of the power line communication channels, makes it difficult to establish a reliable communication between two nodes of the network which are separated by a long distance. Relaying and cooperative communication has been used to overcome this problem. In this paper a two-hop cooperative PLC has been studied, where the data is communicated between a transmitter and a receiver node, through a single array node which has to be selected from a set of available arrays. The relay selection problem can be solved by having channel state information (CSI) at transmitter and selecting the relay which results in the best performance. However, acquiring the channel state information at transmitter increases the complexity of the communication system and introduces undesired overhead to the system. We propose a class of machine learning schemes, namely multi-armed bandit (MAB), to solve the relay selection problem without the knowledge of the channel at the transmitter. Furthermore, we develop a new MAB algorithm which exploits the periodicity of the synchronous impulsive noise of the PLC channel, in order to improve the relay selection algorithm.","Authors":"Nikfar, B (Nikfar, Babak) ; Vinck, AJH (Vinck, A. J. Han)","Title":"Relay Selection in Cooperative Power Line Communication: A Multi-Armed Bandit Approach"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394288900003 ISSN: 0005-1179 eISSN: 1608-3032","Keywords":"online optimization; gradient-free; inexact oracle; stochastic optimization KeyWords Plus:ORACLE","Categories":"Automation & Control Systems; Instruments & Instrumentation Web of Science Categories:Automation & Control Systems; Instruments & Instrumentation","Journal Information":"AUTOMATION AND REMOTE CONTROL Volume: 78 Issue: 2 Pages: 224-234 DOI: 10.1134/S0005117917020035 Published: FEB 2017","Abstract":"In this paper the gradient-free modification of the mirror descent method for convex stochastic online optimization problems is proposed. The crucial assumption in the problem setting is that function realizations are observed with minor noises. The aim of this paper is to derive the convergence rate of the proposed methods and to determine a noise level which does not significantly affect the convergence rate.","Authors":"Gasnikov, AV (Gasnikov, A. V.) ; Krymova, EA (Krymova, E. A.) ; Lagunovskaya, AA (Lagunovskaya, A. A.) ; Usmanova, IN (Usmanova, I. N.) ; Fedorenko, FA (Fedorenko, F. A.) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Lagunovskaya, Anastasia  E-1837-2016  ","Title":"Stochastic online optimization. Single-point and multi-point non-linear multi-armed bandits. Convex and strongly-convex case"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395825300016 ISSN: 0090-6778 eISSN: 1558-0857","Keywords":"Restless bandit; myopic policy; optimality; stochastic order; multivariate analysis KeyWords Plus:RESTLESS BANDITS","Categories":"Engineering; Telecommunications Web of Science Categories:Engineering, Electrical & Electronic; Telecommunications","Journal Information":"IEEE TRANSACTIONS ON COMMUNICATIONS Volume: 65 Issue: 2 Pages: 677-690 DOI: 10.1109/TCOMM.2016.2628899 Published: FEB 2017","Abstract":"We consider the channel access problem arising in opportunistic scheduling over fading channels, cognitive radio networks, and server scheduling. The multi-channel communication system consists of N channels. Each channel evolves as a time-nonhomogeneous multi-state Markov process. At each time instant, a user chooses M channels to transmit information, and obtains some reward, i.e., throughput, based on the states of the chosen channels. The objective is to design an access policy, i.e., which channels should be accessed at each time instant, such that the expected accumulated discounted reward is maximised over a finite or infinite horizon. The considered problem can be cast into a restless multi-armed bandit (RMAB) problem, which is PSPACE-hard, with the optimal policy usually intractable due to the exponential computation complexity. Hence, a natural alternative is to consider the easily implementable myopic policy that only maximises the immediate reward but ignores the impact of the current strategy on the future reward. In this paper, we perform an analytical study on the performance of the myopic policy for the considered RMAB problem, and establish a set of closed-form conditions to guarantee the optimality of the myopic policy.","Authors":"Wang, KH (Wang, Kehao) ; Chen, L (Chen, Lin) ; Yu, JH (Yu, Jihong) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number CHEN, Lin  http://orcid.org/0000-0001-7943-3172","Title":"On Optimality of Myopic Policy in Multi-Channel Opportunistic Access"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000389350600002 ISSN: 1556-6013 eISSN: 1556-6021","Keywords":"Private data collecting; data anonymization; bandit problems; learning policy; dynamic pricing","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY Volume: 12 Issue: 2 Pages: 271-285 DOI: 10.1109/TIFS.2016.2611487 Published: FEB 2017","Abstract":"Recently, the conflict between exploiting the value of personal data and protecting individuals' privacy has attracted much attention. Personal data market provides a promising solution to this conflict, while determining the price of privacy is a tough issue. In this paper, we study the pricing problem in a setting where a data collector sequentially buys data from multiple data owners whose valuations of privacy are randomly drawn from an unknown distribution. To maximize the total payoff, the collector needs to dynamically adjust the prices offered to owners. We model the sequential decision-making problem of the collector as a multi-armed bandit problem with each arm representing a candidate price. Specifically, the privacy protection technique adopted by the collector is taken into account. Protecting privacy generally causes a negative effect on the value of data, and this effect is embodied by the time-variant distributions of the rewards associated with arms. Based on the classic upper confidence bound policy, we propose two learning policies for the bandit problem. The first policy estimates the expected reward of a price by counting how many times the price has been accepted by data owners. The second policy treats the time-variant data value as a context and uses ridge regression to estimate the rewards in different contexts. Simulation results on real-world data demonstrate that by applying the proposed policies, the collector can get a payoff which is close to that he can get by setting a fixed price, which is the best in hindsight, for all data owners.","Authors":"Xu, L (Xu, Lei) ; Jiang, CX (Jiang, Chunxiao) ; Qian, Y (Qian, Yi) ; Zhao, YJ (Zhao, Youjian) ; Li, JH (Li, Jianhua) ; Ren, Y (Ren, Yong)","Title":"Dynamic Privacy Pricing: A Multi-Armed Bandit Approach With Time-Variant Rewards"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000398625800003 ISSN: 1076-9757 eISSN: 1943-5037","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH Volume: 58 Pages: 665-702 Published: 2017","Abstract":"Games with large branching factors pose a significant challenge for game tree search algorithms. In this paper, we address this problem with a sampling strategy for Monte Carlo Tree Search (MCTS) algorithms called naive sampling, based on a variant of the Multi-armed Bandit problem called Combinatorial Multi-armed Bandits (CMAB). We analyze the theoretical properties of several variants of naive sampling, and empirically compare it against the other existing strategies in the literature for CMABs. We then evaluate these strategies in the context of real-time strategy (RTS) games, a genre of computer games characterized by their very large branching factors. Our results show that as the branching factor grows, naive sampling outperforms the other sampling strategies.","Authors":"Ontanon, S (Ontanon, Santiago)","Title":"Combinatorial Multi-armed Bandits for Real-Time Strategy Games"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000398020500045 ISSN: 1877-0509","Keywords":"maintenance decision support system; pattern classification; statistical process control; contextual multi-armed bandit","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Theory & Methods","Journal Information":"XII INTERNATIONAL SYMPOSIUM INTELLIGENT SYSTEMS 2016, (INTELS 2016) Book Series: Procedia Computer Science Volume: 103 Pages: 316-323 DOI: 10.1016/j.procs.2017.01.114 Published: 2017","Abstract":"In this paper we focus on two essential problems of maintenance decision support systems, namely, 1) detection of potential dangerous situation, and 2) classification of this situation in order to recommend an appropriate repair action. The former task is usually solved with the known statistical process control techniques. The latter problem can be reduced to the contextual multi armed bandit problem. We propose a novel algorithm with Bayesian classification of abnormal situation and the softmax rule to explore the decision space. The dangerous situations are detected with the Shewhart control charts for the distances between the current and the normal situations. It is experimentally shown, that our algorithm is more accurate than the known contextual multi-armed methods with stochastic search strategies. (C) 2017 The Authors. Published by Elsevier B. V.","Authors":"Savchenko, AV (Savchenko, A. V.) ; Milov, VR (Milov, V. R.) Edited by:Askhat, D; Ivan, Z; Andrew, K; Eugeny, N","Title":"Decision support in intelligent maintenance-planning systems based on contextual multi-armed bandit algorithm"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397809900069 ISSN: 2169-3536","Keywords":"Ultra-dense small cell networks; energy harvesting; distributed control; user association; multi-armed bandits KeyWords Plus:SYSTEMS","Categories":"Computer Science; Engineering; Telecommunications Web of Science Categories:Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications","Journal Information":"IEEE ACCESS Volume: 5 Pages: 3513-3523 DOI: 10.1109/ACCESS.2017.2676166 Published: 2017","Abstract":"The emerging ultra-dense small cell networks (UD-SCNs) will need to combat a variety of challenges. On the one hand, massive number of devices sharing the limited wireless resources renders centralized control mechanisms infeasible due to the excessive cost of information acquisition and computation. On the other hand, to reduce the energy consumption from fixed power grid and/or battery, network entities (e.g., small cell base stations and user devices) may need to rely on the energy harvested from the ambient environment (e.g., from environmental sources). However, opportunistic energy harvesting introduces uncertainty in the network operation. In this paper, we study the distributed user association problem for energy harvesting UD-SCNs. After reviewing the state-of-the-art research, we outline the major challenges that arise in the presence of energy harvesting due to the uncertainty (e.g., limited knowledge on energy harvesting process or channel profile) as well as limited computational capacities. Finally, we propose an approach based on the mean-field multi-armed bandit games to solve the uplink user association problem for energy harvesting devices in a UD-SCN in the presence of uncertainty.","Authors":"Maghsudi, S (Maghsudi, Setareh) ; Hossain, E (Hossain, Ekram)","Title":"Distributed User Association in Energy Harvesting Dense Small Cell Networks: A Mean-Field Multi-Armed Bandit Approach"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396850900005 ISSN: 1744-5760 eISSN: 1744-5779","Keywords":"Monte Carlo tree search; multi-armed bandit problems; the loosely symmetric model; cognitively inspired computing; causal intuition; satisficing KeyWords Plus:MONTE-CARLO; MODEL","Categories":"Computer Science Web of Science Categories:Computer Science, Theory & Methods","Journal Information":"INTERNATIONAL JOURNAL OF PARALLEL EMERGENT AND DISTRIBUTED SYSTEMS Volume: 32 Issue: 2 Pages: 206-217 Special Issue: SI DOI: 10.1080/17445760.2015.1064918 Published: 2017","Abstract":"Classical search methods on game trees are based on a static evaluation function (that enable quantitative valuation of game states) and a decision strategy (such as the minimax method). These search methods are not always effective in some games such as the game of Go, as construction of the evaluation function is very hard and the search space is extremely huge. Recently, Monte Carlo tree search methods (especially the UCT algorithms) that enable efficient sampling of actions have been shown to be very effective. Here, we propose the loosely symmetric (LS) model applied to trees (LST), which utilises an action value function (LS model) that implements causal intuition of humans. By tuning a single intuitive parameter, LST enables fast search of the optimal action with its efficient satisficing behaviour. The satisficing search realised by LST enables pruning and exhibits intermediate properties between those of breadth-first and depth-first search strategies.","Authors":"Oyo, K (Oyo, Kuratomo) ; Takahashi, T (Takahashi, Tatsuji)","Title":"Efficacy of a causal value function in game tree search"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396850900007 ISSN: 1744-5760 eISSN: 1744-5779","Keywords":"Bounded rationality; bounded optimality; computational rationality; UCB1; multi-armed bandit problems; reinforcement learning","Categories":"Computer Science Web of Science Categories:Computer Science, Theory & Methods","Journal Information":"INTERNATIONAL JOURNAL OF PARALLEL EMERGENT AND DISTRIBUTED SYSTEMS Volume: 32 Issue: 2 Pages: 232-242 Special Issue: SI DOI: 10.1080/17445760.2015.1075531 Published: 2017","Abstract":"When learning and test phases are not separated, there is a trade-off between speed and accuracy. This is a universal problem for agents acting under uncertainty. To address this trade-off, we employ a strategy called satisficing, which looks for actions that are satisfactory with respect to a given reference level. In this study, we introduce a satisficing value function, the loosely symmetric model with variable reference (LSVR) which is an extension of the loosely symmetric model inspired by some causal and perceptual human properties. We tested the performance of the LSVR in Karmed bandit problems that deal with the trade-off in the simplest possible way. Our results show that the LSVR enables effective online optimisation through satisficing.","Authors":"Kohno, Y (Kohno, Yu) ; Takahashi, T (Takahashi, Tatsuji)","Title":"A cognitive satisficing strategy for bandit problems"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000389784400003 ISSN: 1051-2004 eISSN: 1095-4333","Keywords":"Decision making policy; Multi-armed bandit algorithms; Radio frequency energy harvesting; Thompson sampling KeyWords Plus:MULTIARMED BANDIT; ACCESS; DESIGN","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"DIGITAL SIGNAL PROCESSING Volume: 60 Pages: 33-45 DOI: 10.1016/j.dsp.2016.08.014 Published: JAN 2017","Abstract":"Cognitive radio (CR) paradigm with radio frequency energy harvesting (RFEH) have significant potential to improve the network throughput by utilizing vacant spectrum using battery-operated self-sustainable radio terminals. Research efforts relevant to these paradigms are focused on the mode selection policies which decide when to switch from CR mode (i.e., opportunistic vacant spectrum access mode) to the RFEH mode (i.e., battery charging using ambient RF energy) and vice-versa. So far, very little attention has been paid to the dual but competing task of frequency band selection in CR and RFEH modes under partially observable environment in the decentralized wireless networks. Furthermore, the need of tunable bandwidth frequency band access for CRs and lower subband switching cost (SSC) for energy efficient implementation have made the design of the decision making policy (DMP) more challenging. In this paper, a new CR-RFEH DMP has been proposed for RFEH enabled CR terminals in the decentralized wireless networks. The proposed DMP consists of three sub-units: 1) Bayesian approach based tunable Thompson sampling algorithm for subband statistics estimation, 2) Thompson sampling algorithm based subband access scheme exploiting the past collision events to minimize collisions among CRs, and 3) Mode selection scheme. Simulation results show that the proposed DMP offers 10-35% improvement in the throughput of the decentralized network and 40-90% reduction in the number of subband switchings compared to existing DMPs. The simulation results are then validated using real radio signals on the proposed USRP testbed. (C) 2016 Elsevier Inc. All rights reserved.","Authors":"Darak, SJ (Darak, Sumit J.) ; Zhang, HG (Zhang, Honggang) ; Palicot, J (Palicot, Jacques) ; Moy, C (Moy, Christophe)","Title":"Decision making policy for RF energy harvesting enabled cognitive radios in decentralized wireless networks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392285600088 ISSN: 1568-4946 eISSN: 1872-9681","Keywords":"Software Product Line; Software testing; Hyper-heuristic KeyWords Plus:MULTIOBJECTIVE GENETIC ALGORITHM; OF-THE-ART; EVOLUTIONARY ALGORITHMS; OPTIMIZATION","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications","Journal Information":"APPLIED SOFT COMPUTING Volume: 49 Pages: 1232-1242 DOI: 10.1016/j.asoc.2016.07.059 Published: DEC 2016","Abstract":"Deriving products from a Feature Model (FM) for testing Software Product Lines (SPLs) is a hard task. It is important to select a minimum number of products but, at the same time, to consider the coverage of testing criteria such as pairwise, among other factors. To solve such problems Multi-Objective Evolutionary Algorithms (MOEAs) have been successfully applied. However, to design a solution for this and other software engineering problems can be very difficult, because it is necessary to choose among different search operators and parameters. Hyper-heuristics can help in this task, and have raised interest in the Search-Based Software Engineering (SBSE) field. Considering the growing adoption of SPL in the industry and crescent demand for SPL testing approaches, this paper introduces a hyper-heuristic approach to automatically derive products to variability testing of SPLs. The approach works with MOEAs and two selection methods, random and based on FRR-MAB (Fitness Rate Rank based Multi-Armed Bandit). It was evaluated with real FMs and the results show that the proposed approach outperforms the traditional algorithms used in the literature, and that both selection methods present similar performance. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Strickler, A (Strickler, Anderi) ; Lima, JAP (Prado Lima, Jackson A.) ; Vergilio, SR (Vergilio, Silvia R.) ; Pozo, ATR (Pozo, Aurora T. R.) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Prado Lima, Jackson  L-8938-2016   A. Prado Lima, Jackson  http://orcid.org/0000-0003-4993-777X","Title":"Deriving products for variability test of Feature Models with a hyper-heuristic approach"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000391727900043 ISSN: 1063-6692 eISSN: 1558-2566","Keywords":"Whittle's index; restless bandits; birth-and-death processes; fluid approximation; Lagrangian relaxation; index policies KeyWords Plus:INDEX POLICIES; QUEUE; ABANDONMENT; PERFORMANCE; NETWORKS; SYSTEMS; COSTS; RULE","Categories":"Computer Science; Engineering; Telecommunications Web of Science Categories:Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic; Telecommunications","Journal Information":"IEEE-ACM TRANSACTIONS ON NETWORKING Volume: 24 Issue: 6 Pages: 3812-3825 DOI: 10.1109/TNET.2016.2562564 Published: DEC 2016","Abstract":"We develop a unifying framework to obtain efficient index policies for restless multi-armed bandit problems with birth-and-death state evolution. This is a broad class of stochastic resource allocation problems whose objective is to determine efficient policies to share resources among competing projects. In a seminal work, Whittle developed a methodology to derive well-performing (Whittle's) index policies that are obtained by solving a relaxed version of the original problem. Our first main contribution is the derivation of a closed-form expression for Whittle's index as a function of the steady-state probabilities. In some particular cases, qualitative insights can be obtained from its expression; nevertheless, it requires several technical conditions to be verified. We, therefore, formulate a fluid version of the relaxed optimization problem, and in our second main contribution, we develop a fluid index policy. The latter does provide qualitative insights and it is equivalent to Whittle's index policy in the light-traffic regime. The applicability of our approach is illustrated by two important problems: optimal class selection and optimal load balancing. Allowing state-dependent capacities, we can model important phenomena, e.g., power-aware server-farms and opportunistic scheduling in wireless systems. Whittle's index and our fluid index policy show remarkably good performance in numerical simulations.","Authors":"Larranaga, M (Larranaga, Maialen) ; Ayesta, U (Ayesta, Urtzi) ; Verloop, IM (Verloop, Ina Maria)","Title":"Dynamic Control of Birth-and-Death Restless Bandits: Application to Resource-Allocation Problems"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000388498600007 ISSN: 0018-9340 eISSN: 1557-9956","Keywords":"Scheduling; machine learning; many-core platforms; data mining; big-data; multiple streams processing; concept drift KeyWords Plus:MINING SYSTEMS; OPTIMIZATION; CLASSIFIERS","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Hardware & Architecture; Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON COMPUTERS Volume: 65 Issue: 12 Pages: 3591-3605 DOI: 10.1109/TC.2016.2550454 Published: DEC 1 2016","Abstract":"Several techniques have been recently proposed to adapt Big-Data streaming applications to existing many core platforms. Among these techniques, online reinforcement learning methods have been proposed that learn how to adapt at run-time the throughput and resources allocated to the various streaming tasks depending on dynamically changing data stream characteristics and the desired applications performance (e.g., accuracy). However, most of state-of-the-art techniques consider only one single stream input in its application model input and assume that the system knows the amount of resources to allocate to each task to achieve a desired performance. To address these limitations, in this paper we propose a new systematic and efficient methodology and associated algorithms for online learning and energy-efficient scheduling of Big-Data streaming applications with multiple streams on many core systems with resource constraints. We formalize the problem of multi-stream scheduling as a staged decision problem in which the performance obtained for various resource allocations is unknown. The proposed scheduling methodology uses a novel class of online adaptive learning techniques which we refer to as staged multi-armed bandits (S-MAB). Our scheduler is able to learn online which processing method to assign to each stream and how to allocate its resources over time in order to maximize the performance on the fly, at run-time, without having access to any offline information. The proposed scheduler, applied on a face detection streaming application and without using any offline information, is able to achieve similar performance compared to an optimal semi-online solution that has full knowledge of the input stream where the differences in throughput, observed quality, resource usage and energy efficiency are less than 1, 0.3, 0.2 and 4 percent respectively.","Authors":"Kanoun, K (Kanoun, Karim) ; Tekin, C (Tekin, Cem) ; Atienza, D (Atienza, David) ; van der Schaar, M (van der Schaar, Mihaela)","Title":"Big-Data Streaming Applications Scheduling Based on Staged Multi-Armed Bandits"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000388113900002 ISSN: 1536-1233 eISSN: 1558-0660","Keywords":"Cooperative spectrum sharing; incomplete information; multi-armed bandits; dynamic programming KeyWords Plus:COGNITIVE RADIO NETWORKS; INFORMATION; MECHANISMS; DIVERSITY; SELECTION; BANDITS","Categories":"Computer Science; Telecommunications Web of Science Categories:Computer Science, Information Systems; Telecommunications","Journal Information":"IEEE TRANSACTIONS ON MOBILE COMPUTING Volume: 15 Issue: 12 Pages: 2939-2953 DOI: 10.1109/TMC.2016.2516991 Published: DEC 1 2016","Abstract":"Cooperative Spectrum Sharing (CSS) is an appealing approach for primary users (PUs) to share spectrum with secondary users (SUs) because it increases the transmission range or rate of the PUs. Most previous works are focused on developing complex algorithms which may not be fast enough for real-time variations such as channel availability and/or assume perfect information about the network. Instead, we develop a learning mechanism for a PU to enable CSS in a strongly incomplete information scenario with low computational overhead. Our mechanism is based on a Markovian variant of multi-armed bandits (MABs) called superprocess, enhanced with the concept of Upper Confidence Bound (UCB) from stochastic MABs. By means of Monte-Carlo evaluations we show that, despite its low computational overhead, it converges to a low regret solution outperforming baseline approaches such as epsilon-greedy. This algorithm can be extended to include more sophisticated features while maintaining its desirable properties such as low computational overhead and fast speed of convergence.","Authors":"Lopez-Martinez, M (Lopez-Martinez, Mario) ; Alcaraz, JJ (Alcaraz, Juan J.) ; Badia, L (Badia, Leonardo) ; Zorzi, M (Zorzi, Michele)","Title":"A Superprocess with Upper Confidence Bounds for Cooperative Spectrum Sharing"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000387925500001 ISSN: 1381-1231 eISSN: 1572-9397","Keywords":"Monte-Carlo Tree Search; Sacrifice moves; Artificial intelligence; Games KeyWords Plus:GAME; GO; STRATEGY","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Theory & Methods","Journal Information":"JOURNAL OF HEURISTICS Volume: 22 Issue: 6 Pages: 783-813 DOI: 10.1007/s10732-016-9320-y Published: DEC 2016","Abstract":"One of the most basic activities performed by an intelligent agent is deciding what to do next. The decision is usually about selecting the move with the highest expectation, or exploring new scenarios. Monte-Carlo Tree Search (MCTS), which was developed as a game playing agent, deals with this exploration-exploitation 'dilemma' using a multi-armed bandits strategy. The success of MCTS in a wide range of problems, such as combinatorial optimisation, reinforcement learning, and games, is due to its ability to rapidly evaluate problem states without requiring domain-specific knowledge. However, it has been acknowledged that the trade-off between exploration and exploitation is crucial for the performance of the algorithm, and affects the efficiency of the agent in learning deceptive states. One type of deception is states that give immediate rewards, but lead to a suboptimal solution in the long run. These states are known as trap states, and have been thoroughly investigated in previous research. In this work, we study the opposite of trap states, known as sacrifice states, which are deceptive moves that result in a local loss but are globally optimal, and investigate the efficiency of MCTS enhancements in identifying this type of moves.","Authors":"Companez, N (Companez, Nathan) ; Aleti, A (Aleti, Aldeida) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Aleti, Aldeida  http://orcid.org/0000-0002-1716-690X","Title":"Can Monte-Carlo Tree Search learn to sacrifice?"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000386658000042 ISSN: 0018-9545 eISSN: 1939-9359","Keywords":"Antijamming; cognitive radio networks (CRNs); multiradio multichannel; spectrum access","Categories":"Engineering; Telecommunications; Transportation Web of Science Categories:Engineering, Electrical & Electronic; Telecommunications; Transportation Science & Technology","Journal Information":"IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY Volume: 65 Issue: 10 Pages: 8331-8344 DOI: 10.1109/TVT.2015.2511071 Published: OCT 2016","Abstract":"For achieving optimized spectrum usage, most existing opportunistic spectrum sensing and access protocols model the spectrum sensing and access problem as a partially observed Markov decision process by assuming that the information states and/or the primary users' (PUs) traffic statistics are known a priori to the secondary users (SUs). While theoretically sound, the existing solutions may not be effective in practice due to two main concerns. First, the assumptions are not practical, as before the communication starts, PUs' traffic statistics may not be readily available to the SUs. Second and more serious, existing approaches are extremely vulnerable to malicious jamming attacks. By leveraging the same statistic information and stochastic dynamic decision-making process that the SUs would follow, a cognitive attacker with sensing capability can sense and jam the channels to be accessed by SUs, while not interfering PUs. To address these concerns, we formulate the antijamming, multichannel access problem as a nonstochastic multi-armed bandit problem. By leveraging probabilistically shared information between the sender and the receiver, our proposed protocol enables them to hop to the same set of channels with high probability while gaining resilience to jamming attacks without affecting PUs' activities. We analytically show the convergence of the learning algorithms and derive the performance bound based on regret. We further discuss the problem of tracking the best adaptive strategy and characterize the performance bound based on a new regret. Extensive simulation results show that the probabilistic spectrum sensing and access protocol can overcome the limitation of existing solutions and is highly resilient to various jamming attacks even with jammed acknowledgment (ACK) information.","Authors":"Wang, Q (Wang, Qian) ; Ren, K (Ren, Kui) ; Ning, P (Ning, Peng) ; Hu, SS (Hu, Shengshan) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Ren, Kui  http://orcid.org/0000-0003-3441-6277","Title":"Jamming-Resistant Multiradio Multichannel Opportunistic Spectrum Access in Cognitive Radio Networks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000386206500009 ISSN: 0034-6527 eISSN: 1467-937X","Keywords":"Experimentation; Innovation; Stagnation; Multi-Armed Bandit; Correlated Arms; Technologies; R & D KeyWords Plus:STRATEGIC EXPERIMENTATION; INFORMATIONAL CASCADES; SEARCH; BANDITS; GROWTH; MODEL","Categories":"Business & Economics Web of Science Categories:Economics","Journal Information":"REVIEW OF ECONOMIC STUDIES Volume: 83 Issue: 4 Pages: 1579-1613 DOI: 10.1093/restud/rdw008 Published: OCT 2016","Abstract":"How do successive, forward-looking agents experiment with interdependent and endogenous technologies? In this article, trying a radically new technology not only is informative of the value of similar technologies, but also reduces the cost of experimenting with them, in effect expanding the space of affordable technologies. Successful radical experimentation has mixed effects: it improves the immediate outlook for further experimentation but decreases the value and the marginal value of experimentation in a longer term, resulting in less ambitious \"incremental\" experimentation and in a reduced size of radical experimentation. Incremental experimentation lowers the option value of similar technologies, which may spur a new wave of radical experimentation. However, experimentation eventually stagnates for all parameters of the model.","Authors":"Garfagnini, U (Garfagnini, Umberto) ; Strulovici, B (Strulovici, Bruno)","Title":"Social Experimentation with Interdependent and Expanding Technologies"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000376552600004 ISSN: 0957-4174 eISSN: 1873-6793","Keywords":"Spatial crowdsourcing; Task assignment; Combinatorial fractional programming; Multi-armed bandit KeyWords Plus:MULTIARMED BANDIT; CURRENT STATE; SYSTEMS","Categories":"Computer Science; Engineering; Operations Research & Management Science Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science","Journal Information":"EXPERT SYSTEMS WITH APPLICATIONS Volume: 58 Pages: 36-56 DOI: 10.1016/j.eswa.2016.03.022 Published: OCT 1 2016","Abstract":"Spatial crowdsourcing has emerged as a new paradigm for solving problems in the physical world with the help of human workers. A major challenge in spatial crowdsourcing is to assign reliable workers to nearby tasks. The goal of such task assignment process is to maximize the task completion in the face of uncertainty. This process is further complicated when tasks arrivals are dynamic and worker reliability is unknown. Recent research proposals have tried to address the challenge of dynamic task assignment. Yet the majority of the proposals do not consider the dynamism of tasks and workers. They also make the unrealistic assumptions of known deterministic or probabilistic workers' reliabilities. In this paper, we propose a novel approach for dynamic task assignment in spatial crowdsourcing. The proposed approach combines bi-objective optimization with combinatorial multi-armed bandits. We formulate an online optimization problem to maximize task reliability and minimize travel costs in spatial crowdsourcing. We propose the distance-reliability ratio (DRR) algorithm based on a combinatorial fractional programming approach. The DRR algorithm reduces travel costs by 80% while maximizing reliability when compared to existing algorithms. We extend the DRR algorithm for the scenario when worker reliabilities are unknown. We propose a novel algorithm (DRR-UCB) that uses an interval estimation heuristic to approximate worker reliabilities. Experimental results demonstrate that the DRR-UCB achieves high reliability in the face of uncertainty. The proposed approach is particularly suited for real-life dynamic spatial crowdsourcing scenarios. This approach is generalizable to the similar problems in other areas in expert systems. First, it encompasses online assignment problems when the objective function is a ratio of two linear functions. Second, it considers situations when intelligent and repeated assignment decisions are needed under uncertainty. (C) 2016 Elsevier Ltd. All rights reserved.","Authors":"ul Hassan, U (ul Hassan, Umair) ; Curry, E (Curry, Edward) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Curry, Edward  http://orcid.org/0000-0001-8236-6433","Title":"Efficient task assignment for spatial crowdsourcing: A combinatorial fractional optimization approach with semi-bandit learning"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000378952500002 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Multi-armed bandit; Online learning; Recommender systems","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 205 Pages: 16-21 DOI: 10.1016/j.neucom.2016.02.052 Published: SEP 12 2016","Abstract":"We consider a variant of the multi-armed bandit model, which we call multi-armed bandit problem with known trend, where the gambler knows the shape of the reward function of each arm but not its distribution. This new problem is motivated by different on-line problems like active learning, music and interface recommendation applications, where when an arm is sampled by the model the received reward change according to a known trend. By adapting the standard multi-armed bandit algorithm UCB1 to take advantage of this setting, we propose the new algorithm named Adjusted Upper Confidence Bound (A-UCB) that assumes a stochastic model. We provide upper bounds of the regret which compare favorably with the ones of UCB1. We also confirm that experimentally with different simulations. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Bouneffouf, D (Bouneffouf, Djallel) ; Feraud, R (Feraud, Raphael)","Title":"Multi-armed bandit problem with known trend"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000383822700008 ISSN: 0304-3975 eISSN: 1879-2294","Keywords":"Monte-Carlo tree search; Multi-armed bandit; Upper confidence bounds KeyWords Plus:MULTIARMED BANDIT PROBLEM","Categories":"Computer Science Web of Science Categories:Computer Science, Theory & Methods","Journal Information":"THEORETICAL COMPUTER SCIENCE Volume: 644 Pages: 92-105 DOI: 10.1016/j.tcs.2016.06.034 Published: SEP 6 2016","Abstract":"The key to the success of the upper confidence bounds on trees (UCT) algorithm, which is the most widely adopted variant of the Monte-Carlo tree search (MCTS) algorithm, is that it combines MCTS with the upper confidence bounds (UCB) bandit algorithm. The improved UCB algorithm is a bandit algorithm that has a superior regret bound to the UCB algorithm. However, some characteristics of the improved UCB algorithm are not suitable for direct application to MCTS. The combined confidence bounds (CCB) bandit algorithm is a modification of the improved UCB algorithm, making it more suitable for the task of tree searches. The CCB bandit algorithm can be further extended to regulate exploration in simple regret minimization in MCTS, through the exploration regulating factor. In this paper, we present an analysis of the bound on simple regret for the CCB bandit algorithm. We also give a comprehensive overview of how different choices of the exploration factor impact the games of 9 x 9 Go and 9 x 9 Nogo. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Liu, YC (Liu, Yun-Ching) ; Tsuruoka, Y (Tsuruoka, Yoshimasa)","Title":"Modification of improved upper confidence bounds for regulating exploration in Monte-Carlo tree search"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000383822700010 ISSN: 0304-3975 eISSN: 1879-2294","Keywords":"MCTS; Multi-armed bandit problem; Contextual bandit; LinUCB KeyWords Plus:COMPUTER GO","Categories":"Computer Science Web of Science Categories:Computer Science, Theory & Methods","Journal Information":"THEORETICAL COMPUTER SCIENCE Volume: 644 Pages: 114-126 DOI: 10.1016/j.tcs.2016.06.035 Published: SEP 6 2016","Abstract":"UCT is a standard method of Monte Carlo tree search (MCTS) algorithms, which have been applied to various domains and have achieved remarkable success. This study proposes a family of LinUCT algorithms that incorporate LinUCB into MCTS algorithms. LinUCB is a recently developed method that generalizes past episodes by ridge regression with feature vectors and rewards. LinUCB outperforms UCB1 in contextual multi-armed bandit problems. We introduce a straightforward application of LinUCB, LinUCTPLAIN by substituting UCB1 with LinUCB in UCT. We show that it does not work well owing to the minimax structure of game trees. To better handle such tree structures, we present LinUCTaAve and LinUCTFp by further incorporating two existing techniques, rapid action value estimation (RAVE) and feature propagation, which recursively propagates the feature vector of a node to that of its parent. Experiments were conducted with a synthetic model, which is an extension of the standard incremental random tree model in which each node has a feature vector that represents the characteristics of the corresponding position, and Finnsson's shock step game which is used to empirically analyze the performance of UCT with respect to the distribution of suboptimal moves. The experiments results indicate that LinUCTRAve and LinUCTFp outperform UCT, especially when the branching factor is relatively large. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Mandai, Y (Mandai, Yusaku) ; Kaneko, T (Kaneko, Tomoyuki)","Title":"LinUCB applied to Monte Carlo tree search"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000381483500012 ISSN: 1932-4553 eISSN: 1941-0484","Keywords":"Learning policies; mean-variance; multi-armed bandit; risk aversion; stochastic online learning KeyWords Plus:REGRET","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING Volume: 10 Issue: 6 Pages: 1093-1111 DOI: 10.1109/JSTSP.2016.2592622 Published: SEP 2016","Abstract":"The multi-armed bandit (MAB) problems have been studied mainly under the measure of expected total reward accrued over a horizon of length T. In this paper, we address the issue of risk in MAB problems and develop parallel results under the measure of mean-variance, a commonly adopted risk measure in economics and mathematical finance. We show that the model-specific regret and the model-independent regret in terms of the mean-variance of the reward process are lower bounded by Omega(log T) and Omega(T-2/3), respectively. We then show that variations of the UCB policy and the DSEE policy developed for the classic risk-neutral MAB achieve these lower bounds.","Authors":"Vakili, S (Vakili, Sattar) ; Zhao, Q (Zhao, Qing)","Title":"Risk-Averse Multi-Armed Bandit Problems Under Mean-Variance Measure"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000383411200001 ISSN: 1050-5164","Keywords":"Restless bandits; asymptotic optimality; Whittle's index policy; arm-aquiring bandits; nonindexable bandits KeyWords Plus:MARGINAL PRODUCTIVITY INDEXES; ADMISSION CONTROL; ALLOCATION; ABANDONMENT; RULE; QUEUES; ACCESS; SYSTEM","Categories":"Mathematics Web of Science Categories:Statistics & Probability","Journal Information":"ANNALS OF APPLIED PROBABILITY Volume: 26 Issue: 4 Pages: 1947-1995 DOI: 10.1214/15-AAP1137 Published: AUG 2016","Abstract":"We study the asymptotic optimal control of multi-class restless bandits. A restless bandit is a controllable stochastic process whose state evolution depends on whether or not the bandit is made active. Since finding the optimal control is typically intractable, we propose a class of priority policies that are proved to be asymptotically optimal under a global attractor property and a technical condition. We consider both a fixed population of bandits as well as a dynamic population where bandits can depart and arrive. As an example of a dynamic population of bandits, we analyze a multi-class M/M/S + M queue for which we show asymptotic optimality of an index policy. We combine fluid-scaling techniques with linear programming results to prove that when bandits are indexable, Whittle's index policy is included in our class of priority policies. We thereby generalize a result of Weber and Weiss [J. AppL Probab. 27 (1990) 637-648] about asymptotic optimality of Whittle's index policy to settings with (i) several classes of bandits, (ii) arrivals of new bandits and (iii) multiple actions. Indexability of the bandits is not required for our results to hold. For non-indexable bandits, we describe how to select priority policies from the class of asymptotically optimal policies and present numerical evidence that, outside the asymptotic regime, the performance of our proposed priority policies is nearly optimal.","Authors":"Verloop, IM (Verloop, I. M.)","Title":"ASYMPTOTICALLY OPTIMAL PRIORITY POLICIES FOR INDEXABLE AND NONINDEXABLE RESTLESS BANDITS"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000382151800006 ISSN: 0288-3635 eISSN: 1882-7055","Keywords":"Multi-armed Bandit; Swarm Intelligence; Interactive Game; Experiment; Optimal Strategy KeyWords Plus:SOCIAL-LEARNING STRATEGIES","Categories":"Computer Science Web of Science Categories:Computer Science, Hardware & Architecture; Computer Science, Theory & Methods","Journal Information":"NEW GENERATION COMPUTING Volume: 34 Issue: 3 Pages: 291-306 DOI: 10.1007/s00354-016-0306-y Published: AUG 2016","Abstract":"We obtain the conditions for the emergence of the swarm intelligence effect in an interactive game of restless multi-armed bandit (rMAB). A player competes with multiple agents. Each bandit has a payoff that changes with a probability p (c) per round. The agents and player choose one of three options: (1) Exploit (a good bandit), (2) Innovate (asocial learning for a good bandit among n (I) randomly chosen bandits), and (3) Observe (social learning for a good bandit). Each agent has two parameters (c, p (obs) ) to specify the decision: (i) c, the threshold value for Exploit, and (ii) p (obs) , the probability for Observe in learning. The parameters (c, p (obs) ) are uniformly distributed. We determine the optimal strategies for the player using complete knowledge about the rMAB. We show whether or not social or asocial learning is more optimal in the (p (c) , n (I) ) space and define the swarm intelligence effect. We conduct a laboratory experiment (67 subjects) and observe the swarm intelligence effect only if (p (c) , n (I) ) are chosen so that social learning is far more optimal than asocial learning.","Authors":"Yoshida, S (Yoshida, Shunsuke) ; Hisakado, M (Hisakado, Masato) ; Mori, S (Mori, Shintaro)","Title":"Interactive Restless Multi-armed Bandit Game and Swarm Intelligence Effect"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000381437700005 ISSN: 1932-4553 eISSN: 1941-0484","Keywords":"Activity classification; context-aware; online learning; active learning; multi-armed bandits KeyWords Plus:ACTIVITY RECOGNITION; BANDITS; ALGORITHMS; STROKE","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING Volume: 10 Issue: 5 Pages: 865-876 DOI: 10.1109/JSTSP.2016.2553648 Published: AUG 2016","Abstract":"Enabling accurate and low-cost classification of a range of motion activities is important for numerous applications, ranging from disease treatment and in-community rehabilitation of patients to athlete training. This paper proposes a novel contextual online learning method for activity classification based on data captured by low-cost, body-worn inertial sensors, and smartphones. The proposed method is able to address the unique challenges arising in enabling online, personalized and adaptive activity classification without requiring training phase from the individual. Another key challenge of activity classification is that the labels may change over time, as the data as well as the activity to be monitored evolve continuously, and the true label is often costly and difficult to obtain. The proposed algorithm is able to actively learn when to ask for the true label by assessing the benefits and costs of obtaining them. We rigorously characterize the performance of the proposed learning algorithm and Our experiments show that the proposed algorithm outperforms existing algorithms.","Authors":"Xu, J (Xu, Jie) ; Song, LQ (Song, Linqi) ; Xu, JY (Xu, James Y.) ; Pottie, GJ (Pottie, Gregory J.) ; van der Schaar, M (van der Schaar, Mihaela)","Title":"Personalized Active Learning for Activity Classification Using Wireless Wearable Sensors"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000381434500016 ISSN: 0733-8716 eISSN: 1558-0008","Keywords":"Video streaming; information-centric network; spectrum management; cognitive radio KeyWords Plus:ARCHITECTURE; SCHEME","Categories":"Engineering; Telecommunications Web of Science Categories:Engineering, Electrical & Electronic; Telecommunications","Journal Information":"IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS Volume: 34 Issue: 8 Pages: 2247-2259 DOI: 10.1109/JSAC.2016.2577320 Published: AUG 2016","Abstract":"To deal with the rapid growth of mobile data traffic and the user interest shift from peer-to-peer communications to content dissemination-based services, such as video streaming, information-centric networking has emerged as a promising architecture and has been increasingly used for wireless and mobile networks. In this paper, we focus on video dissemination in information-centric cognitive radio networks (IC-CRNs) and investigate the use of harvested bands for proactively caching video contents at the locations close to the interested users to improve the performance of video distribution. With consideration of the dynamic and unobservable nature of some parameters, we formulate the allocation of harvested bands as a Markov decision process with hidden and dynamic parameters and transform it into a partially observable Markov decision process and a multi-armed bandit formulation. Based on them, we develop a new spectrum management mechanism, which maximizes the benefit of proactive video caching as well as the efficiency of spectrum utilization in the IC-CRNs. Extensive simulation results demonstrate the significant performance improvement of the proposed scheme for video streaming.","Authors":"Si, PB (Si, Pengbo) ; Yue, H (Yue, Hao) ; Zhang, YH (Zhang, Yanhua) ; Fang, YG (Fang, Yuguang)","Title":"Spectrum Management for Proactive Video Caching in Information-Centric Cognitive Radio Networks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000381404300009 ISSN: 1976-7277","Keywords":"Cloud computing; virtualization; availability; restless multi-armed bandit; back up KeyWords Plus:OPTIMALITY","Categories":"Computer Science; Telecommunications Web of Science Categories:Computer Science, Information Systems; Telecommunications","Journal Information":"KSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS Volume: 10 Issue: 7 Pages: 3026-3049 DOI: 10.3837/tiis.2016.07.009 Published: JUL 31 2016","Abstract":"Cloud computing promises high performance and cost-efficiency. However, most cloud infrastructures operate at a low utilization, which greatly adheres cost effectiveness. Previous works focus on seeking efficient virtual machine (VM) consolidation strategies to increase the utilization of virtual resources in production environment, but overlook the under-utilization of backup virtual resources. We propose a heuristic time sharing policy of backup VMs derived from the restless multi-armed bandit problem. The proposed policy achieves increasing backup virtual resources utilization and providing high availability. Both the results in simulation and prototype system experiments show that the traditional 1:1 backup provision can be extended to 1:M (M >> 1) between the backup VMs and the service VMs, and the utilization of backup VMs can be enhanced significantly.","Authors":"Li, XY (Li, Xinyi) ; Qi, Y (Qi, Yong) ; Chen, PF (Chen, Pengfei) ; Zhang, XH (Zhang, Xiaohui)","Title":"A Heuristic Time Sharing Policy for Backup Resources in Cloud System"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000384310400032 ISSN: 0090-6778 eISSN: 1558-0857","Keywords":"Cognitive radio; spectrum reuse; multi-armed bandit KeyWords Plus:COGNITIVE RADIO NETWORKS; MULTIARMED BANDIT; SENSOR NETWORKS; ACCESS; GAME; SYSTEMS","Categories":"Engineering; Telecommunications Web of Science Categories:Engineering, Electrical & Electronic; Telecommunications","Journal Information":"IEEE TRANSACTIONS ON COMMUNICATIONS Volume: 64 Issue: 7 Pages: 3092-3103 DOI: 10.1109/TCOMM.2016.2569093 Published: JUL 2016","Abstract":"We formulate and study a multi-user multi-armed bandit problem that exploits the temporal-spatial opportunistic spectrum access (OSA) of primary user (PU) channels, so that secondary users (SUs) who do not interfere with each other can make use of the same PU channel. We first propose a centralized channel allocation policy that has logarithmic regret, but requires a central processor to solve an NP-complete optimization problem at exponentially increasing time intervals. To overcome the high computation complexity at the central processor, we also propose heuristic distributed policies that, however, have linear regrets. Our first distributed policy utilizes a distributed graph coloring and consensus algorithm to determine SUs' channel access ranks, while our second distributed policy incorporates channel access rank learning in a local procedure at each SU at the cost of a higher regret. We compare the performance of our proposed policies with other distributed policies recently proposed for temporal (but not spatial) OSA. We show that all these policies have linear regrets in our temporal-spatial OSA framework. Simulations suggest that our proposed policies have significantly smaller regrets than the other policies when spectrum temporal-spatial reuse is allowed.","Authors":"Zhang, Y (Zhang, Yi) ; Tay, WP (Tay, Wee Peng) ; Li, KH (Li, Kwok Hung) ; Esseghir, M (Esseghir, Moez) ; Gaiti, D (Gaiti, Dominique) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Tay, Wee Peng  A-5110-2011 http://orcid.org/0000-0002-1543-195X","Title":"Learning Temporal-Spatial Spectrum Reuse"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000380597700006 ISSN: 0167-6377 eISSN: 1872-7468","Keywords":"Markov Decision Processes; Multi-Armed Bandit problem; Default risk management; Dynamic resource allocation policies KeyWords Plus:STRATEGIC EXPERIMENTATION; LONG-RUN; MODEL","Categories":"Operations Research & Management Science Web of Science Categories:Operations Research & Management Science","Journal Information":"OPERATIONS RESEARCH LETTERS Volume: 44 Issue: 4 Pages: 451-456 DOI: 10.1016/j.orl.2016.04.008 Published: JUL 2016","Abstract":"We consider a resource allocation problem to decide how to share resources among different companies facing financial difficulties. The objective is to minimize the long term cost due to default events. Using the framework of Multi-Armed Restless Bandits, the optimal policy assigns an index value to each company, which orders its priority to be funded. The index generalizes the return-on-investment (ROI) index under the static setting, and we analyse the influence of the future events on the optimal dynamic policy. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Ayesta, U (Ayesta, U.) ; Erausquin, M (Erausquin, M.) ; Ferreira, E (Ferreira, E.) ; Jacko, P (Jacko, P.)","Title":"Optimal dynamic resource allocation to prevent defaults"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000378850300019 ISSN: 0929-6212 eISSN: 1572-834X","Keywords":"Cognitive radio; Multi-armed bandit; Opportunistic spectrum access; Distributed algorithms KeyWords Plus:ALLOCATION RULES; MEDIUM ACCESS; FRAMEWORK; REGRET; MAC","Categories":"Telecommunications Web of Science Categories:Telecommunications","Journal Information":"WIRELESS PERSONAL COMMUNICATIONS Volume: 89 Issue: 2 Pages: 663-685 DOI: 10.1007/s11277-016-3301-1 Published: JUL 2016","Abstract":"In the framework of cognitive radio, joint spectrum sensing and access strategies have been extensively studied recently. As a matter of fact, the sensing ability of cognitive radio is limited and the channel statistics may not be known as a priori. In this paper, we investigate the blind spectrum selection with the multi-armed bandit model, considering both primary user activities and channel quality to meet diverse QoS requirements, e.g. high transmission success rate for real-time applications and high throughput for best-effort applications. Firstly we propose a policy kth-UCB1 which is based on the UCB1 policy for multi-armed bandit problem but converges to the kth-best arm. Then we design a distributed order-optimal policy for multiple users accessing the rank-best channels according to their QoS requirements. The expected regret of proposed policy is proved to be logarithmic in the number of time slots and the simulation results implies it has better performance.","Authors":"Chen, YQ (Chen, Yongqun) ; Zhou, HB (Zhou, Huaibei) ; Kong, RS (Kong, Ruoshan) ; Huang, JY (Huang, Junyuan) ; Chen, B (Chen, Bo)","Title":"QoS-Based Blind Spectrum Selection with Multi-armed Bandit Problem in Cognitive Radio Networks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000375709800005 ISSN: 1387-2532 eISSN: 1573-7454","Keywords":"Multi-agent negotiation under incomplete information; Empirical methods; GENIUS framework; Algorithm selection KeyWords Plus:AGENT","Categories":"Automation & Control Systems; Computer Science Web of Science Categories:Automation & Control Systems; Computer Science, Artificial Intelligence","Journal Information":"AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS Volume: 30 Issue: 4 Pages: 697-723 DOI: 10.1007/s10458-015-9302-8 Published: JUL 2016","Abstract":"Despite the abundance of strategies in the multi-agent systems literature on repeated negotiation under incomplete information, there is no single negotiation strategy that is optimal for all possible domains. Thus, agent designers face an \"algorithm selection\" problem-which negotiation strategy to choose when facing a new domain and unknown opponent. Our approach to this problem is to design a \"meta-agent\" that predicts the performance of different negotiation strategies at run-time. We study two types of the algorithm selection problem in negotiation: In the off-line variant, an agent needs to select a negotiation strategy for a given domain but cannot switch to a different strategy once the negotiation has begun. For this case, we use supervised learning to select a negotiation strategy for a new domain that is based on predicting its performance using structural features of the domain. In the on-line variant, an agent is allowed to adapt its negotiation strategy over time. For this case, we used multi-armed bandit techniques that balance the exploration-exploitation tradeoff of different negotiation strategies. Our approach was evaluated using the GENIUS negotiation test-bed that is used for the annual international Automated Negotiation Agent Competition which represents the chief venue for evaluating the state-of-the-art multi-agent negotiation strategies. We ran extensive simulations using the test bed with all of the top-contenders from both off-line and on-line negotiation tracks of the competition. The results show that the meta-agent was able to outperform all of the finalists that were submitted to the most recent competition, and to choose the best possible agent (in retrospect) for more settings than any of the other finalists. This result was consistent for both off-line and on-line variants of the algorithm selection problem. This work has important insights for multi-agent systems designers, demonstrating that \"a little learning goes a long way\", despite the inherent uncertainty associated with negotiation under incomplete information.","Authors":"Ilany, L (Ilany, Litan) ; Gal, Y (Gal, Ya'akov)","Title":"Algorithm selection in bilateral negotiation"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000386970200001 ISSN: 2193-1127","Keywords":"crowdsourcing; exploration; exploitation; misinformation; disinformation; social search; bandit problem KeyWords Plus:INTELLIGENCE; EXPLORATION","Categories":"Mathematics; Mathematical Methods In Social Sciences Web of Science Categories:Mathematics, Interdisciplinary Applications; Social Sciences, Mathematical Methods","Journal Information":"EPJ DATA SCIENCE Volume: 5 Article Number: 20 DOI: 10.1140/epjds/s13688-016-0082-4 Published: JUN 6 2016","Abstract":"Collective search for people and information has tremendously benefited from emerging communication technologies that leverage the wisdom of the crowds, and has been increasingly influential in solving time-critical tasks such as the DARPA Network Challenge (DNC, also known as the Red Balloon Challenge). However, while collective search often invests significant resources in encouraging the crowd to contribute new information, the effort invested in verifying this information is comparable, yet often neglected in crowdsourcing models. This paper studies how the exploration-verification trade-off displayed by the teams modulated their success in the DNC, as teams had limited human resources that they had to divide between recruitment (exploration) and verification (exploitation). Our analysis suggests that team performance in the DNC can be modelled as a modified multi-armed bandit (MAB) problem, where information arrives to the team originating from sources of different levels of veracity that need to be assessed in real time. We use these insights to build a data-driven agent-based model, based on the DNC's data, to simulate team performance. The simulation results match the observed teams' behavior and demonstrate how to achieve the best balance between exploration and exploitation for general time-critical collective search tasks.","Authors":"Chen, HH (Chen, Haohui) ; Rahwan, I (Rahwan, Iyad) ; Cebrian, M (Cebrian, Manuel)","Title":"Bandit strategies in social search: the case of the DARPA red balloon challenge"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397240600015 PubMed ID: 27052578 ISSN: 0022-3077 eISSN: 1522-1598","Keywords":"observational learning; reward prediction error; reinforcement learning; ventral striatum; fMRI KeyWords Plus:HUMAN VENTRAL STRIATUM; PREDICTION ERRORS; PREFRONTAL CORTEX; DORSAL STRIATUM; COMPUTATIONS; BEHAVIOR; SYSTEMS; HUMANS; BRAIN; TASK","Categories":"Neurosciences & Neurology; Physiology Web of Science Categories:Neurosciences; Physiology","Journal Information":"JOURNAL OF NEUROPHYSIOLOGY Volume: 115 Issue: 6 Pages: 3195-3203 DOI: 10.1152/jn.00046.2016 Published: JUN 1 2016","Abstract":"A major open question is whether computational strategies thought to be used during experiential learning, specifically model-based and model-free reinforcement learning, also support observational learning. Furthermore, the question of how observational learning occurs when observers must learn about the value of options from observing outcomes in the absence of choice has not been addressed. In the present study we used a multi-armed bandit task that encouraged human participants to employ both experiential and observational learning while they underwent functional magnetic resonance imaging (fMRI). We found evidence for the presence of model-based learning signals during both observational and experiential learning in the intraparietal sulcus. However, unlike during experiential learning, model-free learning signals in the ventral striatum were not detectable during this form of observational learning. These results provide insight into the flexibility of the model-based learning system, implicating this system in learning during observation as well as from direct experience, and further suggest that the model-free reinforcement learning system may be less flexible with regard to its involvement in observational learning.","Authors":"Dunne, S (Dunne, Simon) ; D'Souza, A (D'Souza, Arun) ; O'Doherty, JP (O'Doherty, John P.)","Title":"The involvement of model-based but not model-free learning signals during observational reward learning in the absence of choice"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000380027200009 ISSN: 1536-1284 eISSN: 1558-0687","Categories":"Computer Science; Engineering; Telecommunications Web of Science Categories:Computer Science, Hardware & Architecture; Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications","Journal Information":"IEEE WIRELESS COMMUNICATIONS Volume: 23 Issue: 3 Pages: 64-73 Published: JUN 2016","Abstract":"services, next generation wireless networks are expected to be able to deliver high data rates while wireless resources become more and more scarce. This requires the next generation wireless networks to move toward new networking paradigms that are able to efficiently support resource-demanding applications such as personalized mobile services. Examples of such paradigms foreseen for the emerging 5G cellular networks include very densely deployed small cells and device-to-device communications. For 5G networks, it will be imperative to search for spectrum and energy-efficient solutions to the resource allocation problems that i) are amenable to distributed implementation, ii) are capable of dealing with uncertainty and lack of information, and iii) can cope with users' selfishness. The core objective of this article is to investigate and establish the potential of the MAB framework to address this challenge. In particular, we provide a brief tutorial on bandit problems, including different variations and solution approaches. Furthermore, we discuss recent applications as well as future research directions. In addition, we provide a detailed example of using an MAB model for energy-efficient small cell activation in 5G networks.","Authors":"Maghsudi, S (Maghsudi, Setareh) ; Hossain, E (Hossain, Ekram)","Title":"MULTI-ARMED BANDITS WITH APPLICATION TO 5G SMALL CELLS"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000379040400003 ISSN: 0019-5588 eISSN: 0975-7465","Keywords":"Stochastic processes; applied probability; statistics; discrete optimization KeyWords Plus:ALGORITHM","Categories":"Mathematics Web of Science Categories:Mathematics","Journal Information":"INDIAN JOURNAL OF PURE & APPLIED MATHEMATICS Volume: 47 Issue: 2 Pages: 195-212 DOI: 10.1007/s13226-016-0184-5 Published: JUN 2016","Abstract":"A variant of Simulated Annealing termed Simulated Annealing with Multiplicative Weights (SAMW) has been proposed in the literature. However, convergence was dependent on a parameter beta(T), which was calculated a-priori based on the total iterations T the algorithm would run for. We first show the convergence of SAMW even when a diminishing stepsize beta(k) -> 1 is used, where k is the index of iteration. Using this SAMW as a kernel, a stochastic multi-armed bandit (SMAB) algorithm called SOFTMIX can be improved to obtain the minimum-possible log(2) regret, as compared to log(2) regret of the original. Another modification of SOFTMIX is proposed which avoids the need for a parameter that is dependent on the reward distribution of the arms. Further, a variant of SOFTMIX that uses a comparison term drawn from another popular SMAB algorithm called UCB1 is then described. It is also shown why the proposed scheme is computationally more efficient over UCB1, and an alternative to this algorithm with simpler stepsizes is also proposed. Numerical simulations for all the proposed algorithms are then presented.","Authors":"Abdulla, MS (Abdulla, Mohammed Shahid) ; Bhatnagar, S (Bhatnagar, Shalabh)","Title":"MULTI-ARMED BANDITS BASED ON A VARIANT OF SIMULATED ANNEALING"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000379040400005 ISSN: 0019-5588 eISSN: 0975-7465","Keywords":"Multi-armed Bandit; mechanism design; learning algorithms","Categories":"Mathematics Web of Science Categories:Mathematics","Journal Information":"INDIAN JOURNAL OF PURE & APPLIED MATHEMATICS Volume: 47 Issue: 2 Pages: 229-272 DOI: 10.1007/s13226-016-0186-3 Published: JUN 2016","Abstract":"The multi-armed bandit (MAB) problem is a widely studied problem in machine learning literature in the context of online learning. In this article, our focus is on a specific class of problems namely stochastic MAB problems where the rewards are stochastic. In particular, we emphasize stochastic MAB problems with strategic agents. Dealing with strategic agents warrants the use of mechanism design principles in conjunction with online learning, and leads to non-trivial technical challenges. In this paper, we first provide three motivating problems arising from Internet advertising, crowdsourcing, and smart grids. Next, we provide an overview of stochastic MAB problems and key associated learning algorithms including upper confidence bound (UCB) based algorithms. We provide proofs of important results related to regret analysis of the above learning algorithms. Following this, we present mechanism design for stochastic MAB problems. With the classic example of sponsored search auctions as a backdrop, we bring out key insights in important issues such as regret lower bounds, exploration separated mechanisms, designing truthful mechanisms, UCB based mechanisms, and extension to multiple pull MAB problems. Finally we provide a bird's eye view of recent results in the area and present a few issues that require immediate future attention.","Authors":"Jain, S (Jain, Shweta) ; Bhat, S (Bhat, Satyanath) ; Ghalme, G (Ghalme, Ganesh) ; Padmanabhan, D (Padmanabhan, Divya) ; Narahari, Y (Narahari, Y.)","Title":"MECHANISMS WITH LEARNING FOR STOCHASTIC MULTI-ARMED BANDIT PROBLEMS"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000376599100007 ISSN: 0022-4715 eISSN: 1572-9613","Keywords":"Exploration and exploitation; Decision and information theory; Infomax; Multi-armed bandits; Large deviations KeyWords Plus:MULTIARMED BANDIT PROBLEM; INFORMATION; ALLOCATION; ADAPTATION; NETWORKS; FITNESS; RETINA","Categories":"Physics Web of Science Categories:Physics, Mathematical","Journal Information":"JOURNAL OF STATISTICAL PHYSICS Volume: 163 Issue: 6 Pages: 1454-1476 DOI: 10.1007/s10955-016-1521-0 Published: JUN 2016","Abstract":"Proper balance between exploitation and exploration is what makes good decisions that achieve high reward, like payoff or evolutionary fitness. The Infomax principle postulates that maximization of information directs the function of diverse systems, from living systems to artificial neural networks. While specific applications turn out to be successful, the validity of information as a proxy for reward remains unclear. Here, we consider the multi-armed bandit decision problem, which features arms (slot-machines) of unknown probabilities of success and a player trying to maximize cumulative payoff by choosing the sequence of arms to play. We show that an Infomax strategy (Info-p) which optimally gathers information on the highest probability of success among the arms, saturates known optimal bounds and compares favorably to existing policies. Conversely, gathering information on the identity of the best arm in the bandit leads to a strategy that is vastly suboptimal in terms of payoff. The nature of the quantity selected for Infomax acquisition is then crucial for effective tradeoffs between exploration and exploitation.","Authors":"Reddy, G (Reddy, Gautam) ; Celani, A (Celani, Antonio) ; Vergassola, M (Vergassola, Massimo)","Title":"Infomax Strategies for an Optimal Balance Between Exploration and Exploitation"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000375559700008 ISSN: 0740-817X eISSN: 1545-8830","Keywords":"Multi-objective simulation optimization; ranking and selection; Pareto frontier; maximize the expected maximum; max k-armed bandit problem KeyWords Plus:TRANSGRESSIVE SEGREGANTS; CONSTRAINED SYSTEMS; BUDGET ALLOCATION; UTILITY-THEORY; OPTIMIZATION; SELECTION","Categories":"Engineering; Operations Research & Management Science Web of Science Categories:Engineering, Industrial; Operations Research & Management Science","Journal Information":"IIE TRANSACTIONS Volume: 48 Issue: 6 Pages: 565-578 DOI: 10.1080/0740817X.2015.1096430 Published: JUN 2016","Abstract":"Commercial plant breeders improve economically important traits by selectively mating individuals from a given breeding population. Potential pairings are evaluated before the growing season using Monte Carlo simulation, and a mating design is created to allocate a fixed breeding budget across the parent pairs to achieve desired population outcomes. We introduce a novel objective function for this mating design problem that accurately models the goals of a certain class of breeding experiments. The resulting mating design problem is a computationally burdensome simulation optimization problem on a combinatorially large set of feasible points. We propose a two-step solution to this problem: (i) simulate to estimate the performance of each parent pair and (ii) solve an estimated version of the mating design problem, which is an integer program, using the simulation output. To reduce the computational burden when implementing steps (i) and (ii), we analytically identify a Pareto set of parent pairs that will receive the entire breeding budget at optimality. Since we wish to estimate the Pareto set in step (i) as input to step (ii), we derive an asymptotically optimal simulation budget allocation to estimate the Pareto set that, in our numerical experiments, out-performs Multi-objective Optimal Computing Budget Allocation in reducing misclassifications. Given the estimated Pareto set, we provide a branch-and-bound algorithm to solve the estimated mating design problem. Our approach dramatically reduces the computational effort required to solve the mating design problem when compared with naive methods.","Authors":"Hunter, SR (Hunter, Susan R.) ; McClosky, B (McClosky, Benjamin)","Title":"Maximizing quantitative traits in the mating design problem via simulation-based Pareto estimation"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000377928700009 ISSN: 1939-1374","Keywords":"Recommender systems; online learning; clustering algorithms; multi-armed bandit KeyWords Plus:RESTLESS MULTIARMED BANDIT; REGRET","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems; Computer Science, Software Engineering","Journal Information":"IEEE TRANSACTIONS ON SERVICES COMPUTING Volume: 9 Issue: 3 Pages: 433-445 DOI: 10.1109/TSC.2014.2365795 Published: MAY-JUN 2016","Abstract":"In this paper, we propose a novel large-scale, context-aware recommender system that provides accurate recommendations, scalability to a large number of diverse users and items, differential services, and does not suffer from \"cold start\" problems. Our proposed recommendation system relies on a novel algorithm which learns online the item preferences of users based on their click behavior, and constructs online item-cluster trees. The recommendations are then made by choosing an item-cluster level and then selecting an item within that cluster as a recommendation for the user. This approach is able to significantly improve the learning speed when the number of users and items is large, while still providing high recommendation accuracy. Each time a user arrives at the website, the system makes a recommendation based on the estimations of item payoffs by exploiting past context arrivals in a neighborhood of the current user's context. It exploits the similarity of contexts to learn how to make better recommendations even when the number and diversity of users and items is large. This also addresses the cold start problem by using the information gained from similar users and items to make recommendations for new users and items. We theoretically prove that the proposed algorithm for item recommendations converges to the optimal item recommendations in the long-run. We also bound the probability of making a suboptimal item recommendation for each user arriving to the system while the system is learning. Experimental results show that our approach outperforms the state-of-the-art algorithms by over 20 percent in terms of click through rates.","Authors":"Song, LQ (Song, Linqi) ; Tekin, C (Tekin, Cem) ; van der Schaar, M (van der Schaar, Mihaela)","Title":"Online Learning in Large-Scale Contextual Recommender Systems"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000372594300008 ISSN: 0090-5364","Keywords":"Multi-armed bandit problems; regret bounds; batches; multi-phase allocation; grouped clinical trials; sample size determination; switching cost KeyWords Plus:2 ARMED BANDIT; REGRET BOUNDS; SELECTING 1; ALLOCATION; TESTS; MODEL","Categories":"Mathematics Web of Science Categories:Statistics & Probability","Journal Information":"ANNALS OF STATISTICS Volume: 44 Issue: 2 Pages: 660-681 DOI: 10.1214/15-AOS1381 Published: APR 2016","Abstract":"Motivated by practical applications, chiefly clinical trials, we study the regret achievable for stochastic bandits under the constraint that the employed policy must split trials into a small number of batches. We propose a simple policy, and show that a very small number of batches gives close to minimax optimal regret bounds. As a byproduct, we derive optimal policies with low switching cost for stochastic bandits.","Authors":"Perchet, V (Perchet, Vianney) ; Rigollet, P (Rigollet, Philippe) ; Chassang, S (Chassang, Sylvain) ; Snowberg, E (Snowberg, Erik)","Title":"BATCHED BANDIT PROBLEMS"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000384819200007 ISSN: 0001-8678 eISSN: 1475-6064","Keywords":"Gittins indices; optimal learning; multi-armed bandit; non-Gaussian rewards; probabilistic interpolation KeyWords Plus:MULTIARMED BANDIT PROBLEM; LEVY PROCESSES; MULTIPARAMETER PROCESSES; GITTINS INDEX; CONVERGENCE; INFORMATION; ALGORITHM; TIME","Categories":"Mathematics Web of Science Categories:Statistics & Probability","Journal Information":"ADVANCES IN APPLIED PROBABILITY Volume: 48 Issue: 1 Pages: 112-136 DOI: 10.1017/apr.2015.9 Published: MAR 2016","Abstract":"We propose a novel theoretical characterization of the optimal 'Gittins index' policy in multi-armed bandit problems with non-Gaussian, infinitely divisible reward distributions. We first construct a continuous-time, conditional Levy process which probabilistically interpolates the sequence of discrete-time rewards. When the rewards are Gaussian, this approach enables an easy connection to the convenient time-change properties of a Brownian motion. Although no such device is available in general for the non-Gaussian case, we use optimal stopping theory to characterize the value of the optimal policy as the solution to a free-boundary partial integro-differential equation (PIDE). We provide the free-boundary PIDE in explicit form under the specific settings of exponential and Poisson rewards. We also prove continuity and monotonicity properties of the Gittins index in these two problems, and discuss how the PIDE can be solved numerically to find the optimal index value of a given belief state.","Authors":"Ding, Z (Ding, Zi) ; Ryzhov, IO (Ryzhov, Ilya O.)","Title":"OPTIMAL LEARNING WITH NON-GAUSSIAN REWARDS"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000383045300001 ISSN: 2194-668X eISSN: 2194-6698","Keywords":"Approximation algorithms; Stochastic optimization; Combinatorial optimization KeyWords Plus:MATROID SECRETARY; KNAPSACK-PROBLEM; IMPROVED BOUNDS; STEINER FOREST; ONLINE; COMPLEXITY; DISTRIBUTIONS; UNCERTAINTY; SCHEMES; INTEGER","Categories":"Operations Research & Management Science Web of Science Categories:Operations Research & Management Science","Journal Information":"JOURNAL OF THE OPERATIONS RESEARCH SOCIETY OF CHINA Volume: 4 Issue: 1 Pages: 1-47 DOI: 10.1007/s40305-015-0116-9 Published: MAR 2016","Abstract":"Stochastic optimization has established itself as a major method to handle uncertainty in various optimization problems by modeling the uncertainty by a probability distribution over possible realizations. Traditionally, the main focus in stochastic optimization has been various stochastic mathematical programming (such as linear programming, convex programming). In recent years, there has been a surge of interest in stochastic combinatorial optimization problems from the theoretical computer science community. In this article, we survey some of the recent results on various stochastic versions of classical combinatorial optimization problems. Since most problems in this domain are NP-hard (or #P-hard, or even PSPACE-hard), we focus on the results which provide polynomial time approximation algorithms with provable approximation guarantees. Our discussions are centered around a few representative problems, such as stochastic knapsack, stochastic matching, multi-armed bandit etc. We use these examples to introduce several popular stochastic models, such as the fixed-set model, 2-stage stochastic optimization model, stochastic adaptive probing model etc, as well as some useful techniques for designing approximation algorithms for stochastic combinatorial optimization problems, including the linear programming relaxation approach, boosted sampling, content resolution schemes, Poisson approximation etc. We also provide some open research questions along the way. Our purpose is to provide readers a quick glimpse to the models, problems, and techniques in this area, and hopefully inspire new contributions","Authors":"Li, J (Li, Jian) ; Liu, Y (Liu, Yu)","Title":"Approximation Algorithms for Stochastic Combinatorial Optimization Problems"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000370822200019 PubMed ID: 26780509 ISSN: 1097-6256 eISSN: 1546-1726","Categories":"Neurosciences & Neurology Web of Science Categories:Neurosciences","Journal Information":"NATURE NEUROSCIENCE Volume: 19 Issue: 3 Pages: 471-+ DOI: 10.1038/nn.4223 Published: MAR 2016","Abstract":"Cholinergic neurotransmission affects decision-making, notably through the modulation of perceptual processing in the cortex. In addition, acetylcholine acts on value-based decisions through as yet unknown mechanisms. We found that nicotinic acetylcholine receptors (nAChRs) expressed in the ventral tegmental area (VTA) are involved in the translation of expected uncertainty into motivational value. We developed a multi-armed bandit task for mice with three locations, each associated with a different reward probability. We found that mice lacking the nAChR beta 2 subunit showed less uncertainty-seeking than their wild-type counterparts. Using model-based analysis, we found that reward uncertainty motivated wild-type mice, but not mice lacking the nAChR beta 2 subunit. Selective re-expression of the beta 2 subunit in the VTA was sufficient to restore spontaneous bursting activity in dopamine neurons and uncertainty-seeking. Our results reveal an unanticipated role for subcortical nAChRs in motivation induced by expected uncertainty and provide a parsimonious account for a wealth of behaviors related to nAChRs in the VTA expressing the beta 2 subunit.","Authors":"Naude, J (Naude, Jeremie) ; Tolu, S (Tolu, Stefania) ; Dongelmans, M (Dongelmans, Malou) ; Torquet, N (Torquet, Nicolas) ; Valverde, S (Valverde, Sebastien) ; Rodriguez, G (Rodriguez, Guillaume) ; Pons, S (Pons, Stephanie) ; Maskos, U (Maskos, Uwe) ; Mourot, A (Mourot, Alexandre) ; Marti, F (Marti, Fabio) ; Faure, P (Faure, Philippe) ...More...Less Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Mourot, Alexandre  H-4636-2013 http://orcid.org/0000-0002-8839-7481 Naude, Jeremie  http://orcid.org/0000-0001-5781-6498 Torquet, Nicolas  http://orcid.org/0000-0001-9032-193X","Title":"Nicotinic receptors in the ventral tegmental area promote uncertainty-seeking"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000370486200005 ISSN: 1530-8669 eISSN: 1530-8677","Keywords":"network selection; network handoff; network handoff cost; online learning KeyWords Plus:ADAPTIVE ALLOCATION RULES; MULTIARMED BANDIT PROBLEM; DECISION ALGORITHM; ACCESS","Categories":"Computer Science; Engineering; Telecommunications Web of Science Categories:Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications","Journal Information":"WIRELESS COMMUNICATIONS & MOBILE COMPUTING Volume: 16 Issue: 4 Pages: 441-458 DOI: 10.1002/wcm.2525 Published: MAR 2016","Abstract":"In heterogeneous wireless networks, network selection algorithms provide the user with the optimum network access choice. The optimal network is evaluated according to network parameters. Considering that the network parameters are dynamic and unavailable for the user in realistic heterogeneous wireless network environments, most existing network selection algorithms cannot work effectively. Learning-based algorithms can address the problem of uncertain network parameters, while they commonly need considerable network handoff, resulting in unbearable handoff cost. In order to tackle the uncertainty of network parameters, we formulate the network selection problem as a multi-armed bandit problem. Moreover, two online learning-based network selection algorithms with a special consideration on reducing network handoff cost are proposed. By updating in a block manner, both algorithms achieve optimal logarithmic-order regret and limited network handoff cost. The simulation indicates that the two algorithms can significantly reduce the network handoff cost and improve the transmission performance compared with existing algorithms, simultaneously. Copyright (c) 2014 John Wiley & Sons, Ltd.","Authors":"Du, ZY (Du, Zhiyong) ; Wu, QH (Wu, Qihui) ; Yang, PL (Yang, Panlong)","Title":"Learning with handoff cost constraint for network selection in heterogeneous wireless networks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000362606800005 ISSN: 1384-1076 eISSN: 1384-1092","Keywords":"(cosmology:) cosmic background radiation; cosmology: observations; methods: observational; methods: statistical KeyWords Plus:PROBE WMAP OBSERVATIONS; GRAVITY-WAVES; HIGH-REDSHIFT; 1ST STARS; POLARIZATION; REIONIZATION; EMISSION; DUST; FLUCTUATIONS; FOREGROUNDS","Categories":"Astronomy & Astrophysics Web of Science Categories:Astronomy & Astrophysics","Journal Information":"NEW ASTRONOMY Volume: 43 Pages: 26-36 DOI: 10.1016/j.newast.2015.07.010 Published: FEB 2016","Abstract":"A preferred method to detect the curl-component, or B-mode, signature of inflationary gravitational waves (IGWs) in the cosmic microwave background (CMB) polarization, in the absence of foregrounds and lensing, is a prolonged integration over a single patch of sky of a few square degrees. In practice, however, foregrounds abound and the sensitivity to B modes can be improved considerably by finding the region of sky cleanest of foregrounds. The best strategy to detect B modes thus involves a tradeoff between exploration (to find lower-foreground patches) and exploitation (through prolonged integration). This problem is akin to the multi-armed bandit (MAB) problem in probability theory, wherein a gambler faces a series of slot machines with unknown winning odds and must develop a strategy to maximize his/her winnings with some finite number of pulls. While the optimal MAB strategy remains to be determined, a number of algorithms have been developed in an effort to maximize the winnings. Here, based on this resemblance, we tackle the search for IGW B modes with single frequency experiments in the presence of spatially varying foregrounds by developing adaptive survey strategies to optimize the sensitivity to IGW B modes. We demonstrate, using realistic foreground models and taking lensing-induced B modes into account, that adaptive experiments can substantially improve the upper bound on the tensor-to-scalar ratio (by factors of 2 and 3 in single frequency experiments, and possibly even more). Similar techniques can be applied to other surveys, including 21-cm measurements of signatures of the epoch of reionization, searches for a stochastic primordial gravitational wave background, deep-field imaging by the James Webb Space Telescope or various radio interferometers, and transient follow-up searches. (C) 2015 Elsevier B.V. All rights reserved.","Authors":"Kovetz, ED (Kovetz, Ely D.) ; Kamionkowski, M (Kamionkowski, Marc)","Title":"Cosmic bandits: Exploration versus exploitation in CMB B-mode experiments"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000388376105054 ISBN:978-1-4673-8682-1 ISSN: 0743-1619","Categories":"Automation & Control Systems Web of Science Categories:Automation & Control Systems","Journal Information":"2016 AMERICAN CONTROL CONFERENCE (ACC) Book Series: Proceedings of the American Control Conference Pages: 5263-5269 Published: 2016","Abstract":"Motivated by the goal of formally integrating human designers into computational systems for engineering design optimization, I study decision making under uncertainty with multiple objectives in the context of the multi-armed bandit problem. A key aspect of multi-objective optimization is the need for scalarization, i.e., a way to combine the various objectives into a single well-defined scalar objective function. I study the case where the multi-objective rewards are Gaussian distributed and the scalarization is linear and develop an algorithm that achieves optimal performance, i.e., converges to selecting the best arm at the highest possible rate.","Authors":"Reverdy, P (Reverdy, Paul) Book Group Author(s):IEEE","Title":"Gaussian multi-armed bandit problems with multiple objectives"}, {"Categories":"Computer Science; Robotics Web of Science Categories:Computer Science, Artificial Intelligence; Robotics","Journal Information":"2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016) Pages: 1844-1850 Published: 2016","Abstract":"State-of-the-art neural microprobes contain hundreds of electrodes within a single shaft. Due to hardware and wiring restrictions, it is usually only possible to measure a small subset of the available electrodes simultaneously. The selection of the best channels is typically performed offline either manually or automatically. However, having a fixed selection for long-term observation does not allow the system to react to changes in the neural activity, and may therefore lead to the loss of important information. In this paper, we formulate the process of autonomously selecting the best subset of electrodes as a combinatorial multi-armed bandit problem with non-stationary rewards, thus allowing the probe to adapt its selection policies online. In order to minimize exploratory actions of the probe, we furthermore take advantage of the existing dependencies between neighboring channels. Our approach is an adaptation of the discounted upper confidence bounds (D-UCB) algorithm, and identifies the electrodes providing the largest amount of non-redundant information. To the best of our knowledge, this is the first online approach for the problem of electrode selection. In extensive experiments, we demonstrate that our solution is not only able to converge towards an average optimal selection policy, but it is also able to react to changes in the neural activity or to damages of the recording electrodes.","Authors":"Gordillo, C (Gordillo, Camilo) ; Frank, B (Frank, Barbara) ; Ulbert, I (Ulbert, Istvan) ; Paul, O (Paul, Oliver) ; Ruther, P (Ruther, Patrick) ; Burgard, W (Burgard, Wolfram) Book Group Author(s):IEEE","Title":"Automatic Channel Selection in Neural Microprobes: A Combinatorial Multi-Armed Bandit Approach"}, {"Categories":"Computer Science; Remote Sensing; Telecommunications Web of Science Categories:Computer Science, Hardware & Architecture; Remote Sensing; Telecommunications","Journal Information":"2016 13TH ANNUAL IEEE INTERNATIONAL CONFERENCE ON SENSING, COMMUNICATION, AND NETWORKING (SECON) Pages: 432-440 Published: 2016","Abstract":"In this paper, we address an important problem in the wireless monitoring, i.e., how to choose channels with best (or worst) qualities timely and accurately. We consider both scenarios of one or more sniffers simultaneously monitoring multiple channels in the same area. Since the channel information is initially unknown to the sniffers, we shall adopt learning methods during the monitoring to predict the channel condition by a short time of observation. We formulate this problem as a novel branch of the classic multi-armed bandit (MAB) problem, named exploration bandit problem, to achieve a trade-off between monitoring time/resource budget and the channel selection accuracy. In the multiple sniffer cases, including partly-distributed (with limited communications) and fully-distributed (without any communications) scenarios, we take communication costs and interference costs into account, and analyze how these costs affect the accuracy of channel selection. Extensive simulations are conducted and the results show that the proposed algorithms could achieve higher channel selection accuracy than other exploration bandit approaches, hence it proves the advantages of the proposed algorithms.","Authors":"Xue, Y (Xue, Yuan) ; Zhou, P (Zhou, Pan) ; Jiang, T (Jiang, Tao) ; Mao, SW (Mao, Shiwen) ; Huang, XL (Huang, Xiaolei) Book Group Author(s):IEEE","Title":"Distributed Learning for Multi-Channel Selection in Wireless Network Monitoring"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000392494000052 ISBN:978-1-5090-1732-4","Categories":"Computer Science; Remote Sensing; Telecommunications Web of Science Categories:Computer Science, Hardware & Architecture; Remote Sensing; Telecommunications","Journal Information":"2016 13TH ANNUAL IEEE INTERNATIONAL CONFERENCE ON SENSING, COMMUNICATION, AND NETWORKING (SECON) Pages: 459-467 Published: 2016","Abstract":"We consider the shortest path routing (SPR) problem of a network with time varying link metrics in unknown environments. Due to potential denial of service attacks, the distributions of link states could be stochastic (benign or i.i.d.), contaminated or adversarial (non-i.i.d.) at different temporal and spatial locations. Without any a priori, designing an adaptive SPR protocol to cope with all possible situations in practice optimally is a very challenging issue. In this paper, we present the first solution by formulating it as a multi-armed bandit problem. By introducing novel control parameters to explore link conditions, our proposed algorithms could automatically detect features of the environment within a unified framework and find the optimal SPR strategies with almost optimal learning performance in all possible cases over time. Moreover, we study important issues related to the practical implementation, such as decoupling route selection with multi-path route probing, cooperative learning among multiple sources, the \"cold-start\" issue and delayed feedback of our algorithm. Nonetheless, the proposed SPR algorithms can be implemented with low complexity and they are proved to scale very well with the network size. The efficacy of the proposed solutions is verified by simulations from the real trace-driven datasets. Comparing to existing approaches in a typical network scenario, our algorithm has a 65.3% improvement of network delay given a learning period and a 81.5% improvement of learning duration under a specified network delay.","Authors":"Zhou, P (Zhou, Pan) ; Cheng, L (Cheng, Lin); Wu, DO (Wu, Dapeng Oliver) Book Group Author(s):IEEE","Title":"Shortest Path Routing in Unknown Environments: Is the Adaptive Optimal Strategy Available?"}, {"Categories":"Computer Science; Remote Sensing; Telecommunications Web of Science Categories:Computer Science, Hardware & Architecture; Remote Sensing; Telecommunications","Journal Information":"2016 13TH ANNUAL IEEE INTERNATIONAL CONFERENCE ON SENSING, COMMUNICATION, AND NETWORKING (SECON) Pages: 468-476 Published: 2016","Abstract":"We model and analyze a User-Equipment (UE) based wireless network selection method where individuals act on their stochastic knowledge of the expected behavior off their available networks. In particular, we focus on networks with millimeter-wave (mmWave) radio. Modeling mmWave radio access technologies (RATs) as a stochastic 3-state process based on their physical layer characteristics in Line-of-Sight (LOS), Non-Line-of-Sight (NLOS), and Outage states, we make the realistic assumption that users have no knowledge of the statistics of the RATs and must learn these while maximizing the throughput obtained. We develop an online learning-based approach to access network selection: a user-centric Multi-Armed Bandit Problem that incorporates the cost of switching access networks. We develop an online learning policy that groups network access to minimize costs for RAT selection, analyze the regret (loss due to uncertainty) of our algorithm. We also show that our algorithm obtains optimal regret and in numerical examples achieves 24% increase in total throughput compared to existing techniques for high throughput mmWave RATs that vary over a fast timescale.","Authors":"Wang, M (Wang, Michael) ; Dutta, A (Dutta, Aveek) ; Buccapatnam, S (Buccapatnam, Swapna) ; Chiang, M (Chiang, Mung) Book Group Author(s):IEEE","Title":"Regret-Minimizing Exploration in HetNets with mmWave"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000391578700031 ISBN:978-1-5090-2914-3","Keywords":"HetNets; green networks; energy saving","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Interdisciplinary Applications; Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"2016 8TH IFIP INTERNATIONAL CONFERENCE ON NEW TECHNOLOGIES, MOBILITY AND SECURITY (NTMS) Published: 2016","Abstract":"In this paper, we study the problem of energy saving in Heterogeneous Networks (HetNets). The use of HetNets is one of the promising solutions for future cellular technologies as they can provide high network capacity. The aim of this work is to study the energy efficiency in HetNets. The underlying idea is to perform the energy saving process on a fully distributed manner without the need to know the global network state or to exchange messages. We first express the optimization problem where the optimal cell range expansion bias must be found to maximize the energy efficiency. We introduce after that a learning approach based on Multi-Armed Bandit (MAB) allowing the operating cells to learn, through multiple iterations, the best expansion bias to use.","Authors":"Ameur, H (Ameur, Hocine) ; Esseghir, M (Esseghir, Moez) ; Khoukhi, L (Khoukhi, Lyes) Edited by:Badra, M; Pau, G; Vassiliou, V","Title":"Fully Distributed Approach for Energy Saving in Heterogeneous Networks"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000391523100022 ISBN:978-1-5090-3651-6","Categories":"Computer Science Web of Science Categories:Computer Science, Theory & Methods","Journal Information":"2016 IEEE 1ST INTERNATIONAL WORKSHOPS ON FOUNDATIONS AND APPLICATIONS OF SELF* SYSTEMS (FAS*W) Pages: 118-119 DOI: 10.1109/FAS-W.2016.36 Published: 2016","Abstract":"Motivated by runtime verification of QoS requirements in self-adaptive and self-organizing systems that are able to reconfigure their structure and behavior in response to runtime data, we propose a QoS-aware variant of Thompson sampling for multi-armed bandits. It is applicable in settings where QoS satisfaction of an arm has to be ensured with high confidence efficiently, rather than finding the optimal arm while minimizing regret. Preliminary experimental results encourage further research in the field of QoS-aware decision making.","Authors":"Belzner, L (Belzner, Lenz) ; Gabor, T (Gabor, Thomas) Edited by:Elnikety, S; Lewis, PR; Mullerschloer, C","Title":"QoS-Aware Multi-Armed Bandits"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000391598800234 ISBN:978-1-5090-3254-9","Categories":"Engineering; Telecommunications Web of Science Categories:Engineering, Electrical & Electronic; Telecommunications","Journal Information":"2016 IEEE 27TH ANNUAL INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR, AND MOBILE RADIO COMMUNICATIONS (PIMRC) Pages: 1355-1360 Published: 2016","Abstract":"ognitive radio technology is a promising solution to the exponential growth in bandwidth demand sustained by increasing number of ubiquitous connected devices. The allocated spectrum is opened to the secondary users conditioned on limited interference on the primary owner of the band. A major bottleneck in cognitive radio systems is to find the best available channel quickly from a large accessible set of channels. This work formulates the channel exploration-exploitation dilemma as a multi-arm bandit problem. Existing theoretical solutions to a multi-arm bandit are adapted for cognitive radio and evaluated in an experimental test-bed. It is shown that a Thompson sampling based algorithm efficiently converges to the best channel faster than the existing algorithms and achieves higher asymptotic average throughput. We then propose a multihop extension together with an experimental proof of concept.","Authors":"Toldov, V (Toldov, Viktor) ; Clavier, L (Clavier, Laurent) ; Loscri, V (Loscri, Valeria) ; Mitton, N (Mitton, Nathalie) Book Group Author(s):IEEE","Title":"A Thompson Sampling Approach to Channel Exploration-Exploitation Problem in Multihop Cognitive Radio Networks"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000391598800257 ISBN:978-1-5090-3254-9","Keywords":"Opportunistic spectrum access; cognitive radio networks; cognitive MAC; access strategy; distributed channel selection KeyWords Plus:DYNAMIC SPECTRUM ACCESS; NETWORKS","Categories":"Engineering; Telecommunications Web of Science Categories:Engineering, Electrical & Electronic; Telecommunications","Journal Information":"2016 IEEE 27TH ANNUAL INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR, AND MOBILE RADIO COMMUNICATIONS (PIMRC) Pages: 1496-1501 Published: 2016","Abstract":"In spectrum overlay cognitive wireless networks, when the secondary users are blind, i.e. have no prior knowledge of primary users' activities, a major question remains as to what strategy to adopt to learn the primary state information online while sensing the spectrum for exploiting idle bands. To maximize secondary network spectral utilization while minimizing interference and collision, requires searching for a balance between choosing empirically best channel while investigating other channels for potential opportunities. Moreover, competition should also be resolved to prevent congestion and collision among secondary users who tend to access the same idle channel. In this paper, by compressing the inessential exploration phase, we first propose an asymptotically optimal, fast-converging channel selection algorithm referred to as modified-myopic strategy for the single-user scenario based on the result of multi-armed bandit. Through analysis and simulation modeling, we evaluate the performance of our approach and compare it to that of other strategies in the literature. Next we adopt a game-theoretic model and extend our algorithm to design a fair and low-complexity access strategy for multi-user case based on a generalized CSMA/CA scheme. Finally, we show the improved efficiency of the proposed approach, in particular significantly enhanced for dense networks.","Authors":"Rastegardoost, N (Rastegardoost, Nazanin) ; Jabbari, B (Jabbari, Bijan) Book Group Author(s):IEEE","Title":"Blind Channel Selection Strategies for Distributed Cognitive MAC"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000391662700001 ISSN: 1532-4435","Keywords":"contextual bandit problem; exploration-exploitation tradeoff; nonparametric regression; regret bound; upper confidence bound KeyWords Plus:SLICED INVERSE REGRESSION; DIMENSION REDUCTION; RATES; CONVERGENCE; ALLOCATION","Categories":"Automation & Control Systems; Computer Science Web of Science Categories:Automation & Control Systems; Computer Science, Artificial Intelligence","Journal Information":"JOURNAL OF MACHINE LEARNING RESEARCH Volume: 17 Article Number: 149 Published: 2016","Abstract":"Multi-armed bandit problem is an important optimization game that requires an exploration-exploitation tradeoff to achieve optimal total reward. Motivated from industrial applications such as online advertising and clinical research, we consider a setting where the rewards of bandit machines are associated with covariates, and the accurate estimation of the corresponding mean reward functions plays an important role in the performance of allocation rules. Under a flexible problem setup, we establish asymptotic strong consistency and perform a finite-time regret analysis for a sequential randomized allocation strategy based on kernel estimation. In addition, since many nonparametric and parametric methods in supervised learning may be applied to estimating the mean reward functions but guidance on how to choose among them is generally unavailable, we propose a model combining allocation strategy for adaptive performance. Simulations and a real data evaluation are conducted to illustrate the performance of the proposed allocation strategy.","Authors":"Qian, W (Qian, Wei) ; Yang, YH (Yang, Yuhong)","Title":"Kernel Estimation and Model Combination in A Bandit Problem with Covariates"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000391583800028 ISBN:978-1-5090-5206-6","Keywords":"Multi-armed bandits; Switching regret; Trend detection","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"PROCEEDINGS OF 3RD IEEE/ACM INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS, (DSAA 2016) Pages: 263-271 DOI: 10.1109/DSAA.2016.35 Published: 2016","Abstract":"We study a variation of the classical multi-armed bandits problem. In this problem, the learner has to make a sequence of decisions, picking from a fixed set of choices. In each round, she receives as feedback only the loss incurred from the chosen action. Conventionally, this problem has been studied when losses of the actions are drawn from an unknown distribution or when they are adversarial. In this paper, we study this problem when the losses of the actions also satisfy certain structural properties, and especially, do show a trend structure. When this is true, we show that using trend detection, we can achieve regret of order (O) over tilde (N root TK) with respect to a switching strategy for the version of the problem where a single action is chosen in each round and (O) over tilde (Nm root TK) when m actions are chosen each round. This guarantee is a significant improvement over the conventional benchmark. Our approach can, as a framework, be applied in combination with various well-known bandit algorithms, like Exp3. For both versions of the problem, we give regret guarantees also for the anytime setting, i.e. when length of the choice-sequence is not known in advance. Finally, we pinpoint the advantages of our method by comparing it to some well-known other strategies.","Authors":"Nakhe, P (Nakhe, Paresh) ; Reiffenhauser, R (Reiffenhaeuser, Rebecca) Book Group Author(s):IEEE","Title":"Trend Detection based Regret Minimization for Bandit Problems"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000391436100011 ISBN:978-1-5090-2054-6 ISSN: 0742-1303","Keywords":"IEEE 802.11ac; link adaptation; Multi-Armed Bandit KeyWords Plus:PERFORMANCE; WLANS","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Hardware & Architecture; Engineering, Electrical & Electronic","Journal Information":"2016 IEEE 41ST CONFERENCE ON LOCAL COMPUTER NETWORKS (LCN) Book Series: Conference on Local Computer Networks Pages: 87-94 DOI: 10.1109/LCN.2016.20 Published: 2016","Abstract":"High throughput wireless access networks based on IEEE 802.11ac show a significant challenge in dynamically selecting the link configuration parameters based on channel conditions due to large pool of design set; like number of spatial streams, channel bonding, guard intervals, frame aggregation and different modulation and coding schemes. In this paper, we develop a learning based approach for link adaptation motivated by the multi-armed bandit based distributed learning algorithm. The proposed link adaptation algorithm, BanditLink, explores different possible configuration options based on observing their impact over the network performance at various channel conditions. We analyze the performance of BanditLink from simulation results, and observe that it performs significantly better compared to other competing mechanisms proposed in the literature.","Authors":"Karmakar, R (Karmakar, Raja) ; Chattopadhyay, S (Chattopadhyay, Samiran) ; Chakraborty, S (Chakraborty, Sandip) Book Group Author(s):IEEE","Title":"Dynamic Link Adaptation in IEEE 802.11ac: A Distributed Learning Based Approach"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000391436100088 ISBN:978-1-5090-2054-6 ISSN: 0742-1303","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Hardware & Architecture; Engineering, Electrical & Electronic","Journal Information":"2016 IEEE 41ST CONFERENCE ON LOCAL COMPUTER NETWORKS (LCN) Book Series: Conference on Local Computer Networks Pages: 579-582 DOI: 10.1109/LCN.2016.97 Published: 2016","Abstract":"Data capture is important for some critical network applications, such as network diagnosis and criminal investigation. In multi-channel wireless networks, the fundamental challenge for data capture is how to assign operation channels to wireless sniffers. The existing approaches make some impractical assumptions, such as the prior knowledge on network traffic and the perfect conditions of data capture. In this paper, we relax these assumptions and investigate the sniffer-channel assignment problem in multi-hop scenarios. Especially, sniffer redundancy deployment is discussed, which enables multiple sniffers to monitor one traffic. This problem is formulated as a combinatorial multi-arm bandit (MAB) problem, and a cooperative distribute learning policy is proposed. We analyze the regret of our policy in theory, and validate its effectiveness through numerical simulations.","Authors":"Xu, J (Xu, Jing) ; Liu, W (Liu, Wei) ; Zeng, K (Zeng, Kai) Book Group Author(s):IEEE","Title":"Monitoring Multi-hop Multi-channel Wireless Networks: Online Sniffer Channel Assignment"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000391486100001 ISSN: 1532-4435","Keywords":"combinatorial multi-armed bandit; online learning; upper con fidence bound; social influence maximization; online advertising","Categories":"Automation & Control Systems; Computer Science Web of Science Categories:Automation & Control Systems; Computer Science, Artificial Intelligence","Journal Information":"JOURNAL OF MACHINE LEARNING RESEARCH Volume: 17 Published: 2016","Abstract":"We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where subsets of base arms with unknown distributions form super arms. In each round, a super arm is played and the base arms contained in the super arm are played and their outcomes are observed. We further consider the extension in which more based arms could be probabilistically triggered based on the outcomes of already triggered arms. The reward of the super arm depends on the outcomes of all played arms, and it only needs to satisfy two mild assumptions, which allow a large class of nonlinear reward instances. We assume the availability of an offline (alpha,beta)-approximation oracle that takes the means of the outcome distributions of arms and outputs a super arm that with probability beta generates an alpha fraction of the optimal expected reward. The objective of an online learning algorithm for CMAB is to minimize (alpha,beta)-approximation regret, which is the difference between the alpha,beta fraction of the expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide CUCB algorithm that achieves O(log n) distribution-dependent regret, where n is the number of rounds played, and we further provide distribution-independent bounds for a large class of reward functions. Our regret analysis is tight in that it matches the bound of UCB1 algorithm (up to a constant factor) for the classical MAB problem, and it significantly improves the regret bound in a earlier paper on combinatorial bandits with linear rewards. We apply our CMAB framework to two new applications, probabilistic maximum coverage and social influence maximization, both having nonlinear reward structures. In particular, application to social influence maximization requires our extension on probabilistically triggered arms.","Authors":"Chen, W (Chen, Wei) ; Wang, YJ (Wang, Yajun) ; Yuan, Y (Yuan, Yang) ; Wang, QS (Wang, Qinshi)","Title":"Combinatorial Multi-Armed Bandit and Its Extension to Probabilistically Triggered Arms"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000391461800001 ISSN: 1532-4435","Keywords":"multi-armed bandit; best-arm identification; pure exploration; information-theoretic divergences; sequential testing KeyWords Plus:SEQUENTIAL ALLOCATION","Categories":"Automation & Control Systems; Computer Science Web of Science Categories:Automation & Control Systems; Computer Science, Artificial Intelligence","Journal Information":"JOURNAL OF MACHINE LEARNING RESEARCH Volume: 17 Article Number: 1 Published: 2016","Abstract":"The stochastic multi-armed bandit model is a simple abstraction that has proven useful in many different contexts in statistics and machine learning. Whereas the achievable limit in terms of regret minimization is now well known, our aim is to contribute to a better understanding of the performance in terms of identifying the m best arms. We introduce generic notions of complexity for the two dominant frameworks considered in the literature: fixed-budget and fixed-confidence settings. In the fixed-confidence setting, we provide the first known distribution-dependent lower bound on the complexity that involves information-theoretic quantities and holds when m >= 1 under general assumptions. In the specific case of two armed-bandits, we derive refined lower bounds in both the fixed-confidence and fixed-budget settings, along with matching algorithms for Gaussian and Bernoulli bandit models. These results show in particular that the complexity of the fixed-budget setting may be smaller than the complexity of the fixed-confidence setting, contradicting the familiar behavior observed when testing fully specified alternatives. In addition, we also provide improved sequential stopping rules that have guaranteed error probabilities and shorter average running times. The proofs rely on two technical results that are of independent interest : a deviation lemma for self-normalized sums (Lemma 7) and a novel change of measure inequality for bandit models (Lemma 1).","Authors":"Kaufmann, E (Kaufmann, Emilie) ; Cappe, O (Cappe, Olivier) ; Garivier, A (Garivier, Aurelien)","Title":"On the Complexity of Best-Arm Identification in Multi-Armed Bandit Models"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000391198500081 ISBN:978-1-5090-3933-3 ISSN: 0272-5428","Keywords":"Amplification; derandomization; Free game; multi-armed bandit; sketching KeyWords Plus:REGULARITY LEMMA; ALGORITHM; TIME","Categories":"Computer Science Web of Science Categories:Computer Science, Theory & Methods","Journal Information":"2016 IEEE 57TH ANNUAL SYMPOSIUM ON FOUNDATIONS OF COMPUTER SCIENCE (FOCS) Book Series: Annual IEEE Symposium on Foundations of Computer Science Pages: 770-779 DOI: 10.1109/FOCS.2016.87 Published: 2016","Abstract":"We present techniques for decreasing the error probability of randomized algorithms and for converting randomized algorithms to deterministic (nonuniform) algorithms. Unlike most existing techniques that involve repetition of the randomized algorithm and hence a slowdown, our techniques produce algorithms with a similar run-time to the original randomized algorithms. The amplification technique is related to a certain stochastic multi-armed bandit problem. The derandomization technique - which is the main contribution of this work - points to an intriguing connection between derandomization and sketching/sparsification. We demonstrate the techniques by showing algorithms for approximating free games (constraint satisfaction problems on dense bipartite graphs).","Authors":"Grossman, O (Grossman, Ofer) ; Moshkovitz, D (Moshkovitz, Dana) Book Group Author(s):IEEE","Title":"Amplification and Derandomization Without Slowdown"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390993204135 ISBN:978-1-4799-6664-6 ISSN: 1550-3607","Categories":"Engineering; Telecommunications Web of Science Categories:Engineering, Electrical & Electronic; Telecommunications","Journal Information":"2016 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS (ICC) Book Series: IEEE International Conference on Communications Published: 2016","Abstract":"Without a proper observation of the energy demand of the receiving terminals, the retailer may be obliged to purchase additional energy from the real-time market and may take the risk of losing profit. This paper proposes two combinatorial multi-armed bandit (CMAB) strategies in green cloud radio access network (C-RAN) with simultaneous wireless information and power transfer under the assumption that no initial knowledge of forthcoming energy demand and renewable energy supply are known to the central processor. The aim of the proposed strategies is to find the set of optimal sizes of the energy packages to be purchased from the day-ahead market by observing the instantaneous energy demand and learning from the behaviour of cooperative energy trading, so that the total cost of the retailer can be minimized. Two novel iterative algorithms, namely, ForCMAB energy trading and RevCMAB energy trading are introduced to search for the optimal set of energy packages in ascending and descending order of package sizes, respectively. Simulation results indicate that CMAB approach in our proposed strategies offers the significant advantage in terms of reducing overall energy cost of the retailer, as compared to other schemes without learningbased optimization.","Authors":"Ariffin, WNSFW (Ariffin, Wan Nur Suryani Firuz Wan) ; Zhang, XR (Zhang, Xinruo) ; Nakhai, MR (Nakhai, Mohammad Reza) Book Group Author(s):IEEE","Title":"Combinatorial Multi-armed Bandit Algorithms for Real-time Energy Trading in Green C-RAN"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390993205094 ISBN:978-1-4799-6664-6 ISSN: 1550-3607","Categories":"Engineering; Telecommunications Web of Science Categories:Engineering, Electrical & Electronic; Telecommunications","Journal Information":"2016 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS (ICC) Book Series: IEEE International Conference on Communications Published: 2016","Abstract":"A promising architecture for content caching in wireless small cell networks is storing popular files at small base stations (sBSs) with limited storage capacities. Using localized communication, an sBS serves local user requests, while reducing the load on the macro cellular network. The sBS should cache the most popular files to maximize the number of cache hits. Content popularity is described by a popularity profile containing the expected demand of each file. Assuming a fixed popularity profile of which the sBS has complete knowledge, the optimal content placement problem reduces to ranking the files according to their expected demands and caching the highest ranked ones. Instead, we assume that the popularity profile is varying, for example depending on fluctuating types of users in the vicinity of the sBS, and it is unknown a priori. We present a novel algorithm based on contextual multi-armed bandits, in which the sBS regularly updates its cache content and observes the demands for cached files in different contexts, thereby learning context-dependent popularity profiles over time. We derive a sub-linear regret bound, proving that our algorithm learns smart caching. Our numerical results confirm that by exploiting contextual information, our algorithm outperforms reference algorithms in various scenarios.","Authors":"Mueller, S (Mueller, Sabrina) ; Atan, O (Atan, Onur) ; van der Schaar, M (van der Schaar, Mihaela) ; Klein, A (Klein, Anja) Book Group Author(s):IEEE","Title":"Smart Caching in Wireless Small Cell Networks via Contextual Multi-Armed Bandits"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390993202034 ISBN:978-1-4799-6664-6 ISSN: 1550-3607","Categories":"Engineering; Telecommunications Web of Science Categories:Engineering, Electrical & Electronic; Telecommunications","Journal Information":"2016 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS (ICC) Book Series: IEEE International Conference on Communications Published: 2016","Abstract":"We consider the channel access problem arising in opportunistic scheduling over fading channels, cognitive radio networks, and server scheduling. The multi-channel communication system consists of N channels. Each channel evolves as a time-nonhomogeneous multi-state Markov process. At each time instant, a user chooses M channels to transmit information. Some reward depending on the states of the chosen channels is obtained for each transmission. The objective is to design an access policy that maximizes the expected accumulated discounted reward over a finite or infinite horizon. The considered problem can be cast into a restless multi-armed bandit (RMAB) problem with PSPACE-hardness. A natural alternative is to consider the easily implementable myopic policy. In this paper, we perform an theoretical analysis on the considered RMAB problem, and establish a set of closed-form conditions to guarantee the optimality of the myopic policy.","Authors":"Wang, KH (Wang, Kehao) ; Chen, L (Chen, Lin) ; Yu, JH (Yu, Jihong) Book Group Author(s):IEEE","Title":"On Optimality of Myopic Policy in Multi-channel Opportunistic Access"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390890800112 ISBN:978-1-4503-4073-1","Keywords":"Query pooling; multi-armed bandits","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT Pages: 1089-1098 DOI: 10.1145/2983323.2983816 Published: 2016","Abstract":"Existing retrieval systems rely on a single active query to pull documents from the index. Relevance feedback may be used to iteratively refine the query, but only one query is active at a time. If the user's information need has multiple aspects, the query must represent the union of these aspects. We consider a new paradigm of retrieval where multiple queries are kept \"active\" simultaneously. In the presence of rate limits, the active queries take turns accessing the index to retrieve another \"page\" of results. Turns are assigned by a multi-armed bandit based on user feedback. This allows the system to explore which queries return more relevant results and to exploit the best ones. In empirical tests, query pools outperform solo, combined queries. Significant improvement is observed both when the subtopic queries are known in advance and when the queries are generated in a user-interactive process.","Authors":"Li, C (Li, Cheng) ; Resnick, P (Resnick, Paul) ; Mei, QZ (Mei, Qiaozhu) Book Group Author(s):ACM","Title":"Multiple Queries as Bandit Arms"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390749102097 ISBN:978-1-5090-0622-9","Keywords":"Multi-armed Bandit; Online learning; Recommender systems","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"2016 IEEE CONGRESS ON EVOLUTIONARY COMPUTATION (CEC) Book Series: IEEE Congress on Evolutionary Computation Pages: 2543-2549 Published: 2016","Abstract":"We study here an a problem called \"the multi-armed bandit problem with known trend\", where an agent knows the shape of the reward function of each arm but not its distribution. This problem is motivated by different real world tasks, where when an arm is sampled by the model the received reward change according to a known trend. By adapting the standard multi-armed bandit algorithms, we propose to study the regret upper bounds of three algorithms: the two first one assumes a stochastic model; and the last one is based on a Bayesian approach.","Authors":"Bouneffouf, D (Bouneffouf, Djallel) Book Group Author(s):IEEE","Title":"Finite-time Analysis of the Multi-armed Bandit Problem with Known Trend"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390604100006 ISSN: 1877-0509","Keywords":"Machine learning; feature selection; rank aggregation; multi-armed bandit; parallel computation; MeLiF; MeLiF; PQMeLiF; MAMeLiF","Categories":"Computer Science Web of Science Categories:Computer Science, Theory & Methods","Journal Information":"5TH INTERNATIONAL YOUNG SCIENTIST CONFERENCE ON COMPUTATIONAL SCIENCE, YSC 2016 Book Series: Procedia Computer Science Volume: 101 Pages: 45-52 DOI: 10.1016/j.procs.2016.11.007 Published: 2016","Abstract":"One of the classical problems in machine learning and data mining is feature selection. A feature selection algorithm is expected to be quick, and at the same time it should show high performance. MeLiF algorithm effectively solves this problem using ensembles of ranking filters. This article describes two different ways to improve MeLiF algorithm performance with parallelization. Our experiments shown that proposed schemes improve algorithm performance significantly and increase feature selection quality.","Authors":"Smetannikov, I (Smetannikov, Ivan) ; Isaev, I (Isaev, Ilya) ; Filchenkov, A (Filchenkov, Andrey) Edited by:Boukhanovsky, A; Bubak, M; Balakhontceva, M","Title":"New Approaches to Parallelization in Filters Aggregation Based Feature Selection Algorithms"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000389832500053","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"2016 IEEE/ACM 24TH INTERNATIONAL SYMPOSIUM ON QUALITY OF SERVICE (IWQOS) Published: 2016","Abstract":"Social networks have been popular platforms for information propagation. An important use case is viral marketing: given a promotion budget, an advertiser can choose some influential users as the seed set and provide them free or discounted sample products; in this way, the advertiser hopes to increase the popularity of the product in the users' friend circles by the world-of-mouth effect, and thus maximizes the number of users that information of the production can reach. There has been a body of literature studying the influence maximization problem. Nevertheless, the existing studies mostly investigate the problem on a one-off basis, assuming fixed known influence probabilities among users, or the knowledge of the exact social network topology. In practice, the social network topology and the influence probabilities are typically unknown to the advertiser, which can be varying over time, i.e., in cases of newly established, strengthened or weakened social ties. In this paper, we focus on a dynamic non-stationary social network and design a randomized algorithm, RSB, based on multi-armed bandit optimization, to maximize influence propagation over time. The algorithm produces a sequence of online decisions and calibrates its explore-exploit strategy utilizing outcomes of previous decisions. It is rigorously proven to achieve an upper-bounded regret in reward and applicable to large-scale social networks. Practical effectiveness of the algorithm is evaluated using real-world datasets, which demonstrates that our algorithm outperforms previous stationary methods under non-stationary conditions.","Authors":"Bao, YX (Bao, Yixin) ; Wang, XK (Wang, Xiaoke) ; Wang, Z (Wang, Zhi) ; Wu, C (Wu, Chuan) ; Lau, FCM (Lau, Francis C. M.) Book Group Author(s):IEEE","Title":"Online Influence Maximization in Non-Stationary Social Networks"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000389832500062","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"2016 IEEE/ACM 24TH INTERNATIONAL SYMPOSIUM ON QUALITY OF SERVICE (IWQOS) Published: 2016","Abstract":"Mobile crowdsensing has become a novel and promising paradigm in collecting environmental data. A critical problem in improving the QoS of crowdsensing is to decide which users to select to perform sensing tasks, in order to obtain the most informative data, while maintaining the total sensing costs below a given budget. The key challenges lie in (i) finding an effective measure of the informativeness of users' data, (ii) learning users' sensing costs which are unknown a priori, and (iii) designing efficient user selection algorithms that achieve low-regret guarantees. In this paper, we build Gaussian Processes (G-Ps) to model spatial locations, and provide a mutual information-based criteria to characterize users' informativeness. To tackle the second and third challenges, we model the problem as a budgeted multi-armed bandit (MAB) problem based on stochastic assumptions, and propose an algorithm with theoretically proven low-regret guarantee. Our theoretical analysis and evaluation results both demonstrate that our algorithm can efficiently select most informative users under stringent constraints.","Authors":"Yang, S (Yang, Shuo) ; Wu, F (Wu, Fan) ; Tang, SJ (Tang, Shaojie) ; Luo, T (Luo, Tie) ; Gao, XF (Gao, Xiaofeng) ; Kong, LH (Kong, Linghe) ; Chen, GH (Chen, Guihai) Book Group Author(s):IEEE","Title":"Selecting Most Informative Contributors with Unknown Costs for Budgeted Crowdsensing"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000389516201105 ISBN:978-1-4673-8026-3 ISSN: 1050-4729","Categories":"Automation & Control Systems; Robotics Web of Science Categories:Automation & Control Systems; Robotics","Journal Information":"2016 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA) Book Series: IEEE International Conference on Robotics and Automation ICRA Pages: 1957-1964 Published: 2016","Abstract":"This paper presents the Dexterity Network (Dex-Net) 1.0, a dataset of 3D object models and a sampling-based planning algorithm to explore how Cloud Robotics can be used for robust grasp planning. The algorithm uses a Multi-Armed Bandit model with correlated rewards to leverage prior grasps and 3D object models in a growing dataset that currently includes over 10,000 unique 3D object models and 2.5 million parallel-jaw grasps. Each grasp includes an estimate of the probability of force closure under uncertainty in object and gripper pose and friction. Dex-Net 1.0 uses Multi-View Convolutional Neural Networks (MV-CNNs), a new deep learning method for 3D object classification, to provide a similarity metric between objects, and the Google Cloud Platform to simultaneously run up to 1,500 virtual cores, reducing experiment runtime by up to three orders of magnitude. Experiments suggest that correlated bandit techniques can use a cloud-based network of object models to significantly reduce the number of samples required for robust grasp planning. We report on system sensitivity to variations in similarity metrics and in uncertainty in pose and friction. Code and updated information is available at http://berkeleyautomation.github.io/dex-net/.","Authors":"Mahler, J (Mahler, Jeffrey) ; Pokorny, FT (Pokorny, Florian T.) ; Hou, B (Hou, Brian) ; Roderick, M (Roderick, Melrose) ; Laskey, M (Laskey, Michael) ; Aubry, M (Aubry, Mathieu) ; Kohlhoff, K (Kohlhoff, Kai) ; Kroger, T (Kroeger, Torsten) ; Kuffner, J (Kuffner, James) ; Goldberg, K (Goldberg, Ken) Edited by:Okamura, A; Menciassi, A; Ude, A; Burschka, D; Lee, D; Arrichiello, F; Liu, H; Moon, H; Neira, J; Sycara, K; Yokoi, K; Martinet, P; Oh, P; Valdastri, P; Krovi, V","Title":"Dex-Net 1.0: A Cloud-Based Network of 3D Objects for Robust Grasp Planning Using a Multi-Armed Bandit Model with Correlated Rewards"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390154400228 ISBN:978-1-4673-9953-1","Categories":"Computer Science; Telecommunications Web of Science Categories:Computer Science, Information Systems; Telecommunications","Journal Information":"IEEE INFOCOM 2016 - THE 35TH ANNUAL IEEE INTERNATIONAL CONFERENCE ON COMPUTER COMMUNICATIONS Published: 2016","Abstract":"Inspired by cognitive radio networks, we consider a setting where multiple users share several channels modeled as a multi-user multi-armed bandit (MAB) problem. The characteristics of each channel are unknown and are different for each user. Each user can choose between the channels, but her success depends on the particular channel chosen as well as on the selections of other users: if two users select the same channel their messages collide and none of them manages to send any data. Our setting is fully distributed, so there is no central control. As in many communication systems, the users cannot set up a direct communication protocol, so information exchange must be limited to a minimum. We develop an algorithm for learning a stable configuration for the multi-user MAB problem. We further offer both convergence guarantees and experiments inspired by real communication networks, including comparison to state-of-the-art algorithms.","Authors":"Avner, O (Avner, Orly) ; Mannor, S (Mannor, Shie) Book Group Author(s):IEEE","Title":"Multi-user lax communications: a multi-armed bandit approach"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390154400226 ISBN:978-1-4673-9953-1","Categories":"Computer Science; Telecommunications Web of Science Categories:Computer Science, Information Systems; Telecommunications","Journal Information":"IEEE INFOCOM 2016 - THE 35TH ANNUAL IEEE INTERNATIONAL CONFERENCE ON COMPUTER COMMUNICATIONS Published: 2016","Abstract":"In cognitive radio networks (CRNs), dynamic spectrum access has been proposed to improve the spectrum utilization, but it also generates spectrum misuse problems. One common solution to these problems is to deploy monitors to detect misbehaviors on certain channel. However, in multi-channel CRNs, it is very costly to deploy monitors on every channel. With a limited number of monitors, we have to decide which channels to monitor. In addition, we need to determine how long to monitor each channel and in which order to monitor, because switching channels incurs costs. Moreover, the information about the misuse behavior is not available a priori. To answer those questions, we model the spectrum usage monitoring problem as an adversarial multi-armed bandit problem with switching costs and design two effective online algorithms, SpecWatch and SpecWatch(+). In SpecWatch, we select strategies based on the monitoring history and repeat the same strategy for certain timeslots to reduce switching costs. We prove its expected weak regret, i.e., the performance difference between the solution of SpecWatch and optimal (fixed) solution, is O(T-2/3), where T is the time horizon. Whereas, in SpecWatch+, we select strategies more strategically to improve the performance. We show its actual weak regret is O(T-2/3) with probability 1-delta, for any delta is an element of (0, 1). Both algorithms are evaluated through extensive simulations.","Authors":"Li, M (Li, Ming) ; Yang, DJ (Yang, Dejun) ; Lin, J (Lin, Jian) ; Li, M (Li, Ming) ; Tang, J (Tang, Jian) Book Group Author(s):IEEE","Title":"SpecWatch: Adversarial Spectrum Usage Monitoring in CRNs with Unknown Statistics"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390154400268 ISBN:978-1-4673-9953-1","Keywords":"Opportunistic spectrum scheduling; protocol interference model; restless multi-armed bandit; approximation algorithm KeyWords Plus:RESTLESS BANDIT; ACCESS; INDEXABILITY; OPTIMALITY; CHANNELS; CAPACITY; INDEX","Categories":"Computer Science; Telecommunications Web of Science Categories:Computer Science, Information Systems; Telecommunications","Journal Information":"IEEE INFOCOM 2016 - THE 35TH ANNUAL IEEE INTERNATIONAL CONFERENCE ON COMPUTER COMMUNICATIONS Published: 2016","Abstract":"Given a set of communication links in cognitive radio networks, assume that the underlying channel state information along each link is unknown; however, we can estimate it by exploiting the feedbacks and evolutions of channel states. Assume time is divided into time-slots. Under the protocol interference model, the opportunistic spectrum scheduling problem aims to select interference-free links to transmit at each time-slot to maximize the average throughput over the long time horizon. Existing works on the opportunistic spectrum scheduling problem cannot satisfyingly address the wireless interference constraints. We apply the framework of restless multi-armed bandit and develop approximation algorithms for the problem with stochastic identical links and nonidentical links respectively. Based on the updated estimations of channel states, the proposed algorithms keep refining future link scheduling decisions. We also obtain approximation bounds of these two proposed algorithms.","Authors":"Xu, XH (Xu, Xiaohua) ; Song, M (Song, Min) Book Group Author(s):IEEE","Title":"Approximation Algorithms for Wireless Opportunistic Spectrum Scheduling in Cognitive Radio Networks"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000388373402086 ISBN:978-1-4799-9988-0 ISSN: 1520-6149","Keywords":"Online learning; stochastic approximation; stochastic Cramer-Rao bound; continuum multi-armed bandit; sequential decision making KeyWords Plus:STOCHASTIC-APPROXIMATION","Categories":"Acoustics; Engineering Web of Science Categories:Acoustics; Engineering, Electrical & Electronic","Journal Information":"2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING PROCEEDINGS Book Series: International Conference on Acoustics Speech and Signal Processing ICASSP Pages: 2289-2293 Published: 2016","Abstract":"The problem of online learning and optimization of unknown Markov jump linear models is considered. A new online learning algorithm, referred to as Markovian simultaneous perturbations stochastic approximation (MSPSA), is proposed. It is shown that MSPSA achieves the minimax regret order of Theta(root T). Using the Van Trees inequality (stochastic Cramer-Rao bound), it is shown that Theta(root T) is the lowest regret order achievable. Simulation results show scenarios that MSPSA offers significant gain over the greedy certainty equivalent approaches.","Authors":"Baltaoglu, S (Baltaoglu, Sevi) ; Tong, L (Tong, Lang) ; Zhao, Q (Zhao, Qing) Book Group Author(s):IEEE","Title":"ONLINE LEARNING AND OPTIMIZATION OF MARKOV JUMP LINEAR MODELS"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000388373403080 ISBN:978-1-4799-9988-0 ISSN: 1520-6149","Keywords":"adaptive group-testing; Boolean compressive sensing; information gain; multi-armed bandit","Categories":"Acoustics; Engineering Web of Science Categories:Acoustics; Engineering, Electrical & Electronic","Journal Information":"2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING PROCEEDINGS Book Series: International Conference on Acoustics Speech and Signal Processing ICASSP Pages: 3261-3265 Published: 2016","Abstract":"A new method for solving adaptive Boolean compressive sensing is proposed. By greedy maximization of an expected information gain, a conventional method controls the pool-size for adaptive Boolean compressive sensing. However, the conventional greedy method has the drawback that it has no guarantee of convergence to the optimal strategy. To solve the problem, based on the multi-armed bandit, the proposed method controls the pool-size adaptively. The information gain of the conventional greedy method is rewritten as the reward of the multi-armed bandit, and the multi-armed bandit is introduced into adaptive Boolean compressive sensing. Experimental results indicate that the correct rate of exact recovery of the proposed method converges to 1 fast without prior knowledge about the number of defective items and that the proposed method outperforms the conventional greedy method in the case that the number of defective items is large.","Authors":"Kawaguchi, Y (Kawaguchi, Yohei) ; Togami, M (Togami, Masahito) Book Group Author(s):IEEE","Title":"ADAPTIVE BOOLEAN COMPRESSIVE SENSING BY USING MULTI-ARMED BANDIT"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000388373403160 ISBN:978-1-4799-9988-0 ISSN: 1520-6149","Keywords":"Cognitive radio; multi-armed bandit; spectrum reuse KeyWords Plus:GAME","Categories":"Acoustics; Engineering Web of Science Categories:Acoustics; Engineering, Electrical & Electronic","Journal Information":"2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING PROCEEDINGS Book Series: International Conference on Acoustics Speech and Signal Processing ICASSP Pages: 3661-3665 Published: 2016","Abstract":"We formulate and study a multi-user multi-armed bandit (MAB) problem that exploits the temporal-spatial reuse of primary user (PU) channels so that secondary users (SUs) who do not interfere with each other can make use of the same PU channel. We first propose a centralized channel allocation policy that has logarithmic regret, but requires a central processor to solve a NP-complete optimization problem at exponentially increasing time intervals. To avoid the high computation complexity at the central processor and the need for SU synchronization, we propose a heuristic distributed policy that incorporates channel access rank learning in a local procedure at each SU at the cost of a higher regret. We compare the performance of our proposed policies with other distributed policies recently proposed for opportunistic spectrum access. Simulations suggest that our proposed policies significantly outperform the benchmark algorithms when spectrum temporal-spatial reuse is allowed.","Authors":"Zhang, Y (Zhang, Yi) ; Tay, WP (Tay, Wee Peng) ; Li, KH (Li, Kwok Hung) ; Esseghir, M (Esseghir, Moez) ; Gaiti, D (Gaiti, Dominique) Book Group Author(s):IEEE Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Tay, Wee Peng  A-5110-2011 http://orcid.org/0000-0002-1543-195X","Title":"OPPORTUNISTIC SPECTRUM ACCESS WITH TEMPORAL-SPATIAL REUSE IN COGNITIVE RADIO NETWORKS"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000389914600011 ISSN: 1935-7524","Keywords":"Contextual bandit problem; MABC; nonparametric bandit; adaptive estimation; regret bound KeyWords Plus:MULTIARMED BANDIT; CONFIDENCE BANDS; CLASSIFIERS; RATES","Categories":"Mathematics Web of Science Categories:Statistics & Probability","Journal Information":"ELECTRONIC JOURNAL OF STATISTICS Volume: 10 Issue: 1 Pages: 242-270 DOI: 10.1214/15-EJS1104 Published: 2016","Abstract":"Motivated by applications in personalized web services and clinical research, we consider a multi-armed bandit problem in a setting where the mean reward of each arm is associated with some covariates. A multi-stage randomized allocation with arm elimination algorithm is proposed to combine the flexibility in reward function modeling and a theoretical guarantee of a cumulative regret minimax rate. When the function smoothness parameter is unknown, the algorithm is equipped with a histogram estimation based smoothness parameter selector using Lepski's method, and is shown to maintain the regret minimax rate up to a logarithmic factor under a \"self-similarity\" condition.","Authors":"Qian, W (Qian, Wei) ; Yang, YH (Yang, Yuhong) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Yang, Yuhong  http://orcid.org/0000-0003-3618-3083","Title":"Randomized allocation with arm elimination in a bandit problem with covariates"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000389809600009 ISBN:978-1-4503-4137-0","Keywords":"Exploratory search; intent modeling; multi-armed bandits; relevance feedback; probabilistic user models","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"PROCEEDINGS OF THE 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES (IUI'16) Pages: 71-75 DOI: 10.1145/2856767.2856803 Published: 2016","Abstract":"In exploratory search, the user starts with an uncertain information need and provides relevance feedback to the system's suggestions to direct the search. The search system learns the user intent based on this feedback and employs it to recommend novel results. However, the amount of user feedback is very limited compared to the size of the information space to be explored. To tackle this problem, we take into account user feedback on both the retrieved items (documents) and their features (keywords). In order to combine feedback from multiple domains, we introduce a coupled multi-armed bandits algorithm, which employs a probabilistic model of the relationship between the domains. Simulation results show that with multidomain feedback, the search system can find the relevant items in fewer iterations than with only one domain. A preliminary user study indicates improvement in user satisfaction and quality of retrieved information.","Authors":"Daee, P (Daee, Pedram) ; Pyykko, J (Pyykko, Joel) ; Glowacka, D (Glowacka, Dorota) ; Kaski, S (Kaski, Samuel) Book Group Author(s):ACM Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Kaski, Samuel  B-6684-2008 http://orcid.org/0000-0003-1925-9154","Title":"Interactive Intent Modeling from Multiple Feedback Domains"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000389809000010 ISBN:978-1-4503-4217-9","Keywords":"Non-stationary multi-armed bandit problem; dynamic pricing problem; UCB (upper confidence bound) algorithms; linear regression; expected regret; sequential decision making","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications","Journal Information":"PROCEEDINGS OF THE THIRD ACM IKDD CONFERENCE ON DATA SCIENCES (CODS) DOI: 10.1145/2888451.2888475 Published: 2016","Abstract":"We first propose an online learning model wherein rewards for different actions/arms used by the user can be correlated and the reward stream can be non-stationary. Thus, this extends the standard multi-armed bandit learning model. We propose two algorthims, Greedy and Regression based UCB, that attempt to minimize the expected regret. We also obtain non-trivial upper bounds for the expected regret through theoretical analysis. We also provide some evidence for sub-polynomial increase in expected regret upon appropriate tuning of algorithm input parameters. These models are motivated by the problem of dynamic pricing of a product faced by a typical online retailer.","Authors":"Mayekar, P (Mayekar, Prathamesh) ; Hemachandra, N (Hemachandra, Nandyala) Book Group Author(s):ACM","Title":"Some algorithms for correlated bandits with non-stationary rewards : Regret bounds and applications"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000389338600015 ISBN:978-3-319-43904-4; 978-3-319-43903-7 ISSN: 0302-9743","Keywords":"Multi-target tracking; Jamming; Nondetections; Random Kalman filter; Index policies; Restless bandits; Whittle index; Indexability KeyWords Plus:MARGINAL PRODUCTIVITY INDEXES; DYNAMIC ALLOCATION INDEXES; TARGET TRACKING; RESTLESS BANDITS; RADAR","Categories":"Computer Science Web of Science Categories:Computer Science, Software Engineering; Computer Science, Theory & Methods","Journal Information":"Analytical and Stochastic Modelling Techniques and Applications Book Series: Lecture Notes in Computer Science Volume: 9845 Pages: 210-222 DOI: 10.1007/978-3-319-43904-4_15 Published: 2016","Abstract":"This paper proposes a tractable priority-index policy based on the Whittle index for multi-target tracking with jamming and nondetections. The policy is to be used by M phased-array radars tracking the positions of N > M targets moving according to independent scalar Gauss-Markov linear dynamics, which allows use of a random Kalman filter for track-error variance updates. The dynamics incorporate random jamming or nondetections decreasing the quality of measurement. The paper exploits the natural problem formulation as a multi-armed restless bandit problem with real-state projects by deploying Whittle's index policy. The issues of indexability (existence of the index) and index evaluation are addressed by deploying a method introduced by the author in earlier work, based on a set of sufficient indexability conditions. Preliminary numerical results are reported providing evidence that such conditions hold and evaluating the index.","Authors":"Nino-Mora, J (Nino-Mora, Jose) Edited by:Wittevrongel, S; PhungDuc, T","Title":"Whittle's Index Policy for Multi-Target Tracking with Jamming and Nondetections"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000389381200060 ISBN:978-3-662-49390-8; 978-3-662-49389-2 ISSN: 0302-9743","Keywords":"Portfolio selection problem; Machine learning; Multi-armed bandit; Derivative-free optimization KeyWords Plus:SELECTION","Categories":"Computer Science; Robotics Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Interdisciplinary Applications; Robotics","Journal Information":"Intelligent Information and Database Systems, ACIIDS 2016, Pt II Book Series: Lecture Notes in Artificial Intelligence Volume: 9622 Pages: 620-629 DOI: 10.1007/978-3-662-49390-8_60 Published: 2016","Abstract":"In this paper, we are interested in studying and solving the portfolio selection problem by means of a machine learning method. Particularly, we use a chi-armed bandit algorithm called Hierarchical Optimistic Optimization (HOO). HOO is an optimization approach that can be used for finding optima of box constrained nonlinear and nonconvex functions. Under some restrictions, such as locally Lipschitz condition, HOO can provide global solutions. Our idea consists in using HOO for solving some NP-hard variants of the portfolio selection problem. We test this approach on some data sets and report the results. In order to verify the quality of the solutions, we compare them with the best known solutions, provided by a derivative-free approach, called DIRECT. The preliminary numerical experiments give promising results.","Authors":"Moeini, M (Moeini, Mahdi) ; Wendt, O (Wendt, Oliver) ; Krumrey, L (Krumrey, Linus) Edited by:Nguyen, NT; Trawinski, B; Fujita, H; Hong, TP","Title":"Portfolio Optimization by Means of a chi-Armed Bandit Algorithm"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000388603101108 ISBN:978-1-4673-9814-5 ISSN: 1525-3511","Keywords":"Energy efficiency; Epoch greedy algorithm; LTE-U; Multi-Armed Bandit; WiFi KeyWords Plus:NETWORKS","Categories":"Telecommunications Web of Science Categories:Telecommunications","Journal Information":"2016 IEEE WIRELESS COMMUNICATIONS AND NETWORKING CONFERENCE Book Series: IEEE Wireless Communications and Networking Conference Published: 2016","Abstract":"In order to cope with the phenomenal growth of mobile data traffic, unlicensed spectrum can be utilized by the Long Term Evolution (LTE) cellular systems. However, ensuring fair coexistence with WiFi is a mandatory requirement. In one approach, periodically configurable transmission gaps can be used to facilitate a coexistence between WiFi and LTE. In this paper, a Multi-Armed Bandit (MAB) based dynamic duty cycle selection method is proposed for configuration of transmission gaps ensuring a better coexistence for both technologies. Then the concept is further strengthened with downlink power control mechanism using the same algorithm leading to a high energy efficiency and interference reduction. Performance results are given for different user equipment and WiFi station densities in which it is shown that significant improvements in overall throughput and energy efficiency can be achieved.","Authors":"Sriyananda, MGS (Sriyananda, M. G. S.) ; Parvez, I (Parvez, Imtiaz) ; Guvenc, I (Guvenc, Ismail) ; Bennis, M (Bennis, Mehdi) ; Sarwat, AI (Sarwat, Arif I.) Book Group Author(s):IEEE","Title":"Multi-Armed Bandit for LTE-U and WiFi Coexistence in Unlicensed Bands"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000389019700026 ISBN:978-3-319-44953-1; 978-3-319-44952-4 ISSN: 0302-9743","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Theory & Methods","Journal Information":"PRINCIPLES AND PRACTICE OF CONSTRAINT PROGRAMMING, CP 2016 Book Series: Lecture Notes in Computer Science Volume: 9892 Pages: 388-404 DOI: 10.1007/978-3-319-44953-1_25 Published: 2016","Abstract":"We consider the problem of selecting the best variable-value strategy for solving a given problem in constraint programming. We show that the recent Embarrassingly Parallel Search method (EPS) can be used for this purpose. EPS proposes to solve a problem by decomposing it in many subproblems and to give them on-demand to workers which run in parallel. Our method uses a sample of these subproblems for comparing strategies in order to select the most promising one to be used for solving the remaining subproblems. Each subproblem of the sample is solved with all the candidate strategies in parallel using a timeout that is twice the time of the best one. The selection of the strategy is then based on the Wilcoxon signed rank test. This test is able to deal with censored data caused by timeouts and makes no assumption on the solving time distribution. The experiments we performed on a set of classical benchmarks for satisfaction and optimization problems show that our method selects most of the time the best strategy. Our method also outperforms the portfolio approach consisting of running some strategies in parallel and is competitive with the multi armed bandit framework.","Authors":"Palmieri, A (Palmieri, Anthony) ; Regin, JC (Regin, Jean-Charles) ; Schaus, P (Schaus, Pierre) Edited by:Rueher, M","Title":"Parallel Strategies Selection"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000380532904013 ISBN:978-1-4503-3362-7","Keywords":"Educational games; optimization; data-driven design; multi-armed bandits; continuous improvement KeyWords Plus:ALLOCATION; SCIENCE","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Computer Science, Theory & Methods","Journal Information":"34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016 Pages: 4142-4153 DOI: 10.1145/2858036.2858425 Published: 2016","Abstract":"\"Multi-armed bandits\" offer a new paradigm for the AI-assisted design of user interfaces. To help designers understand the potential, we present the results of two experimental comparisons between bandit algorithms and random assignment. Our studies are intended to show designers how bandits algorithms are able to rapidly explore an experimental design space and automatically select the optimal design configuration. Our present focus is on the optimization of a game design space. The results of our experiments show that bandits can make data-driven design more efficient and accessible to interface designers, but that human participation is essential to ensure that AI systems optimize for the right metric. Based on our results, we introduce several design lessons that help keep human design judgment in the loop. We also consider the future of human-technology teamwork in AI-assisted design and scientific inquiry. Finally, as bandits deploy fewer low-performing conditions than typical experiments, we discuss ethical implications for bandits in large-scale experiments in education.","Authors":"Lomas, JD (Lomas, J. Derek) ; Forlizzi, J (Forlizzi, Jodi) ; Poonwala, N (Poonwala, Nikhil) ; Patel, N (Patel, Nirmal) ; Shodhan, S (Shodhan, Sharan) ; Patel, K (Patel, Kishan) ; Koedinger, K (Koedinger, Ken) ; Brunskill, E (Brunskill, Emma) Book Group Author(s):ACM","Title":"Interface Design Optimization as a Multi-Armed Bandit Problem"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000387962100004 ISBN:978-3-319-45823-6; 978-3-319-45822-9 ISSN: 0302-9743","Keywords":"Software project scheduling; Hyper-heuristics; Adaptive operator selection; Sliding multi-armed bandit KeyWords Plus:ALGORITHM","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Theory & Methods","Journal Information":"PARALLEL PROBLEM SOLVING FROM NATURE - PPSN XIV Book Series: Lecture Notes in Computer Science Volume: 9921 Pages: 37-47 DOI: 10.1007/978-3-319-45823-6_4 Published: 2016","Abstract":"Software project scheduling plays an important role in reducing the cost and duration of software projects. It is an NP-hard combinatorial optimization problem that has been addressed based on single and multi-objective algorithms. However, such algorithms have always used fixed genetic operators, and it is unclear which operators would be more appropriate across the search process. In this paper, we propose an evolutionary hyper-heuristic to solve the software project scheduling problem. Our novelties include the following: (1) this is the first work to adopt an evolutionary hyper-heuristic for the software project scheduling problem; (2) this is the first work for adaptive selection of both crossover and mutation operators; (3) we design different credit assignment methods for mutation and crossover; and (4) we use a sliding multi-armed bandit strategy to adaptively choose both crossover and mutation operators. The experimental results show that the proposed algorithm can solve the software project scheduling problem effectively.","Authors":"Wu, XL (Wu, Xiuli) ; Consoli, P (Consoli, Pietro) ; Minku, L (Minku, Leandro) ; Ochoa, G (Ochoa, Gabriela) ; Yao, X (Yao, Xin) Edited by:Handl, J; Hart, E; Lewis, PR; LopezIbanez, M; Ochoa, G; Paechter, B","Title":"An Evolutionary Hyper-heuristic for the Software Project Scheduling Problem"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000387430600009 ISBN:978-3-319-40970-2; 978-3-319-40969-6 ISSN: 0302-9743","Categories":"Computer Science; Science & Technology - Other Topics Web of Science Categories:Computer Science, Theory & Methods; Logic","Journal Information":"THEORY AND APPLICATIONS OF SATISFIABILITY TESTING - SAT 2016 Book Series: Lecture Notes in Computer Science Volume: 9710 Pages: 123-140 DOI: 10.1007/978-3-319-40970-2_9 Published: 2016","Abstract":"In this paper, we propose a framework for viewing solver branching heuristics as optimization algorithms where the objective is to maximize the learning rate, defined as the propensity for variables to generate learnt clauses. By viewing online variable selection in SAT solvers as an optimization problem, we can leverage a wide variety of optimization algorithms, especially from machine learning, to design effective branching heuristics. In particular, we model the variable selection optimization problem as an online multi-armed bandit, a special-case of reinforcement learning, to learn branching variables such that the learning rate of the solver is maximized. We develop a branching heuristic that we call learning rate branching or LRB, based on a well-known multi-armed bandit algorithm called exponential recency weighted average and implement it as part of MiniSat and CryptoMiniSat. We upgrade the LRB technique with two additional novel ideas to improve the learning rate by accounting for reason side rate and exploiting locality. The resulting LRB branching heuristic is shown to be faster than the VSIDS and conflict history-based (CHB) branching heuristics on 1975 application and hard combinatorial instances from 2009 to 2014 SAT Competitions. We also show that CryptoMiniSat with LRB solves more instances than the one with VSIDS. These experiments show that LRB improves on state-of-the-art.","Authors":"Liang, JH (Liang, Jia Hui) ; Ganesh, V (Ganesh, Vijay) ; Poupart, P (Poupart, Pascal) ; Czarnecki, K (Czarnecki, Krzysztof) Edited by:Creignou, N; LeBerre, D","Title":"Learning Rate Based Branching Heuristic for SAT Solvers"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000386654000036 ISBN:978-1-5090-2061-4 ISSN: 2154-0217","Categories":"Engineering; Telecommunications Web of Science Categories:Engineering, Electrical & Electronic; Telecommunications","Journal Information":"2016 13TH INTERNATIONAL SYMPOSIUM ON WIRELESS COMMUNICATION SYSTEMS (ISWCS) Book Series: International Symposium on Wireless Communication Systems Pages: 194-198 Published: 2016","Abstract":"In this paper, a novel approach for improving channel prediction is proposed. Its main novelty consists in optimizing the post processing of acquired measurements intended to assess channel quality. To this end, a Multi-Armed Bandit based algorithm is designed to choose in an adaptive and online manner the optimal filtering parameters, taking into account the target usage in the network as well as the employed prediction algorithm. Performance evaluation is made following 3GPP assumptions and compared to conventional fixed filtering operation. Substantial gains in terms of prediction accuracy and UE throughput are obtained, especially in cases where the user context is fast varying.","Authors":"Feki, A (Feki, Afef) ; Amara, M (Amara, Mustapha) ; Mekki, S (Mekki, Sami) Book Group Author(s):IEEE","Title":"Measurement Model Optimization for Channel Prediction Improvement in Wireless Networks"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000382226300048 ISBN:978-1-4503-3936-0","Keywords":"Online Learning; Mechanism Design; Ad Auctions","Categories":"Computer Science; Business & Economics Web of Science Categories:Computer Science, Interdisciplinary Applications; Economics","Journal Information":"EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION Pages: 597-598 DOI: 10.1145/2940716.2940771 Published: 2016","Abstract":"Ad Exchange platforms connect online publishers and advertisers and facilitate selling billions of impressions every day. We study these environments from the perspective of a publisher who wants to find the profit maximizing exchange to sell his inventory. Ideally, the publisher would run an auction among exchanges. However, this is not possible due to technological and other practical considerations. The publisher needs to send each impression to one of the exchanges with an asking price. We model the problem as a variation of multi-armed bandits where exchanges (arms) can behave strategically in order to maximizes their own profit. We propose mechanisms that find the best exchange with sub-linear regret and have desirable incentive properties.","Authors":"Nazerzadeh, H (Nazerzadeh, Hamid) ; Leme, RP (Leme, Renato Paes) ; Rostamizadeh, A (Rostamizadeh, Afshin) ; Syed, U (Syed, Umar) Book Group Author(s):ACM","Title":"Where to Sell: Simulating Auctions From Learning Algorithms"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000385793700066 ISBN:978-1-61499-672-9; 978-1-61499-671-2 ISSN: 0922-6389","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"ECAI 2016: 22ND EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE Book Series: Frontiers in Artificial Intelligence and Applications Volume: 285 Pages: 560-568 DOI: 10.3233/978-1-61499-672-9-560 Published: 2016","Abstract":"Multi-Armed Bandits (MABs) have been widely considered in the last decade to model settings in which an agent wants to learn the action providing the highest expected reward among a fixed set of available actions during the operational life of a system. Classical techniques provide solutions that minimize the regret due to learning in settings where selecting an arm has no cost. Though, in many real world applications the learner has to pay some cost for pulling each arm and the learning process is constrained by a fixed budget B. This problem is addressed in the literature as the Budgeted MAB (BMAB). In this paper, for the first time, we study the problem of Budgeted Continuous-Armed Bandit (BCAB), where the set of the possible actions consists in a continuous set (e.g., a range of prices) and the learner suffers from a random reward and cost at each round. We provide a novel algorithm, named B-Zoom, which suffers a regret of (O) over tilde (Bd+1/d+2), where d is the Zooming dimension of the problem. Finally, we provide an empirical analysis showing that, despite a lower average performance, the proposed approach is more robust to adverse settings as compared to existing algorithms designed for BMAB.","Authors":"Trovo, F (Trovo, Francesco) ; Paladino, S (Paladino, Stefano) ; Restelli, M (Restelli, Marcello) ; Gatti, N (Gatti, Nicola) Edited by:Kaminka, GA; Fox, M; Bouquet, P; Hullermeier, E; Dignum, V; Dignum, F; VanHarmelen, F","Title":"Budgeted Multi-Armed Bandit in Continuous Action Space"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000379883800030 ISBN:978-0-7918-5708-3","Categories":"Engineering; Operations Research & Management Science Web of Science Categories:Engineering, Industrial; Engineering, Mechanical; Operations Research & Management Science","Journal Information":"INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE, 2015, VOL 2B Article Number: V02BT03A030 Published: 2016","Abstract":"Heuristics and meta-heuristics are often used to solve complex real-world problems such as the non-linear, non convex, and multi-objective combinatorial optimization problems that regularly appear in system design and architecture. Unfortunately, the performance of a specific heuristic is largely dependent on the specific problem at hand. Moreover, a heuristic's performance can vary throughout the optimization process. Hyper-heuristics is one approach that can maintain relatively good performance over the course of an optimization process and across a variety of problems without parameter retuning or major modifications. Given a set of domain-specific and domain-independent heuristics, a hyper heuristic adapts its search strategy over time by selecting the most promising heuristics to use at a given point. A hyper-heuristic must have: 1) a credit assignment strategy to rank the heuristics by their likelihood of producing improving solutions; and 2) a heuristic selection strategy based on the credits assigned to each heuristic. The literature contains many examples of hyper-heuristics with effective credit assignment and heuristic selection strategies for single-objective optimization problems. In multi-objective optimization problems, however, defining credit is less straightforward because there are often competing objectives. Therefore, there is a need to define and assign credit so that heuristics are rewarded for finding solutions with good trades between the objectives. This paper studies, for the first time, different combinations of credit definition, credit aggregation, and heuristic selection strategies. Credit definitions are based on different applications of the notion of Pareto dominance, namely: A1) dominance of the offspring with respect to the parent solutions; A2) ability to produce non-dominated solutions with respect to the entire population; A3) Pareto ranking with respect to the entire population. Two different credit aggregation strategies for assigning credit are also examined. A heuristic will receive credit for: B1) only the solutions it created in the current iteration or B2) all solutions it created that are in the current population. Different heuristic selection strategies are considered including: C1) probability matching; C2) dynamic multi-armed bandit; and C3) Hyper-GA. Thus, we conduct an experiment with three factors: credit definition (A1, A2, A3), credit aggregation (B1, B2), and heuristic selection (Cl, C2, C3) and conduct a full factorial experiment. Performance is measured by hyper-volume of the last population. All algorithms are tested on a design problem for a climate monitoring satellite constellation instead of classical benchmarking problems to apply domain-specific heuristics within the hyper-heuristic.","Authors":"Hitomi, N (Hitomi, Nozomi) ; Selva, D (Selva, Daniel) Book Group Author(s):ASME","Title":"THE EFFECT OF CREDIT DEFINITION AND AGGREGATION STRATEGIES ON MULTI-OBJECTIVE HYPER-HEURISTICS"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000378745100037 ISBN:978-3-319-30000-9; 978-3-319-29999-0 ISSN: 0302-9743","Keywords":"Algorithmic learning; Online learning; Bandit problem","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Theory & Methods","Journal Information":"LANGUAGE AND AUTOMATA THEORY AND APPLICATIONS, LATA 2016 Book Series: Lecture Notes in Computer Science Volume: 9618 Pages: 412-423 DOI: 10.1007/978-3-319-30000-9_32 Published: 2016","Abstract":"We study the loss version of adversarial multi-armed bandit problems with one lossless arm. We show an adversary's strategy that forces any player to suffer K - 1 - O(1/T) loss where K is the number of arms and T is the number of rounds.","Authors":"Nakamura, A (Nakamura, Atsuyoshi) ; Helmbold, DP (Helmbold, David P.) ; Warmuth, MK (Warmuth, Manfred K.) Edited by:Dediu, AH; Janousek, J; MartinVide, C; Truthe, B","Title":"Noise Free Multi-armed Bandit Game"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000377358200087 ISBN:978-0-7695-5670-3 ISSN: 1060-3425","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Information Systems; Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"2016 49TH HAWAII INTERNATIONAL CONFERENCE ON SYSTEM SCIENCES (HICSS) Book Series: Proceedings of the Annual Hawaii International Conference on System Sciences Pages: 726-734 DOI: 10.1109/HICSS.2016.95 Published: 2016","Abstract":"Search engine marketing provided by search engines enable companies to promote their products to internet users based on their queries is now a major online advertising channel. In most search-based advertising services, advertisers could have dozens of keywords for the same product or service, and in most instances, the reward of each keyword is unknown. Identifying the most profitable keywords becomes challenge for advertisers. In this research, we develop a new model named POKER based on multi-armed bandit problem to help solving this keyword choosing problem, we compare POKER with three currently frequently-used keyword selecting strategies by simulation. It turns out that POKER's aggregate performance overmatches other strategies.","Authors":"Qin, J (Qin, Juan) ; Qi, W (Qi, Wei) ; Zhou, BJ (Zhou, Baojian) Edited by:Bui, TX; Sprague, RH","Title":"RESEARCH ON OPTIMAL SELECTION STRATEGY OF SEARCH ENGINE KEYWORDS BASED ON MULTI-ARMED BANDIT"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000371620200016 ISSN: 2372-0468 eISSN: 2372-0484","Keywords":"natural computing; atomic switch; tug-of-war dynamics; amoeba-inspired computing; multi-armed bandit problem; reinforcement learning KeyWords Plus:MULTIARMED BANDIT PROBLEM; OF-WAR MODEL; MEDIUM ACCESS; OPTIMIZATION; EXPLORATION","Categories":"Materials Science Web of Science Categories:Materials Science, Multidisciplinary","Journal Information":"AIMS MATERIALS SCIENCE Volume: 3 Issue: 1 Pages: 245-259 DOI: 10.3934/matersci.2016.1.245 Published: 2016","Abstract":"We propose a simple model for an atomic switch-based decision maker (ASDM), and show that, as long as its total number of metal atoms is conserved when coupled with suitable operations, an atomic switch system provides a sophisticated \"decision-making\" capability that is known to be one of the most important intellectual abilities in human beings. We considered a popular decision-making problem studied in the context of reinforcement learning, the multi-armed bandit problem (MAB); the problem of finding, as accurately and quickly as possible, the most profitable option from a set of options that gives stochastic rewards. These decisions are made as dictated by each volume of precipitated metal atoms, which is moved in a manner similar to the fluctuations of a rigid body in a tug-of-war game. The \"tug-of-war (TOW) dynamics\" of the ASDM exhibits higher efficiency than conventional reinforcement-learning algorithms. We show analytical calculations that validate the statistical reasons for the ASDM to produce such high performance, despite its simplicity. Efficient MAB solvers are useful for many practical applications, because MAB abstracts a variety of decision-making problems in real-world situations where an efficient trial-and-error is required. The proposed scheme will open up a new direction in physics-based analog-computing paradigms, which will include such things as \"intelligent nanodevices\" based on self-judgment.","Authors":"Kim, SJ (Kim, Song-Ju) ; Tsuruoka, T (Tsuruoka, Tohru) ; Hasegawa, T (Hasegawa, Tsuyoshi) ; Aono, M (Aono, Masashi) ; Terabe, K (Terabe, Kazuya) ; Aono, M (Aono, Masakazu)","Title":"Decision maker based on atomic switches"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000370578900001 ISSN: 1076-9757 eISSN: 1943-5037","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH Volume: 55 Pages: 317-359 Published: 2016","Abstract":"Crowdsourcing markets have emerged as a popular platform for matching available workers with tasks to complete. The payment for a particular task is typically set by the task's requester, and may be adjusted based on the quality of the completed work, for example, through the use of \"bonus\" payments. In this paper, we study the requester's problem of dynamically adjusting quality-contingent payments for tasks. We consider a multi-round version of the well-known principal-agent model, whereby in each round a worker makes a strategic choice of the effort level which is not directly observable by the requester. In particular, our formulation significantly generalizes the budget-free online task pricing problems studied in prior work. We treat this problem as a multi-armed bandit problem, with each \"arm\" representing a potential contract. To cope with the large (and in fact, infinite) number of arms, we propose a new algorithm, AgnosticZooming, which discretizes the contract space into a finite number of regions, effectively treating each region as a single arm. This discretization is adaptively refined, so that more promising regions of the contract space are eventually discretized more finely. We analyze this algorithm, showing that it achieves regret sublinear in the time horizon and substantially improves over non-adaptive discretization (which is the only competing approach in the literature). Our results advance the state of art on several different topics: the theory of crowdsourcing markets, principal-agent problems, multi-armed bandits, and dynamic pricing.","Authors":"Ho, CJ (Ho, Chien-Ju) ; Slivkins, A (Slivkins, Aleksandrs) ; Vaughan, JW (Vaughan, Jennifer Wortman)","Title":"Adaptive Contract Design for Crowdsourcing Markets: Bandit Algorithms for Repeated Principal-Agent Problems"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000367399100001 PubMed ID: 27212781 ISSN: 0269-9648 eISSN: 1469-8951","Categories":"Engineering; Operations Research & Management Science; Mathematics Web of Science Categories:Engineering, Industrial; Operations Research & Management Science; Statistics & Probability","Journal Information":"PROBABILITY IN THE ENGINEERING AND INFORMATIONAL SCIENCES Volume: 30 Issue: 1 Pages: 1-23 DOI: 10.1017/S026996481500025X Published: JAN 2016","Abstract":"Motivated by a class of Partially Observable Markov Decision Processes with application in surveillance systems in which a set of imperfectly observed state processes is to be inferred from a subset of available observations through a Bayesian approach, we formulate and analyze a special family of multi-armed restless bandit problems. We consider the problem of finding an optimal policy for observing the processes that maximizes the total expected net rewards over an infinite time horizon subject to the resource availability. From the Lagrangian relaxation of the original problem, an index policy can be derived, as long as the existence of the Whittle index is ensured. We demonstrate that such a class of reinitializing bandits in which the projects' state deteriorates while active and resets to its initial state when passive until its completion possesses the structural property of indexability and we further show how to compute the index in closed form. In general, the Whittle index rule for restless bandit problems does not achieve optimality. However, we show that the proposed Whittle index rule is optimal for the problem under study in the case of stochastically heterogenous arms under the expected total criterion, and it is further recovered by a simple tractable rule referred to as the 1-limited Round Robin rule. Moreover, we illustrate the significant suboptimality of other widely used heuristic: the Myopic index rule, by computing in closed form its suboptimality gap. We present numerical studies which illustrate for the more general instances the performance advantages of the Whittle index rule over other simple heuristics.","Authors":"Villar, SS (Villar, Sofia S.)","Title":"INDEXABILITY AND OPTIMAL INDEX POLICIES FOR A CLASS OF REINITIALISING RESTLESS BANDITS"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000366805900018 ISSN: 1568-4946 eISSN: 1872-9681","Keywords":"Adaptive operator selection; Operator based algorithms; Multi armed bandit; Island models KeyWords Plus:BANDIT PROBLEM; EVOLUTIONARY ALGORITHMS","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications","Journal Information":"APPLIED SOFT COMPUTING Volume: 38 Pages: 257-268 DOI: 10.1016/j.asoc.2015.09.024 Published: JAN 2016","Abstract":"In this paper, we propose new scenarios for simulating search operators whose behaviors often change continuously during the search. In these scenarios, the performance of such operators decreases while they are applied. This is motivated by the fact that operators for optimization problems are often roughly classified into exploitation and exploration operators. Our simulation model is used to compare the performances of operator selection policies and to identify their ability to handle specific non-stationary operators. An experimental study highlights respective behaviors of operator selection policies when faced to such non-stationary search scenarios. (C) 2015 Elsevier B.V. All rights reserved.","Authors":"Goeffon, A (Goeffon, Adrien) ; Lardeux, F (Lardeux, Frederic) ; Saubion, F (Saubion, Frederic) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Lardeux, Frederic  http://orcid.org/0000-0001-8636-3870","Title":"Simulating non-stationary operators in search algorithms"}]