[{"Document Information":"Document Type:Article Language:English Accession Number: WOS:000391296100006 ISSN: 1053-587X eISSN: 1941-0476","Keywords":"Ensemble learning; meta-learning; online learning; regret; confidence bound; multi-armed bandits; contextual bandits; medical informatics KeyWords Plus:MULTIAGENT OPTIMIZATION; ALGORITHMS; CONSENSUS; REGRESSION; NETWORKS; EXPERT","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON SIGNAL PROCESSING Volume: 65 Issue: 4 Pages: 888-903 DOI: 10.1109/TSP.2016.2626250 Published: FEB 15 2017","Abstract":"Extracting actionable intelligence from distributed, heterogeneous, correlated, and high-dimensional data sources requires run-time processing and learning both locally and globally. In the last decade, a large number of meta-learning techniques have been proposed in which local learners make online predictions based on their locally collected data instances, and feed these predictions to an ensemble learner, which fuses them and issues a global prediction. However, most of these works do not provide performance guarantees or, when they do, these guarantees are asymptotic. None of these existing works provide confidence estimates about the issued predictions or rate of learning guarantees for the ensemble learner. In this paper, we provide a systematic ensemble learning method called Hedged Bandits, which comes with both long-run (asymptotic) and short-run (rate of learning) performance guarantees. Moreover, our approach yields performance guarantees with respect to the optimal local prediction strategy, and is also able to adapt its predictions in a data-driven manner. We illustrate the performance of Hedged Bandits in the context of medical informatics and show that it outperforms numerous online and offline ensemble learning methods.","Authors":"Tekin, C (Tekin, Cem) ; Yoon, J (Yoon, Jinsung) ; van der Schaar, M (van der Schaar, Mihaela)","Title":"Adaptive Ensemble Learning With Confidence Bounds"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395736700041 ISSN: 1089-7798 eISSN: 1558-2558","Keywords":"Contextual bandit; underwater cooperative communication; relay selection","Categories":"Telecommunications Web of Science Categories:Telecommunications","Journal Information":"IEEE COMMUNICATIONS LETTERS Volume: 21 Issue: 2 Pages: 382-385 DOI: 10.1109/LCOMM.2016.2625300 Published: FEB 2017","Abstract":"Cooperative relay transmission is an attractive architecture for underwater acoustic networks. However, designing relay selection policies in the harsh underwater environment is difficult. In this letter, we model relay selection as a contextual bandit problem-an important extension of multi-armed bandit. Through this way, we can achieve relay selection based on a bit of contextual communication environment information about relay nodes instead of instantaneous or statistical channel state information. Our proposed relay selection technique enables highly stable performance of the cooperative system in a complex and changeable underwater environment, and the process of relay selection can be simplified and easily facilitate efficient cooperative transmission. Simulation results illustrate the effectiveness and the robustness of this relay selection technique.","Authors":"Li, XB (Li, Xinbin) ; Liu, JJ (Liu, Jiajia) ; Yan, L (Yan, Lei) ; Han, S (Han, Song) ; Guan, XP (Guan, Xinping)","Title":"Relay Selection in Underwater Acoustic Cooperative Networks: A Contextual Bandit Approach"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000398020500045 ISSN: 1877-0509","Keywords":"maintenance decision support system; pattern classification; statistical process control; contextual multi-armed bandit","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Theory & Methods","Journal Information":"XII INTERNATIONAL SYMPOSIUM INTELLIGENT SYSTEMS 2016, (INTELS 2016) Book Series: Procedia Computer Science Volume: 103 Pages: 316-323 DOI: 10.1016/j.procs.2017.01.114 Published: 2017","Abstract":"In this paper we focus on two essential problems of maintenance decision support systems, namely, 1) detection of potential dangerous situation, and 2) classification of this situation in order to recommend an appropriate repair action. The former task is usually solved with the known statistical process control techniques. The latter problem can be reduced to the contextual multi armed bandit problem. We propose a novel algorithm with Bayesian classification of abnormal situation and the softmax rule to explore the decision space. The dangerous situations are detected with the Shewhart control charts for the distances between the current and the normal situations. It is experimentally shown, that our algorithm is more accurate than the known contextual multi-armed methods with stochastic search strategies. (C) 2017 The Authors. Published by Elsevier B. V.","Authors":"Savchenko, AV (Savchenko, A. V.) ; Milov, VR (Milov, V. R.) Edited by:Askhat, D; Ivan, Z; Andrew, K; Eugeny, N","Title":"Decision support in intelligent maintenance-planning systems based on contextual multi-armed bandit algorithm"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000382677100012 ISSN: 1053-587X eISSN: 1941-0476","Keywords":"Personalized education; course sequence recommendation; dynamic programming; contextual bandits","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON SIGNAL PROCESSING Volume: 64 Issue: 20 Pages: 5340-5352 DOI: 10.1109/TSP.2016.2595495 Published: OCT 15 2016","Abstract":"Given the variability in student learning, it is becoming increasingly important to tailor courses as well as course sequences to student needs. This paper presents a systematic methodology for offering personalized course sequence recommendations to students. First, a forward-search backward-induction algorithm is developed that can optimally select course sequences to decrease the time required for a student to graduate. The algorithm accounts for prerequisite requirements (typically present in higher level education) and course availability. Second, using the tools of multiarmed bandits, an algorithm is developed that can optimally recommend a course sequence that both reduces the time to graduate while also increasing the overall GPA of the student. The algorithm dynamically learns how students with different contextual backgrounds perform for given course sequences and, then, recommends an optimal course sequence for new students. Using real-world student data from the UCLA Mechanical and Aerospace Engineering Department, we illustrate how the proposed algorithms outperform other methods that do not include student contextual information when making course sequence recommendations.","Authors":"Xu, J (Xu, Jie) ; Xing, TW (Xing, Tianwei) ; van der Schaar, M (van der Schaar, Mihaela)","Title":"Personalized Course Sequence Recommendations"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000383822700010 ISSN: 0304-3975 eISSN: 1879-2294","Keywords":"MCTS; Multi-armed bandit problem; Contextual bandit; LinUCB KeyWords Plus:COMPUTER GO","Categories":"Computer Science Web of Science Categories:Computer Science, Theory & Methods","Journal Information":"THEORETICAL COMPUTER SCIENCE Volume: 644 Pages: 114-126 DOI: 10.1016/j.tcs.2016.06.035 Published: SEP 6 2016","Abstract":"UCT is a standard method of Monte Carlo tree search (MCTS) algorithms, which have been applied to various domains and have achieved remarkable success. This study proposes a family of LinUCT algorithms that incorporate LinUCB into MCTS algorithms. LinUCB is a recently developed method that generalizes past episodes by ridge regression with feature vectors and rewards. LinUCB outperforms UCB1 in contextual multi-armed bandit problems. We introduce a straightforward application of LinUCB, LinUCTPLAIN by substituting UCB1 with LinUCB in UCT. We show that it does not work well owing to the minimax structure of game trees. To better handle such tree structures, we present LinUCTaAve and LinUCTFp by further incorporating two existing techniques, rapid action value estimation (RAVE) and feature propagation, which recursively propagates the feature vector of a node to that of its parent. Experiments were conducted with a synthetic model, which is an extension of the standard incremental random tree model in which each node has a feature vector that represents the characteristics of the corresponding position, and Finnsson's shock step game which is used to empirically analyze the performance of UCT with respect to the distribution of suboptimal moves. The experiments results indicate that LinUCTRAve and LinUCTFp outperform UCT, especially when the branching factor is relatively large. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Mandai, Y (Mandai, Yusaku) ; Kaneko, T (Kaneko, Tomoyuki)","Title":"LinUCB applied to Monte Carlo tree search"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000381437700005 ISSN: 1932-4553 eISSN: 1941-0484","Keywords":"Activity classification; context-aware; online learning; active learning; multi-armed bandits KeyWords Plus:ACTIVITY RECOGNITION; BANDITS; ALGORITHMS; STROKE","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING Volume: 10 Issue: 5 Pages: 865-876 DOI: 10.1109/JSTSP.2016.2553648 Published: AUG 2016","Abstract":"Enabling accurate and low-cost classification of a range of motion activities is important for numerous applications, ranging from disease treatment and in-community rehabilitation of patients to athlete training. This paper proposes a novel contextual online learning method for activity classification based on data captured by low-cost, body-worn inertial sensors, and smartphones. The proposed method is able to address the unique challenges arising in enabling online, personalized and adaptive activity classification without requiring training phase from the individual. Another key challenge of activity classification is that the labels may change over time, as the data as well as the activity to be monitored evolve continuously, and the true label is often costly and difficult to obtain. The proposed algorithm is able to actively learn when to ask for the true label by assessing the benefits and costs of obtaining them. We rigorously characterize the performance of the proposed learning algorithm and Our experiments show that the proposed algorithm outperforms existing algorithms.","Authors":"Xu, J (Xu, Jie) ; Song, LQ (Song, Linqi) ; Xu, JY (Xu, James Y.) ; Pottie, GJ (Pottie, Gregory J.) ; van der Schaar, M (van der Schaar, Mihaela)","Title":"Personalized Active Learning for Activity Classification Using Wireless Wearable Sensors"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000377928700009 ISSN: 1939-1374","Keywords":"Recommender systems; online learning; clustering algorithms; multi-armed bandit KeyWords Plus:RESTLESS MULTIARMED BANDIT; REGRET","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems; Computer Science, Software Engineering","Journal Information":"IEEE TRANSACTIONS ON SERVICES COMPUTING Volume: 9 Issue: 3 Pages: 433-445 DOI: 10.1109/TSC.2014.2365795 Published: MAY-JUN 2016","Abstract":"In this paper, we propose a novel large-scale, context-aware recommender system that provides accurate recommendations, scalability to a large number of diverse users and items, differential services, and does not suffer from \"cold start\" problems. Our proposed recommendation system relies on a novel algorithm which learns online the item preferences of users based on their click behavior, and constructs online item-cluster trees. The recommendations are then made by choosing an item-cluster level and then selecting an item within that cluster as a recommendation for the user. This approach is able to significantly improve the learning speed when the number of users and items is large, while still providing high recommendation accuracy. Each time a user arrives at the website, the system makes a recommendation based on the estimations of item payoffs by exploiting past context arrivals in a neighborhood of the current user's context. It exploits the similarity of contexts to learn how to make better recommendations even when the number and diversity of users and items is large. This also addresses the cold start problem by using the information gained from similar users and items to make recommendations for new users and items. We theoretically prove that the proposed algorithm for item recommendations converges to the optimal item recommendations in the long-run. We also bound the probability of making a suboptimal item recommendation for each user arriving to the system while the system is learning. Experimental results show that our approach outperforms the state-of-the-art algorithms by over 20 percent in terms of click through rates.","Authors":"Song, LQ (Song, Linqi) ; Tekin, C (Tekin, Cem) ; van der Schaar, M (van der Schaar, Mihaela)","Title":"Online Learning in Large-Scale Contextual Recommender Systems"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000376283900019 PubMed ID: 25807575 ISSN: 2168-2194","Keywords":"Breast cancer; computer-aided diagnosis system; contextual learning; multiarmed bandit (MAB); online learning KeyWords Plus:COMPUTER-AIDED DIAGNOSIS; FEATURE SUBSET-SELECTION; BI-RADS; MAMMOGRAPHY; SYSTEM; MASSES; RATES; CLASSIFICATION; RADIOLOGISTS; ENHANCEMENT","Categories":"Computer Science; Mathematical & Computational Biology; Medical Informatics Web of Science Categories:Computer Science, Information Systems; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Medical Informatics","Journal Information":"IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS Volume: 20 Issue: 3 Pages: 902-914 DOI: 10.1109/JBHI.2015.2414934 Published: MAY 2016","Abstract":"Clinicians need to routinely make management decisions about patients who are at risk for a disease such as breast cancer. This paper presents a novel clinical decision support tool that is capable of helping physicians make diagnostic decisions. We apply this support system to improve the specificity of breast cancer screening and diagnosis. The system utilizes clinical context (e.g., demographics, medical history) to minimize the false positive rates while avoiding false negatives. An online contextual learning algorithm is used to update the diagnostic strategy presented to the physicians over time. We analytically evaluate the diagnostic performance loss of the proposed algorithm, in which the true patient distribution is not known and needs to be learned, as compared with the optimal strategy where all information is assumed known, and prove that the false positive rate of the proposed learning algorithm asymptotically converges to the optimum. In addition, our algorithm also has the important merit that it can provide individualized confidence estimates about the accuracy of the diagnosis recommendation. Moreover, the relevancy of contextual features is assessed, enabling the approach to identify specific contextual features that provide the most value of information in reducing diagnostic errors. Experiments were conducted using patient data collected at a large academic medical center. Our proposed approach outperforms the current clinical practice by 36% in terms of false positive rate given a 2% false negative rate.","Authors":"Song, LQ (Song, Linqi) ; Hsu, W (Hsu, William) ; Xu, J (Xu, Jie) ; van der Schaar, M (van der Schaar, Mihaela)","Title":"Using Contextual Learning to Improve Diagnostic Accuracy: Application in Breast Cancer Screening"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000391662700001 ISSN: 1532-4435","Keywords":"contextual bandit problem; exploration-exploitation tradeoff; nonparametric regression; regret bound; upper confidence bound KeyWords Plus:SLICED INVERSE REGRESSION; DIMENSION REDUCTION; RATES; CONVERGENCE; ALLOCATION","Categories":"Automation & Control Systems; Computer Science Web of Science Categories:Automation & Control Systems; Computer Science, Artificial Intelligence","Journal Information":"JOURNAL OF MACHINE LEARNING RESEARCH Volume: 17 Article Number: 149 Published: 2016","Abstract":"Multi-armed bandit problem is an important optimization game that requires an exploration-exploitation tradeoff to achieve optimal total reward. Motivated from industrial applications such as online advertising and clinical research, we consider a setting where the rewards of bandit machines are associated with covariates, and the accurate estimation of the corresponding mean reward functions plays an important role in the performance of allocation rules. Under a flexible problem setup, we establish asymptotic strong consistency and perform a finite-time regret analysis for a sequential randomized allocation strategy based on kernel estimation. In addition, since many nonparametric and parametric methods in supervised learning may be applied to estimating the mean reward functions but guidance on how to choose among them is generally unavailable, we propose a model combining allocation strategy for adaptive performance. Simulations and a real data evaluation are conducted to illustrate the performance of the proposed allocation strategy.","Authors":"Qian, W (Qian, Wei) ; Yang, YH (Yang, Yuhong)","Title":"Kernel Estimation and Model Combination in A Bandit Problem with Covariates"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390993205094 ISBN:978-1-4799-6664-6 ISSN: 1550-3607","Categories":"Engineering; Telecommunications Web of Science Categories:Engineering, Electrical & Electronic; Telecommunications","Journal Information":"2016 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS (ICC) Book Series: IEEE International Conference on Communications Published: 2016","Abstract":"A promising architecture for content caching in wireless small cell networks is storing popular files at small base stations (sBSs) with limited storage capacities. Using localized communication, an sBS serves local user requests, while reducing the load on the macro cellular network. The sBS should cache the most popular files to maximize the number of cache hits. Content popularity is described by a popularity profile containing the expected demand of each file. Assuming a fixed popularity profile of which the sBS has complete knowledge, the optimal content placement problem reduces to ranking the files according to their expected demands and caching the highest ranked ones. Instead, we assume that the popularity profile is varying, for example depending on fluctuating types of users in the vicinity of the sBS, and it is unknown a priori. We present a novel algorithm based on contextual multi-armed bandits, in which the sBS regularly updates its cache content and observes the demands for cached files in different contexts, thereby learning context-dependent popularity profiles over time. We derive a sub-linear regret bound, proving that our algorithm learns smart caching. Our numerical results confirm that by exploiting contextual information, our algorithm outperforms reference algorithms in various scenarios.","Authors":"Mueller, S (Mueller, Sabrina) ; Atan, O (Atan, Onur) ; van der Schaar, M (van der Schaar, Mihaela) ; Klein, A (Klein, Anja) Book Group Author(s):IEEE","Title":"Smart Caching in Wireless Small Cell Networks via Contextual Multi-Armed Bandits"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390890800168 ISBN:978-1-4503-4073-1","Keywords":"Contextual bandits; latent feature learning; online recommendations; regret analysis KeyWords Plus:ALGORITHM","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT Pages: 1633-1642 DOI: 10.1145/2983323.2983847 Published: 2016","Abstract":"Contextual bandit algorithms provide principled online learning solutions to find optimal trade-offs between exploration and exploitation with companion side-information. Most contextual bandit algorithms simply assume the learner would have access to the entire set of features, which govern the generation of payoffs from a user to an item. However, in practice it is challenging to exhaust all relevant features ahead of time, and oftentimes due to privacy or sampling constraints many factors are unobservable to the algorithm. Failing to model such hidden factors leads a system to make constantly suboptimal predictions. In this paper, we propose to learn the hidden features for contextual bandit algorithms. Hidden features are explicitly introduced in our reward generation assumption, in addition to the observable contextual features. A scalable bandit algorithm is achieved via coordinate descent, in which closed form solutions exist at each iteration for both hidden features and bandit parameters. Most importantly, we rigorously prove that the developed contextual bandit algorithm achieves a sublinear upper regret bound with high probability, and a linear regret is inevitable if one fails to model such hidden features. Extensive experimentation on both simulations and large-scale real-world datasets verified the advantages of the proposed algorithm compared with several state-of-the-art contextual bandit algorithms and existing ad-hoc combinations between bandit algorithms and matrix factorization methods.","Authors":"Wang, HZ (Wang, Huazheng) ; Wu, QY (Wu, Qingyun) ; Wang, HN (Wang, Hongning) Book Group Author(s):ACM","Title":"Learning Hidden Features for Contextual Bandits"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390890800169 ISBN:978-1-4503-4073-1","Keywords":"Online Learning to Rank; Dual-Point Dueling Bandit Gradient Descent; Multi-Point Deterministic Gradient Descent; Interleaved Comparison","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT Pages: 1643-1652 DOI: 10.1145/2983323.2983774 Published: 2016","Abstract":"With the rapid development of information retrieval (IR) systems, online learning to rank (OLR) approaches, which allow retrieval systems to automatically learn best parameters from user interactions, have attracted great research interests in recent years. In OLR, the algorithms usually need to explore some uncertain retrieval results for updating current parameters meanwhile guaranteeing to produce quality retrieval results by exploiting what have already been learned, and the final retrieval results is an interleaved list from both exploratory and exploitative results. However, existing OLR algorithms perform exploration based on either only one stochastic direction or multiple randomly selected stochastic directions, which always involve large variance and uncertainty into the exploration, and may further harm the retrieval quality. Moreover, little historical exploration knowledge is considered when conducting current exploration. In this paper, we propose two OLR algorithms that improve the reliability of the exploration by constructing robust exploratory directions. First, we describe a Dual-Point Dueling Bandit Gradient Descent (DP-DBGD) approach with a Contextual Interleaving (CI) method. In particular, the exploration of DP-DBGD is carefully conducted via two opposite stochastic directions and the proposed CI method constructs a qualified interleaved retrieval result list by taking historical explorations into account. Second, we introduce a Multi-Point Deterministic Gradient Descent (MP-DGD) method that constructs a set of deterministic standard unit basis vectors for exploration. In MP-DGD, each basis direction will be explored and the parameter updating is performed by walking along the combination of exploratory winners from the basis vectors. We conduct experiments on several datasets and show that both DP-DBGD and MP-DGD improve the online learning to rank performance over 10% compared with baseline methods.","Authors":"Zhao, T (Zhao, Tong) ; King, I (King, Irwin) Book Group Author(s):ACM","Title":"Constructing Reliable Gradient Exploration for Online Learning to Rank"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390749104114 ISBN:978-1-5090-0622-9","Keywords":"Recommender Systems; Context-Aware; exploration/exploitation","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"2016 IEEE CONGRESS ON EVOLUTIONARY COMPUTATION (CEC) Book Series: IEEE Congress on Evolutionary Computation Pages: 4667-4674 Published: 2016","Abstract":"Context-Aware Recommender Systems can be naturally modelled as an exploration/exploitation trade-off (exr/exp) problem, where the system has to choose between maximizing its expected rewards dealing with its current knowledge (exploitation) and learning more about the unknown user's preferences to improve its knowledge (exploration). This problem has been addressed by the reinforcement learning community but they do not consider the risk level of the current user's situation, where it may be dangerous to recommend items the user may not desire in her current situation if the risk level is high. We introduce in this paper an algorithm named R-UCB that considers the risk level of the user's situation to adaptively balance between exr and exp. The detailed analysis of the experimental results reveals several important discoveries in the exr/exp behaviour.","Authors":"Bouneffouf, D (Bouneffouf, Djallel) Book Group Author(s):IEEE","Title":"Contextual Bandit Algorithm for Risk-Aware Recommender Systems"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390596500055 ISBN:978-1-5090-1746-1","Keywords":"Energy harvesting communications; Markov decision process (MDP); online learning; contextual bandits","Categories":"Engineering; Telecommunications Web of Science Categories:Engineering, Electrical & Electronic; Telecommunications","Journal Information":"2016 INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND COMMUNICATIONS (SPCOM) Published: 2016","Abstract":"We consider the problem of power allocation over a time-varying channel with an unknown distribution in energy harvesting communication systems. In this problem, the transmitter needs to choose its transmit power based on the amount of stored energy in its battery with the goal of maximizing the average rate obtained over time. We model this problem as a Markov decision process (MDP) with the transmitter as the agent, the battery status as the state, the transmit power as the action and the rate obtained as the reward. The average reward maximization problem over the MDP can be solved by a linear program (LP) that uses the transition probabilities for the state-action pairs and their mean rewards to choose a power allocation policy. Since the rewards associated the state-action pairs are unknown, we propose an online learning algorithm called UCLP that learns these rewards and adapts its policy with time. The UCLP algorithm solves the LP at each time-step to choose its policy using the upper confidence bounds on the rewards. We prove that the reward loss or regret incurred by UCLP is upper bounded by a constant.","Authors":"Sakulkar, P (Sakulkar, Pranav) ; Krishnamachari, B (Krishnamachari, Bhaskar) Book Group Author(s):IEEE","Title":"Online Learning of Power Allocation Policies in Energy Harvesting Communications"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000389914600011 ISSN: 1935-7524","Keywords":"Contextual bandit problem; MABC; nonparametric bandit; adaptive estimation; regret bound KeyWords Plus:MULTIARMED BANDIT; CONFIDENCE BANDS; CLASSIFIERS; RATES","Categories":"Mathematics Web of Science Categories:Statistics & Probability","Journal Information":"ELECTRONIC JOURNAL OF STATISTICS Volume: 10 Issue: 1 Pages: 242-270 DOI: 10.1214/15-EJS1104 Published: 2016","Abstract":"Motivated by applications in personalized web services and clinical research, we consider a multi-armed bandit problem in a setting where the mean reward of each arm is associated with some covariates. A multi-stage randomized allocation with arm elimination algorithm is proposed to combine the flexibility in reward function modeling and a theoretical guarantee of a cumulative regret minimax rate. When the function smoothness parameter is unknown, the algorithm is equipped with a histogram estimation based smoothness parameter selector using Lepski's method, and is shown to maintain the regret minimax rate up to a logarithmic factor under a \"self-similarity\" condition.","Authors":"Qian, W (Qian, Wei) ; Yang, YH (Yang, Yuhong) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Yang, Yuhong  http://orcid.org/0000-0003-3618-3083","Title":"Randomized allocation with arm elimination in a bandit problem with covariates"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000389805900009 ISBN:978-3-319-46687-3; 978-3-319-46686-6 ISSN: 0302-9743","Keywords":"Social recommendation; Linear bandits; Online learning KeyWords Plus:MULTIARMED BANDITS; OPTIMIZATION","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Theory & Methods","Journal Information":"NEURAL INFORMATION PROCESSING, ICONIP 2016, PT I Book Series: Lecture Notes in Computer Science Volume: 9947 Pages: 80-90 DOI: 10.1007/978-3-319-46687-3_9 Published: 2016","Abstract":"Recommender systems provide personalized suggestions by learning users' preference based on their historical feedback. To alleviate the heavy relying on historical data, several online recommendation methods are recently proposed and have shown the effectiveness in solving data sparsity and cold start problems in recommender systems. However, existing online recommendation methods neglect the use of social connections among users, which has been proven as an effective way to improve recommendation accuracy in offline settings. In this paper, we investigate how to leverage social connections to improve online recommendation performance. In particular, we formulate the online social recommendation task as a contextual bandit problem and propose a Locality-sensitive Linear Bandit (LS.Lin) method to solve it. The proposed model incorporates users' local social relations into a linear contextual bandit model and is capable to deal with the dynamic changes of user preference and the network structure. We provide a theoretical analysis to the proposed LS. Lin method and then demonstrate its improved performance for online social recommendation in empirical studies compared with baseline methods.","Authors":"Zhao, T (Zhao, Tong) ; King, I (King, Irwin) Edited by:Hirose, A; Ozawa, S; Doya, K; Ikeda, K; Lee, M; Liu, D","Title":"Locality-Sensitive Linear Bandit Model for Online Social Recommendation"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000389019600012 ISBN:978-3-319-31750-2; 978-3-319-31749-6 ISSN: 0302-9743","Keywords":"Contextual bandit; Piled rewards; Upper confidence bound","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods","Journal Information":"ADVANCES IN KNOWLEDGE DISCOVERY AND DATA MINING, PAKDD 2016, PT II Book Series: Lecture Notes in Artificial Intelligence Volume: 9652 Pages: 143-155 DOI: 10.1007/978-3-319-31750-2_12 Published: 2016","Abstract":"We study the contextual bandit problem with linear payoff function. In the traditional contextual bandit problem, the algorithm iteratively chooses an action based on the observed context, and immediately receives a reward for the chosen action. Motivated by a practical need in many applications, we study the design of algorithms under the piled-reward setting, where the rewards are received as a pile instead of immediately. We present how the Linear Upper Confidence Bound (Lin-UCB) algorithm for the traditional problem can be naively applied under the piled-reward setting, and prove its regret bound. Then, we extend LinUCB to a novel algorithm, called Linear Upper Confidence Bound with Pseudo Reward (LinUCBPR), which digests the observed contexts to choose actionsmore strategically before the piled rewards are received. We prove that LinUCBPR can match LinUCB in the regret bound under the piled-reward setting. Experiments on the artificial and real-world datasets demonstrate the strong performance of LinUCBPR in practice.","Authors":"Huang, KH (Huang, Kuan-Hao) ; Lin, HT (Lin, Hsuan-Tien) Edited by:Bailey, J; Khan, L; Washio, T; Dobbie, G; Huang, JZ; Wang, R","Title":"Linear Upper Confidence Bound Algorithm for Contextual Bandit Problem with Piled Rewards"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000382226300079 ISBN:978-1-4503-3936-0","Keywords":"Online Learning; Contextual Bandits; Ellipsoid Method; Online Ads; Revenue Management","Categories":"Computer Science; Business & Economics Web of Science Categories:Computer Science, Interdisciplinary Applications; Economics","Journal Information":"EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION Pages: 817-817 DOI: 10.1145/2940716.2940728 Published: 2016","Authors":"Cohen, MC (Cohen, Maxime C.) ; Lobel, I (Lobel, Ilan) ; Leme, RP (Leme, Renato Paes) Book Group Author(s):ACM","Title":"Feature-based Dynamic Pricing"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000364397200006 PubMed ID: 26378878 ISSN: 0899-7667 eISSN: 1530-888X","Categories":"Computer Science; Neurosciences & Neurology Web of Science Categories:Computer Science, Artificial Intelligence; Neurosciences","Journal Information":"NEURAL COMPUTATION Volume: 27 Issue: 11 Pages: 2447-2475 DOI: 10.1162/NECO_a_00782 Published: NOV 2015","Abstract":"We consider a task assignment problem in crowdsourcing, which is aimed at collecting as many reliable labels as possible within a limited budget. A challenge in this scenario is how to cope with the diversity of tasks and the task-dependent reliability of workers; for example, a worker may be good at recognizing the names of sports teams but not be familiar with cosmetics brands. We refer to this practical setting as heterogeneous crowdsourcing. In this letter, we propose a contextual bandit formulation for task assignment in heterogeneous crowdsourcing that is able to deal with the exploration-exploitation trade-off in worker selection. We also theoretically investigate the regret bounds for the proposed method and demonstrate its practical usefulness experimentally.","Authors":"Zhang, H (Zhang, Hao) ; Ma, Y (Ma, Yao) ; Sugiyama, M (Sugiyama, Masashi)","Title":"Bandit-Based Task Assignment for Heterogeneous Crowdsourcing"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000358514100005 PubMed ID: 26087490 ISSN: 1057-7149 eISSN: 1941-0042","Keywords":"Image stream mining; active learning; online classification; online learning; contextual experts; breast cancer diagnosis KeyWords Plus:BREAST-CANCER DIAGNOSIS; CLASSIFICATION; ALGORITHMS; REGRESSION; SUPPORT; BANDITS","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON IMAGE PROCESSING Volume: 24 Issue: 11 Pages: 3666-3679 DOI: 10.1109/TIP.2015.2446936 Published: NOV 2015","Abstract":"We propose an image stream mining method in which images arrive with contexts (metadata) and need to be processed in real time by the image mining system (IMS), which needs to make predictions and derive actionable intelligence from these streams. After extracting the features of the image by preprocessing, IMS determines online the classifier to use on the extracted features to make a prediction using the context of the image. A key challenge associated with stream mining is that the prediction accuracy of the classifiers is unknown, since the image source is unknown; thus, these accuracies need to be learned online. Another key challenge of stream mining is that learning can only be done by observing the true label, but this is costly to obtain. To address these challenges, we model the image stream mining problem as an active, online contextual experts problem, where the context of the image is used to guide the classifier selection decision. We develop an active learning algorithm and show that it achieves regret sublinear in the number of images that have been observed so far. To further illustrate and assess the performance of our proposed methods, we apply them to diagnose breast cancer from the images of cellular samples obtained from the fine needle aspirate of breast mass. Our findings show that very high diagnosis accuracy can be achieved by actively obtaining only a small fraction of true labels through surgical biopsies. Other applications include video surveillance and video traffic monitoring.","Authors":"Tekin, C (Tekin, Cem) ; van der Schaar, M (van der Schaar, Mihaela)","Title":"Active Learning in Context-Driven Stream Mining With an Application to Image Mining"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000356141600011 ISSN: 1053-587X eISSN: 1941-0476","Keywords":"Contextual bandits; cooperative learning; distributed learning; multi-user bandits; multi-user learning; online learning KeyWords Plus:MULTIARMED BANDIT; OPTIMIZATION","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON SIGNAL PROCESSING Volume: 63 Issue: 14 Pages: 3700-3714 DOI: 10.1109/TSP.2015.2430837 Published: JUL 15 2015","Abstract":"In this paper, we propose a novel framework for decentralized, online learning by many learners. At each moment of time, an instance characterized by a certain context may arrive to each learner; based on the context, the learner can select one of its own actions (which gives a reward and provides information) or request assistance from another learner. In the latter case, the requester pays a cost and receives the reward but the provider learns the information. In our framework, learners are modeled as cooperative contextual bandits. Each learner seeks to maximize the expected reward from its arrivals, which involves trading off the reward received from its own actions, the information learned from its own actions, the reward received from the actions requested of others and the cost paid for these actions-taking into account what it has learned about the value of assistance from each other learner. We develop distributed online learning algorithms and provide analytic bounds to compare the efficiency of these with algorithms with the complete knowledge (oracle) benchmark (in which the expected reward of every action in every context is known by every learner). Our estimates show that regret-the loss incurred by the algorithm-is sublinear in time. Our theoretical framework can be used in many practical applications including Big Data mining, event detection in surveillance sensor networks and distributed online recommendation systems.","Authors":"Tekin, C (Tekin, Cem) ; van der Schaar, M (van der Schaar, Mihaela)","Title":"Distributed Online Learning via Cooperative Contextual Bandits"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000354480600012 ISSN: 1932-4553 eISSN: 1941-0484","Keywords":"Contextual bandits; regret; dimensionality reduction; learning relevance; recommender systems; online learning; active learning KeyWords Plus:TRACKING; SUPPORT","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING Volume: 9 Issue: 4 Pages: 716-727 DOI: 10.1109/JSTSP.2015.2402646 Published: JUN 2015","Abstract":"Recommender systems, medical diagnosis, network security, etc., require on-going learning and decision-making in real time. These-and many others-represent perfect examples of the opportunities and difficulties presented by Big Data: the available information often arrives from a variety of sources and has diverse features so that learning from all the sources may be valuable but integrating what is learned is subject to the curse of dimensionality. This paper develops and analyzes algorithms that allow efficient learning and decision-making while avoiding the curse of dimensionality. We formalize the information available to the learner/decision-maker at a particular time as a context vector which the learner should consider when taking actions. In general the context vector is very high dimensional, but in many settings, the most relevant information is embedded into only a few relevant dimensions. If these relevant dimensions were known in advance, the problem would be simple-but they are not. Moreover, the relevant dimensions may be different for different actions. Our algorithm learns the relevant dimensions for each action, and makes decisions based in what it has learned. Formally, we build on the structure of a contextual multi-armed bandit by adding and exploiting a relevance relation. We prove a general regret bound for our algorithm whose time order depends only on the maximum number of relevant dimensions among all the actions, which in the special case where the relevance relation is single-valued (a function), reduces to (O) over tilde (T2(root 2-1)) in the absence of a relevance relation, the best known contextual bandit algorithms achieve regret (O) over tilde (T(D+1)/(D+2)), where D is the full dimension of the context vector. Our algorithm alternates between exploring and exploiting and does not require observing outcomes during exploitation (so allows for active learning). Moreover, during exploitation, suboptimal actions are chosen with arbitrarily low probability. Our algorithm is tested on datasets arising from network security and online news article recommendations.","Authors":"Tekin, C (Tekin, Cem) ; van der Schaar, M (van der Schaar, Mihaela)","Title":"RELEAF: An Algorithm for Learning and Exploiting Relevance"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000351586300008 ISSN: 1520-9210 eISSN: 1941-0077","Keywords":"Content aggregation; distributed online learning; multi-armed bandits; social multimedia KeyWords Plus:DATA STREAMS; RECOMMENDATION; VIDEO","Categories":"Computer Science; Telecommunications Web of Science Categories:Computer Science, Information Systems; Computer Science, Software Engineering; Telecommunications","Journal Information":"IEEE TRANSACTIONS ON MULTIMEDIA Volume: 17 Issue: 4 Pages: 549-561 DOI: 10.1109/TMM.2015.2403234 Published: APR 2015","Abstract":"The last decade has witnessed a tremendous growth in the volume as well as the diversity of multimedia content generated by a multitude of sources (news agencies, social media, etc.). Faced with a variety of content choices, consumers are exhibiting diverse preferences for content; their preferences often depend on the context in which they consume content as well as various exogenous events. To satisfy the consumers' demand for such diverse content, multimedia content aggregators (CAs) have emerged which gather content from numerous multimedia sources. A key challenge for such systems is to accurately predict what type of content each of its consumers prefers in a certain context, and adapt these predictions to the evolving consumers' preferences, contexts, and content characteristics. We propose a novel, distributed, online multimedia content aggregation framework, which gathers content generated by multiple heterogeneous producers to fulfill its consumers' demand for content. Since both the multimedia content characteristics and the consumers' preferences and contexts are unknown, the optimal content aggregation strategy is unknown a priori. Our proposed content aggregation algorithm is able to learn online what content to gather and how to match content and users by exploiting similarities between consumer types. We prove bounds for our proposed learning algorithms that guarantee both the accuracy of the predictions as well as the learning speed. Importantly, our algorithms operate efficiently even when feedback from consumers is missing or content and preferences evolve over time. Illustrative results highlight the merits of the proposed content aggregation system in a variety of settings.","Authors":"Tekin, C (Tekin, Cem) ; van der Schaar, M (van der Schaar, Mihaela)","Title":"Contextual Online Learning for Multimedia Content Aggregation"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000209845800008 ISSN: 2168-6750","Keywords":"Semantic computing; context-adaptive learning; clinical decision support systems; healthcare informatics; distributed multi-user learning; contextual bandits","Categories":"Computer Science; Telecommunications Web of Science Categories:Computer Science, Information Systems; Telecommunications","Journal Information":"IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTING Volume: 3 Issue: 2 Pages: 220-234 Special Issue: SI DOI: 10.1109/TETC.2014.2386133 Published: APR-JUN 2015","Abstract":"In this paper, we propose an expert selection system that learns online the best expert to assign to each patient depending on the context of the patient. In general, the context can include an enormous number and variety of information related to the patient's health condition, age, gender, previous drug doses, and so forth, but the most relevant information is embedded in only a few contexts. If these most relevant contexts were known in advance, learning would be relatively simple but they are not. Moreover, the relevant contexts may be different for different health conditions. To address these challenges, we develop a new class of algorithms aimed at discovering the most relevant contexts and the best clinic and expert to use to make a diagnosis given a patient's contexts. We prove that as the number of patients grows, the proposed context adaptive algorithm will discover the optimal expert to select for patients with a specific context. Moreover, the algorithm also provides confidence bounds on the diagnostic accuracy of the expert it selects, which can be considered by the primary care physician before making the final decision. While our algorithm is general and can be applied in numerous medical scenarios, we illustrate its functionality and performance by applying it to a real-world breast cancer diagnosis data set. Finally, while the application we consider in this paper is medical diagnosis, our proposed algorithm can be applied in other environments where expertise needs to be discovered.","Authors":"Tekin, C (Tekin, Cem) ; Atan, O (Atan, Onur) ; Van der Schaar, M (Van der Schaar, Mihaela)","Title":"Discover the Expert: Context-Adaptive Expert Selection for Medical Diagnosis"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000382666600009 ISBN:978-1-4503-3473-0","Keywords":"Information retrieval; Online learning; Learning to rank","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB Pages: 19-20 DOI: 10.1145/2740908.2742718 Published: 2015","Abstract":"Online learning to rank holds great promise for learning personalized search result rankings. First algorithms have been proposed, namely absolute feedback approaches, based on contextual bandits learning; and relative feedback approaches, based on gradient methods and inferred preferences between complete result rankings. Both types of approaches have shown promise, but they have not previously been compared to each other. It is therefore unclear which type of approach is the most suitable for which online learning to rank problems. In this work we present the first empirical comparison of absolute and relative online learning to rank approaches.","Authors":"Chen, YW (Chen, Yiwei) ; Hofmann, K (Hofmann, Katja) Book Group Author(s):ACM","Title":"Online Learning to Rank: Absolute vs. Relative"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000382666600217 ISBN:978-1-4503-3473-0","Keywords":"Experimental design; counterfactual analysis; Web search; query correction/rewriting; information retrieval; contextual bandits KeyWords Plus:IR","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB Pages: 929-934 DOI: 10.1145/2740908.2742562 Published: 2015","Abstract":"Optimizing an interactive system against a predefined online metric is particularly challenging, especially when the metric is computed from user feedback such as clicks and payments. The key challenge is the counterfactual nature: in the case of Web search, any change to a component of the search engine may result in a different search result page for the same query, but we normally cannot infer reliably from search log how users would react to the new result page. Consequently, it appears impossible to accurately estimate online metrics that depend on user feedback, unless the new engine is actually run to serve live users and compared with a baseline in a controlled experiment. This approach, while valid and successful, is unfortunately expensive and time-consuming. In this paper, we propose to address this problem using causal inference techniques, under the contextual-bandit framework. This approach effectively allows one to run potentially many online experiments offline from search log, making it possible to estimate and optimize online metrics quickly and inexpensively. Focusing on an important component in a commercial search engine, we show how these ideas can be instantiated and applied, and obtain very promising results that suggest the wide applicability of these techniques.","Authors":"Li, LH (Li, Lihong) ; Chen, SB (Chen, Shunbao) ; Kleban, J (Kleban, Jim) ; Gupta, A (Gupta, Ankur) Book Group Author(s):ACM","Title":"Counterfactual Estimation and Optimization of Click Metrics in Search Engines: A Case Study"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000382666600290 ISBN:978-1-4503-3473-0","Keywords":"Ad recommendation; reinforcement learning; off-policy evaluation KeyWords Plus:BOOTSTRAP CONFIDENCE-INTERVALS; PREDICTION; DISEASE","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB Pages: 1305-1310 DOI: 10.1145/2740908.2741998 Published: 2015","Abstract":"The main objective in the ad recommendation problem is to find a strategy that, for each visitor of the website, selects the ad that has the highest probability of being clicked. This strategy could be computed using supervised learning or contextual bandit algorithms, which treat two visits of the same user as two separate independent visitors, and thus, optimize greedily for a single step into the future. Another approach would be to use reinforcement learning (RL) methods, which differentiate between two visits of the same user and two different visitors, and thus, optimizes for multiple steps into the future or the life-time value (LTV) of a customer. While greedy methods have been well-studied, the LTV approach is still in its infancy, mainly due to two fundamental challenges: how to compute a good LTV strategy and how to evaluate a solution using historical data to ensure its \"safety\" before deployment. In this paper, we tackle both of these challenges by proposing to use a family of off-policy evaluation techniques with statistical guarantees about the performance of a new strategy. We apply these methods to a real ad recommendation problem, both for evaluating the final performance and for optimizing the parameters of the RL algorithm. Our results show that our LTV optimization algorithm equipped with these off-policy evaluation techniques outperforms the greedy approaches. They also give fundamental insights on the difference between the click through rate (CTR) and LTV metrics for performance evaluation in the ad recommendation problem.","Authors":"Theocharous, G (Theocharous, Georgios) ; Thomas, PS (Thomas, Philip S.) ; Ghavamzadeh, M (Ghavamzadeh, Mohammad) Book Group Author(s):ACM","Title":"Ad Recommendation Systems for Life-Time Value Optimization"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000382307300034 ISBN:978-1-4503-3621-5","Keywords":"Recommender Systems; Personalization; Contextual Bandit; Probability Matching; Bootstrapping KeyWords Plus:MULTIARMED BANDIT; OPTIMIZATION","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL Pages: 323-332 DOI: 10.1145/2766462.2767707 Published: 2015","Abstract":"Personalized recommendation services have gained increasing popularity and attention in recent years as most useful information can be accessed online in real-time. Most online recommender systems try to address the information needs of users by virtue of both user and content information. Despite extensive recent advances, the problem of personalized recommendation remains challenging for at least two reasons. First, the user and item repositories undergo frequent changes, which makes traditional recommendation algorithms ineffective. Second, the so-called cold-start problem is difficult to address, as the information for learning a recommendation model is limited for new items or new users. Both challenges are formed by the dilemma of exploration and exploitation. In this paper, we formulate personalized recommendation as a contextual bandit problem to solve the exploration/exploitation dilemma. Specifically in our work, we propose a parameter-free bandit strategy, which employs a principled resampling approach called online bootstrap, to derive the distribution of estimated models in an online manner. Under the paradigm of probability matching, the proposed algorithm randomly samples a model from the derived distribution for every recommendation. Extensive empirical experiments on two real-world collections of web data (including online advertising and news recommendation) demonstrate the effectiveness of the proposed algorithm in terms of the click-through rate. The experimental results also show that this proposed algorithm is robust in the cold-start situation, in which there is no sufficient data or knowledge to tune the parameters.","Authors":"Tang, L (Tang, Liang) ; Jiang, YX (Jiang, Yexi) ; Li, L (Li, Lei) ; Zeng, CQ (Zeng, Chunqiu) ; Li, T (Li, Tao) Book Group Author(s):ACM","Title":"Personalized Recommendation via Parameter-Free Contextual Bandits"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000380396700012 ISBN:978-1-5090-0016-6","Keywords":"Online Recommender Systems; Multi-armed Bandits; Multi-objective","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems","Journal Information":"2015 BRAZILIAN CONFERENCE ON INTELLIGENT SYSTEMS (BRACIS 2015) Pages: 68-73 DOI: 10.1109/BRACIS.2015.67 Published: 2015","Abstract":"The contextual bandit framework have become a popular solution for online interactive recommender systems. Traditionally, the literature in interactive recommender systems has been focused on recommendation accuracy. However, it has been increasingly recognized that accuracy is not enough as the only quality criteria. Thus, other concepts have been suggested to improve recommendation evaluation, such as diversity and novelty. Simultaneously considering multiple criteria in payoff functions leads to a multi-objective recommendation. In this paper, we model the payoff function of contextual bandits to considering accuracy, diversity and novelty simultaneously. We evaluated our proposed algorithm on the Yahoo! Front Page Module dataset that contains over 33 million events. Results showed that: (a) we are able to improve recommendation quality when equally considering all objectives, and (b) we allow for adjusting the compromise between accuracy, diversity and novelty, so that recommendation emphasis can be adjusted according to the needs of different users.","Authors":"Lacerda, A (Lacerda, Anisio) Book Group Author(s):IEEE","Title":"Contextual Bandits for Multi-Objective Recommender Systems"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000375768500005 ISBN:978-3-319-27992-3; 978-3-319-27991-6 ISSN: 0302-9743","Categories":"Computer Science; Robotics Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods; Robotics","Journal Information":"ADVANCES IN COMPUTER GAMES, ACG 2015 Book Series: Lecture Notes in Computer Science Volume: 9525 Pages: 41-52 DOI: 10.1007/978-3-319-27992-3_5 Published: 2015","Abstract":"UCT is a de facto standard method for Monte-Carlo tree search (MCTS) algorithms, which have been applied to various domains and have achieved remarkable success. This study proposes a family of LinUCT algorithms that incorporate LinUCB into MCTS algorithms. LinUCB is a recently developed method that generalizes past episodes by ridge regression with feature vectors and rewards. LinUCB outperforms UCB1 in contextual multi-armed bandit problems. We introduce a straightforward application of LinUCB, LinUCT(PLAIN) by substituting UCB1 with LinUCB in UCT. We show that it does not work well owing to the minimax structure of game trees. To better handle such tree structures, we present LinUCT(RAVE) and LinUCT(FP) by further incorporating two existing techniques, rapid action value estimation (RAVE) and feature propagation, which recursively propagates the feature vector of a node to that of its parent. Experiments were conducted with a synthetic model, which is an extension of the standard incremental random tree model in which each node has a feature vector that represents the characteristics of the corresponding position. The experimental results indicate that LinUCT(RAVE), LinUCT(FP), and their combination LinUCT(RAVE-FP) outperform UCT, especially when the branching factor is relatively large.","Authors":"Mandai, Y (Mandai, Yusaku) ; Kaneko, T (Kaneko, Tomoyuki) Edited by:Plaat, A; VandenHerik, J; Kosters, W","Title":"LinUCB Applied to Monte-Carlo Tree Search"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000374592500071 ISBN:978-1-5090-0163-7 ISSN: 1082-3409","Keywords":"Multi-armed bandit; Contextual Decision Process; Poisson Regression KeyWords Plus:ALLOCATION","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Theory & Methods","Journal Information":"2015 IEEE 27TH INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE (ICTAI 2015) Book Series: Proceedings-International Conference on Tools With Artificial Intelligence Pages: 542-549 DOI: 10.1109/ICTAI.2015.85 Published: 2015","Abstract":"The contextual bandit problem has been of major interest in the last few years. This corresponds to a sequential decision process where an agent has to choose at each iteration an action to perform, according to some knowledge about the decision environment and the current available actions, with the aim to maximize a cumulative amount of rewards over time. Many instances of the problem exist, depending on the kind of rewards we collect - real, binary, natural and various algorithms are known to be efficient for some of these instances, either empirically or theoretically. In this paper we focus on the case of count payoffs, which corresponds to bandit problems where rewards are integer rewards, potentially unbounded. Based on a Bayesian Poisson regression model, we propose two new contextual bandit algorithms for this particular case with several concrete applications in real life: an Upper Confidence Bound algorithm and a Thompson Sampling strategy. Our approaches present the advantage to remain analytically tractable and computationally efficient. We experiment the algorithms on both simulated data and a real world scenario of spread maximization on a social network.","Authors":"Gisselbrecht, T (Gisselbrecht, Thibault) ; Lamprier, S (Lamprier, Sylvain) ; Gallinari, P (Gallinari, Patrick) Book Group Author(s):IEEE","Title":"Policies for Contextual Bandit Problems with Count Payoffs"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000369892500002 ISBN:978-3-319-25485-2; 978-3-319-25484-5 ISSN: 1865-0929","Keywords":"Online evaluation; A/B testing; Contextual bandits; Dueling bandits; Interleaved comparison; Online learning to rank; Counterfactual analysis; Experiment design","Categories":"Computer Science; Information Science & Library Science Web of Science Categories:Computer Science, Information Systems; Computer Science, Theory & Methods; Information Science & Library Science","Journal Information":"INFORMATION RETRIEVAL, RUSSIR 2014 Book Series: Communications in Computer and Information Science Volume: 505 Pages: 21-41 DOI: 10.1007/978-3-319-25485-2_2 Published: 2015","Abstract":"Online experimentation for information retrieval (IR) focuses on insights that can be gained from user interactions with IR systems, such as web search engines. The most common form of online experimentation, A/B testing, is widely used in practice, and has helped sustain continuous improvement of the current generation of these systems. As online experimentation is taking a more and more central role in IR research and practice, new techniques are being developed to address, e.g., questions regarding the scale and fidelity of experiments in online settings. This paper gives an overview of the currently available tools. This includes techniques that are already in wide use, such as A/B testing and interleaved comparisons, as well as techniques that have been developed more recently, such as bandit approaches for online learning to rank. This paper summarizes and connects the wide range of techniques and insights that have been developed in this field to date. It concludes with an outlook on open questions and directions for ongoing and future research.","Authors":"Hofmann, K (Hofmann, Katja) Edited by:Braslavski, P; Karpov, N; Worring, M; Volkovich, Y; Ignatov, DI","Title":"Online Experimentation for Information Retrieval"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000348786900002 ISSN: 0883-4237","Keywords":"Contextual bandits; doubly robust estimators; causal inference KeyWords Plus:REGRESSION ESTIMATION; TREATMENT REGIMES; CAUSAL INFERENCE; MODELS","Categories":"Mathematics Web of Science Categories:Statistics & Probability","Journal Information":"STATISTICAL SCIENCE Volume: 29 Issue: 4 Pages: 485-511 Special Issue: SI DOI: 10.1214/14-STS500 Published: NOV 2014","Abstract":"We study sequential decision making in environments where rewards are only partially observed, but can be modeled as a function of observed contexts and the chosen action by the decision maker. This setting, known as contextual bandits, encompasses a wide variety of applications such as health care, content recommendation and Internet advertising. A central task is evaluation of a new policy given historic data consisting of contexts, actions and received rewards. The key challenge is that the past data typically does not faithfully represent proportions of actions taken by a new policy. Previous approaches rely either on models of rewards or models of the past policy. The former are plagued by a large bias whereas the latter have a large variance. In this work, we leverage the strengths and overcome the weaknesses of the two approaches by applying the doubly robust estimation technique to the problems of policy evaluation and optimization. We prove that this approach yields accurate value estimates when we have either a good (but not necessarily consistent) model of rewards or a good (but not necessarily consistent) model of past policy. Extensive empirical comparison demonstrates that the doubly robust estimation uniformly improves over existing techniques, achieving both lower variance in value estimation and better policies. As such, we expect the doubly robust approach to become common practice in policy evaluation and optimization.","Authors":"Dudik, M (Dudik, Miroslav) ; Erhan, D (Erhan, Dumitru) ; Langford, J (Langford, John) ; Li, L (Li, Lihong)","Title":"Doubly Robust Policy Evaluation and Optimization"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000344852100001 ISSN: 0824-7935 eISSN: 1467-8640","Keywords":"reinforcement learning; model-free learning; structure learning; abstraction selection KeyWords Plus:ALGORITHMS; TIME; CONVERGENCE; MODEL","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"COMPUTATIONAL INTELLIGENCE Volume: 30 Issue: 4 Pages: 657-699 DOI: 10.1111/coin.12016 Published: NOV 2014","Abstract":"This article addresses reinforcement learning problems based on factored Markov decision processes (MDPs) in which the agent must choose among a set of candidate abstractions, each build up from a different combination of state components. We present and evaluate a new approach that can perform effective abstraction selection that is more resource-efficient and/or more general than existing approaches. The core of the approach is to make selection of an abstraction part of the learning agent's decision-making process by augmenting the agent's action space with internal actions that select the abstraction it uses. We prove that under certain conditions this approach results in a derived MDP whose solution yields both the optimal abstraction for the original MDP and the optimal policy under that abstraction. We examine our approach in three domains of increasing complexity: contextual bandit problems, episodic MDPs, and general MDPs with context-specific structure. (c) 2013 Wiley Periodicals, Inc.","Authors":"van Seijen, H (van Seijen, Harm) ; Whiteson, S (Whiteson, Shimon) ; Kester, L (Kester, Leon)","Title":"EFFICIENT ABSTRACTION SELECTION IN REINFORCEMENT LEARNING"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000344638800017 ISSN: 1532-4435","Keywords":"reinforcement learning; policy search; movement primitives; active learning; multi-task learning KeyWords Plus:MOVEMENT PRIMITIVES; BANDIT PROBLEM; MODELS","Categories":"Automation & Control Systems; Computer Science Web of Science Categories:Automation & Control Systems; Computer Science, Artificial Intelligence","Journal Information":"JOURNAL OF MACHINE LEARNING RESEARCH Volume: 15 Pages: 3371-3399 Published: OCT 2014","Abstract":"We consider the problem of learning skills that are versatilely applicable. One popular approach for learning such skills is contextual policy search in which the individual tasks are represented as context vectors. We are interested in settings in which the agent is able to actively select the tasks that it examines during the learning process. We argue that there is a better way than selecting each task equally often because some tasks might be easier to learn at the beginning and the knowledge that the agent can extract from these tasks can be transferred to similar but more difficult tasks. The methods that we propose for addressing the task-selection problem model the learning process as a non-stationary multi-armed bandit problem with custom intrinsic reward heuristics so that the estimated learning progress will be maximized. This approach does neither make any assumptions about the underlying contextual policy search algorithm nor about the policy representation. We present empirical results on an artificial benchmark problem and a ball throwing problem with a simulated Mitsubishi PA-10 robot arm which show that active context selection can improve the learning of skills considerably.","Authors":"Fabisch, A (Fabisch, Alexander) ; Metzen, JH (Metzen, Jan Hendrik)","Title":"Active Contextual Policy Search"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000340528900012 ISSN: 1932-4553 eISSN: 1941-0484","Keywords":"Collaborative learning; contextual bandits; distributed recommender systems; multi-agent online learning; regret KeyWords Plus:MULTIARMED BANDIT; OPTIMIZATION","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING Volume: 8 Issue: 4 Pages: 638-652 DOI: 10.1109/JSTSP.2014.2299517 Published: AUG 2014","Abstract":"In this paper, we consider decentralized sequential decision making in distributed online recommender systems, where items are recommended to users based on their search query as well as their specific background including history of bought items, gender and age, all of which comprise the context information of the user. In contrast to centralized recommender systems, in which there is a single centralized seller who has access to the complete inventory of items as well as the complete record of sales and user information, in decentralized recommender systems each seller/learner only has access to the inventory of items and user information for its own products and not the products and user information of other sellers, but can get commission if it sells an item of another seller. Therefore, the sellers must distributedly find out for an incoming user which items to recommend ( from the set of own items or items of another seller), in order to maximize the revenue from own sales and commissions. We formulate this problem as a cooperative contextual bandit problem, analytically bound the performance of the sellers compared to the best recommendation strategy given the complete realization of user arrivals and the inventory of items, as well as the context-dependent purchase probabilities of each item, and verify our results via numerical examples on a distributed data set adapted based on Amazon data. We evaluate the dependence of the performance of a seller on the inventory of items the seller has, the number of connections it has with the other sellers, and the commissions which the seller gets by selling items of other sellers to its users.","Authors":"Tekin, C (Tekin, Cem) ; Zhang, S (Zhang, Simpson) ; van der Schaar, M (van der Schaar, Mihaela)","Title":"Distributed Online Learning in Social Recommender Systems"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000344638400003 ISSN: 1532-4435","Keywords":"contextual bandits; structured prediction; ranking; online learning; regret bounds; generalized linear KeyWords Plus:MINIMIZATION; ALGORITHMS","Categories":"Automation & Control Systems; Computer Science Web of Science Categories:Automation & Control Systems; Computer Science, Artificial Intelligence","Journal Information":"JOURNAL OF MACHINE LEARNING RESEARCH Volume: 15 Pages: 2451-2487 Published: JUL 2014","Abstract":"We present a novel multilabel/ranking algorithm working in partial information settings. The algorithm is based on 2nd-order descent methods, and relies on upper-confidence bounds to trade-off exploration and exploitation. We analyze this algorithm in a partial adversarial setting, where covariates can be adversarial, but multilabel probabilities are ruled by (generalized) linear models. We show O(T-1/2 log T) regret bounds, which improve in several ways on the existing results. We test the effectiveness of our upper-confidence scheme by contrasting against full-information baselines on diverse real-world multilabel data sets, often obtaining comparable performance.","Authors":"Gentile, C (Gentile, Claudio) ; Orabona, F (Orabona, Francesco)","Title":"On Multilabel Classification and Ranking with Bandit Feedback"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000344638400006 ISSN: 1532-4435","Keywords":"multi-armed bandits; contextual bandits; regret; Lipschitz-continuity; metric space","Categories":"Automation & Control Systems; Computer Science Web of Science Categories:Automation & Control Systems; Computer Science, Artificial Intelligence","Journal Information":"JOURNAL OF MACHINE LEARNING RESEARCH Volume: 15 Pages: 2533-2568 Published: JUL 2014","Abstract":"In a multi-armed bandit (MAB) problem, an online algorithm makes a sequence of choices. In each round it chooses from a time-invariant set of alternatives and receives the payoff associated with this alternative. While the case of small strategy sets is by now well-understood, a lot of recent work has focused on MAB problems with exponentially or infinitely large strategy sets, where one needs to assume extra structure in order to make the problem tractable. In particular, recent literature considered information on similarity between arms. We consider similarity information in the setting of contextual bandits, a natural extension of the basic MAB problem where before each round an algorithm is given the context a hint about the payoffs in this round. Contextual bandits are directly motivated by placing advertisements on web pages, one of the crucial problems in sponsored search. A particularly simple way to represent similarity information in the contextual bandit setting is via a similarity distance between the context-arm pairs which bounds from above the difference between the respective expected payoffs. Prior work on contextual bandits with similarity uses \"uniform\" partitions of the similarity space, so that each context-arm pair is approximated by the closest pair in the partition. Algorithms based on \"uniform\" partitions disregard the structure of the payoffs and the context arrivals, which is potentially wasteful. We present algorithms that are based on adaptive partitions, and take advantage of \"benign\" payoffs and context arrivals without sacrificing the worst-case performance. The central idea is to maintain a finer partition in high-payoff regions of the similarity space and in popular regions of the context space. Our results apply to several other settings, e.g., MAB with constrained temporal change (Slivkins and Upfal, 2008) and sleeping bandits (Kleinberg et al., 2008a).","Authors":"Slivkins, A (Slivkins, Aleksandrs)","Title":"Contextual Bandits with Similarity Information"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000346245200046 ISBN:978-3-319-12643-2; 978-3-319-12642-5 ISSN: 0302-9743","Keywords":"CARS; Thompson Sampling; Contextual bandits","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods","Journal Information":"NEURAL INFORMATION PROCESSING, ICONIP 2014, PT III Book Series: Lecture Notes in Computer Science Volume: 8836 Pages: 373-380 Published: 2014","Abstract":"To follow the dynamicity of the user's content, researchers have recently started to model interactions between users and the Context-Aware Recommender Systems (CARS) as a bandit problem where the system needs to deal with exploration and exploitation dilemma. In this sense, we propose to study the freshness of the user's content in CARS through the bandit problem. We introduce in this paper an algorithm named Freshness-Aware Thompson Sampling (FA-TS) that manages the recommendation of fresh document according to the user's risk of the situation. The intensive evaluation and the detailed analysis of the experimental results reveals several important discoveries in the exploration/exploitation (exr/exp) behaviour.","Authors":"Bouneffouf, D (Bouneffouf, Djallel) Edited by:Loo, CK; Yap, KS; Wong, KW; Teoh, A; Huang, K","Title":"Freshness-Aware Thompson Sampling"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000380426900067 ISBN:978-1-4799-8009-3 ISSN: 2474-0195","Keywords":"Stream mining; context-adaptive learning; distributed multi-user learning; contextual bandits KeyWords Plus:WIRELESS SENSOR NETWORKS; ALGORITHMS; REGRESSION; TRACKING","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"2014 52ND ANNUAL ALLERTON CONFERENCE ON COMMUNICATION, CONTROL, AND COMPUTING (ALLERTON) Book Series: Annual Allerton Conference on Communication Control and Computing Pages: 483-490 Published: 2014","Abstract":"Emerging stream mining applications require classification of large data streams generated by single or multiple heterogeneous sources. Different classifiers can be used to produce predictions. However, in many practical scenarios the distribution over data and labels (and hence the accuracies of the classifiers) may be unknown a priori and may change in unpredictable ways over time. We consider data streams that are characterized by their context information which can be used as meta-data to choose which classifier should be used to make a specific prediction. Since the context information can be high dimensional, learning the best classifiers to make predictions using contexts suffers from the curse of dimensionality. In this paper, we propose a context-adaptive learning algorithm which learns online what is the best context, learner, and classifier to use to process a data stream. Then, we theoretically bound the regret of the proposed algorithm and show that its time order is independent of the dimension of the context space. Our numerical results illustrate that our algorithm outperforms most prior online learning algorithms, for which such online performance bounds have not been proven.","Authors":"Tekin, C (Tekin, Cem) ; Canzian, L (Canzian, Luca) ; van der Schaar, M (van der Schaar, Mihaela) Book Group Author(s):IEEE","Title":"Context-Adaptive Big Data Stream Mining"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000380919400112 ISBN:978-1-4799-3512-3 ISSN: 2334-0983","Categories":"Computer Science; Engineering; Telecommunications Web of Science Categories:Computer Science, Theory & Methods; Engineering, Electrical & Electronic; Telecommunications","Journal Information":"2014 IEEE GLOBAL COMMUNICATIONS CONFERENCE (GLOBECOM 2014) Book Series: IEEE Global Communications Conference Pages: 2423-2423 Published: 2014","Abstract":"Enabling accurate and low-cost classification of a range of motion activities is of significant importance for wireless health through body worn inertial sensors and smartphones, due to the need by healthcare and fitness professonals to monitor exercises for quality and compliance. This paper proposes a novel contextual multi-armed bandits approach for large-scale activity classification. The proposed method is able to address the unique challenges arising from scaling, lack of training data and adaptation by melding context augmentation and continuous online learning into traditional activity classification. We rigorously characterize the performance of the proposed learning algorithm and prove that the learning regret (i.e. reward loss) is sublinear in time, thereby ensuring fast convergence to the optimal reward as well as providing short-term performance guarantees. Our experiments show that the proposed algorithm outperforms existing algorithms in terms of both providing higher classification accuracy as well as lower energy consumption.","Authors":"Xu, J (Xu, Jie) ; Xu, JY (Xu, James Y.) ; Song, LQ (Song, Linqi) ; Pattie, GJ (Pattie, Gregory J.) ; van der Schaar, M (van der Schaar, Mihaela) Book Group Author(s):IEEE","Title":"Context-Driven Online Learning for Activity Classification in Wireless Health"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000324663000024 ISSN: 0957-4174 eISSN: 1873-6793","Keywords":"Contextual bandits; Online advertising; Recommender systems; One-to-one Marketing; Empirical Bayes KeyWords Plus:EXPLOITATION; EXPLORATION","Categories":"Computer Science; Engineering; Operations Research & Management Science Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science","Journal Information":"EXPERT SYSTEMS WITH APPLICATIONS Volume: 40 Issue: 18 Pages: 7400-7406 DOI: 10.1016/j.eswa.2013.07.041 Published: DEC 15 2013","Abstract":"Machine and Statistical Learning techniques are used in almost all online advertisement systems. The problem of discovering which content is more demanded (e.g. receive more clicks) can be modeled as a multi-armed bandit problem. Contextual bandits (i.e., bandits with covariates, side information or associative reinforcement learning) associate, to each specific content, several features that define the \"context\" in which it appears (e.g. user, web page, time, region). This problem can be studied in the stochastic/statistical setting by means of the conditional probability paradigm using the Bayes' theorem. However, for very large contextual information and/or real-time constraints, the exact calculation of the Bayes' rule is computationally infeasible. In this article, we present a method that is able to handle large contextual information for learning in contextual-bandits problems. This method was tested in the Challenge on Yahoo! dataset at ICML2012's Workshop \"new Challenges for Exploration & Exploitation 3\", obtaining the second place. Its basic exploration policy is deterministic in the sense that for the same input data (as a time-series) the same results are obtained. We address the deterministic exploration vs. exploitation issue, explaining the way in which the proposed method deterministically finds an effective dynamic trade-off based solely in the input-data, in contrast to other methods that use a random number generator. (C) 2013 Elsevier Ltd. All rights reserved.","Authors":"Martin, JA (Antonio Martin H, Jose) ; Vargas, AM (Vargas, Ana M.)","Title":"Linear Bayes policy for learning in contextual-bandits"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000320488200011 ISSN: 0090-5364","Keywords":"Nonparametric bandit; contextual bandit; multi-armed bandit; adaptive partition; successive elimination; sequential allocation; regret bounds KeyWords Plus:RANDOMIZED ALLOCATION; REGRET BOUNDS; CLASSIFIERS","Categories":"Mathematics Web of Science Categories:Statistics & Probability","Journal Information":"ANNALS OF STATISTICS Volume: 41 Issue: 2 Pages: 693-721 DOI: 10.1214/13-AOS1101 Published: APR 2013","Abstract":"We consider a multi-armed bandit problem in a setting where each arm produces a noisy reward realization which depends on an observable random covariate. As opposed to the traditional static multi-armed bandit problem, this setting allows for dynamically changing rewards that better describe applications where side information is available We adopt a nonparametric model where the expected rewards are smooth functions of the covariate and where the hardness of the problem is captured by a margin parameter. To maximize the expected cumulative reward, we introduce a policy called Adaptively Binned Successive Elimination (AB SE) that adaptively decomposes the global problem into suitably \"localized\" static bandit problems. This policy constructs an adaptive partition using a variant of the Successive Elimination (SE) policy. Our results include sharper regret bounds for the SE policy in a static bandit problem and minimax optimal regret bounds for the ABSE policy in the dynamic problem.","Authors":"Perchet, V (Perchet, Vianney) ; Rigollet, P (Rigollet, Philippe)","Title":"THE MULTI-ARMED BANDIT PROBLEM WITH COVARIATES"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000315981900005 ISSN: 1532-4435","Keywords":"online learning; clickthrough data; diversity; multi-armed bandits; contextual bandits; regret; metric spaces","Categories":"Automation & Control Systems; Computer Science Web of Science Categories:Automation & Control Systems; Computer Science, Artificial Intelligence","Journal Information":"JOURNAL OF MACHINE LEARNING RESEARCH Volume: 14 Pages: 399-436 Published: FEB 2013","Abstract":"Most learning to rank research has assumed that the utility of different documents is independent, which results in learned ranking functions that return redundant results. The few approaches that avoid this have rather unsatisfyingly lacked theoretical foundations, or do not scale. We present a learning-to-rank formulation that optimizes the fraction of satisfied users, with several scalable algorithms that explicitly takes document similarity and ranking context into account. Our formulation is a non-trivial common generalization of two multi-armed bandit models from the literature: ranked bandits (Radlinski et al., 2008) and Lipschitz bandits (Kleinberg et al., 2008b). We present theoretical justifications for this approach, as well as a near-optimal algorithm. Our evaluation adds optimizations that improve empirical performance, and shows that our algorithms learn orders of magnitude more quickly than previous approaches.","Authors":"Slivkins, A (Slivkins, Aleksandrs) ; Radlinski, F (Radlinski, Filip) ; Gollapudi, S (Gollapudi, Sreenivas)","Title":"Ranked Bandits in Metric Spaces: Learning Diverse Rankings over Large Document Collections"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000350802400200 ISBN:978-1-4799-3409-6 ISSN: 2474-0195","Categories":"Automation & Control Systems; Computer Science; Telecommunications Web of Science Categories:Automation & Control Systems; Computer Science, Information Systems; Telecommunications","Journal Information":"2013 51ST ANNUAL ALLERTON CONFERENCE ON COMMUNICATION, CONTROL, AND COMPUTING (ALLERTON) Book Series: Annual Allerton Conference on Communication Control and Computing Pages: 1435-1442 Published: 2013","Abstract":"Distributed, online data mining systems have emerged as a result of applications requiring analysis of large amounts of correlated and high- dimensional data produced by multiple distributed data sources. We propose a distributed online data classification framework where data is gathered by distributed data sources and processed by a heterogeneous set of distributed learners which learn online, at run- time, how to classify the different data streams either by using their locally available classification functions or by helping each other by classifying each other's data. Importantly, since the data is gathered at different locations, sending the data to another learner to process incurs additional costs such as delays, and hence this will be only beneficial if the benefits obtained from a better classification will exceed the costs. We model the problem of joint classification by the distributed and heterogeneous learners from multiple data sources as a distributed contextual bandit problem where each data is characterized by a specific context. We develop a distributed online learning algorithm for which we can prove sublinear regret. Compared to prior work in distributed online data mining, our work is the first to provide analytic regret results characterizing the performance of the proposed algorithm.","Authors":"Tekin, C (Tekin, Cem) ; van der Schaar, M (van der Schaar, Mihaela) Book Group Author(s):IEEE","Title":"Distributed Online Big Data Classification Using Context Information"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000353341700004 ISBN:978-1-4799-2528-5 ISSN: 2376-6816","Keywords":"machine learning; contextual bandit; upper confidence bound","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"2013 CONFERENCE ON TECHNOLOGIES AND APPLICATIONS OF ARTIFICIAL INTELLIGENCE (TAAI) Book Series: Conference on Technologies and Applications of Artificial Intelligence Pages: 19-24 DOI: 10.1109/TAAI.2013.18 Published: 2013","Abstract":"The contextual bandit problem is typically used to model online applications such as article recommendation. However, the problem cannot fully meet certain needs of these applications, such as performing multiple actions at the same time. We defined a new Contextual Bandit Problem with Multiple Actions (CBMA), which is an extension of the traditional contextual bandit problem and fits the online applications better. We adapt some existing contextual bandit algorithms for our CBMA problem, and developed the new Pairwise Regression with Upper Confidence Bound (PairUCB) algorithm which addresses the new properties of the new CBMA problem. Experimental results demonstrate that PairUCB significantly outperforms other approaches.","Authors":"Chang, YH (Chang, Ya-Hsuan) ; Lin, HT (Lin, Hsuan-Tien) Book Group Author(s):IEEE Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Lin, Hsuan-Tien  http://orcid.org/0000-0003-2968-0671","Title":"Pairwise Regression with Upper Confidence Bound for Contextual Bandit with Multiple Actions"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000353639700090 ISBN:978-1-4503-2240-9","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems; Computer Science, Theory & Methods","Journal Information":"2013 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM) Pages: 626-633 Published: 2013","Abstract":"Maximizing the spread and influence of the messages being published is a challenge for many social network users. Selecting the right content according to the information context and the user characteristics is essential for achieving this goal. We propose a model to automatically choose which information to publish on social networks given a set of possible messages. This model will tend to maximize the spread of the published message for a specific audience. The algorithm is based on the use of a contextual bandit model treating each new potential message as an arm to be selected. We conduct experiments on a Twitter dataset, comparing different algorithms and exploring the influence of the content and the characteristics of the messages on the information spread. The results demonstrate the model's ability to maximize the published information flow as well as it's ability to adapt its behavior to each particular audience.","Authors":"Lage, R (Lage, Ricardo) ; Gallinari, P (Denoyer, Ludovic) ; Gallinari, P (Gallinari, Patrick) ; Dolog, P (Dolog, Peter) Edited by:Ozyer, T; Carrington, P","Title":"Choosing which message to publish on social networks: A Contextual bandit approach"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000307020700013 ISSN: 1532-4435","Keywords":"multi-armed bandits; contextual bandits; exploration-exploitation; sequential allocation; Thompson sampling KeyWords Plus:MULTIARMED BANDIT; DYNAMIC ALLOCATION; ENVIRONMENTS; EXPLORATION; ALGORITHMS; POLICIES; REGRET","Categories":"Automation & Control Systems; Computer Science Web of Science Categories:Automation & Control Systems; Computer Science, Artificial Intelligence","Journal Information":"JOURNAL OF MACHINE LEARNING RESEARCH Volume: 13 Pages: 2069-2106 Published: JUN 2012","Abstract":"In sequential decision problems in an unknown environment, the decision maker often faces a dilemma over whether to explore to discover more about the environment, or to exploit current knowledge. We address the exploration-exploitation dilemma in a general setting encompassing both standard and contextualised bandit problems. The contextual bandit problem has recently resurfaced in attempts to maximise click-through rates in web based applications, a task with significant commercial interest. In this article we consider an approach of Thompson (1933) which makes use of samples from the posterior distributions for the instantaneous value of each action. We extend the approach by introducing a new algorithm, Optimistic Bayesian Sampling (OBS), in which the probability of playing an action increases with the uncertainty in the estimate of the action value. This results in better directed exploratory behaviour. We prove that, under unrestrictive assumptions, both approaches result in optimal behaviour with respect to the average reward criterion of Yang and Zhu (2002). We implement OBS and measure its performance in simulated Bernoulli bandit and linear regression domains, and also when tested with the task of personalised news article recommendation on a Yahoo! Front Page Today Module data set. We find that OBS performs competitively when compared to recently proposed benchmark algorithms and outperforms Thompson's method throughout.","Authors":"May, BC (May, Benedict C.) ; Korda, N (Korda, Nathan) ; Lee, A (Lee, Anthony) ; Leslie, DS (Leslie, David S.) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Lee, Anthony  http://orcid.org/0000-0001-7765-0616 Leslie, David Stuart  http://orcid.org/0000-0001-5253-7676","Title":"Optimistic Bayesian Sampling in Contextual-Bandit Problems"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000309341302119 ISBN:978-1-4673-1490-9 ISSN: 2161-4393","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"2012 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN) Book Series: IEEE International Joint Conference on Neural Networks (IJCNN) Published: 2012","Abstract":"In this paper we argue that contextual multi-armed bandit algorithms could open avenues for designing self. learning security modules for computer networks and related tasks. The paper has two contributions: a conceptual and an algorithmical one. The conceptual contribution is to formulate the real-world problem of preventing HTTP-based attacks on web servers as a one-shot sequential learning problem, namely as a contextual multi-armed bandit. Our second contribution is to present CMABFAS, a new and computationally very cheap algorithm for general contextual multi-armed bandit learning that specifically targets domains with finite actions. We illustrate how CMABFAS could be used to design a fully self-learning meta filter for web servers that does not rely on feedback from the end-user (i.e., does not require labeled data) and report first convincing simulation results.","Authors":"Jung, T (Jung, Tobias) ; Martin, S (Martin, Sylvain) ; Ernst, D (Ernst, Damien) ; Leduc, G (Leduc, Guy) Book Group Author(s):IEEE","Title":"Contextual Multi-armed Bandits for Web Server Defense"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000345089800040 ISBN:978-3-642-34487-9 ISSN: 0302-9743","Keywords":"Recommender system; Machine learning; Exploration/exploitation dilemma; Artificial intelligence","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Computer Science, Theory & Methods","Journal Information":"NEURAL INFORMATION PROCESSING, ICONIP 2012, PT III Book Series: Lecture Notes in Computer Science Volume: 7665 Pages: 324-331 Published: 2012","Abstract":"Most existing approaches in Mobile Context-Aware Recommender Systems focus on recommending relevant items to users taking into account contextual information, such as time, location, or social aspects. However, none of them has considered the problem of user's content evolution. We introduce in this paper an algorithm that tackles this dynamicity. It is based on dynamic exploration/exploitation and can adaptively balance the two aspects by deciding which user's situation is most relevant for exploration or exploitation. Within a deliberately designed offline simulation framework we conduct evaluations with real online event log data. The experimental results demonstrate that our algorithm outperforms surveyed algorithms.","Authors":"Bouneffouf, D (Bouneffouf, Djallel) ; Bouzeghoub, A (Bouzeghoub, Amel) ; Gancarski, AL (Gancarski, Alda Lopes) Edited by:Huang, T; Zeng, Z; Li, C; Leung, CS","Title":"A Contextual-Bandit Algorithm for Mobile Context-Aware Recommender System"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000296003300006 ISSN: 0308-597X","Keywords":"Regional-scale systems; Common-pool resources; Complexity; Coral Triangle KeyWords Plus:SOCIAL-ECOLOGICAL SYSTEMS; RESOURCE-MANAGEMENT; PROTECTED AREAS; BIODIVERSITY CONSERVATION; FISHERIES MANAGEMENT; SOLOMON-ISLANDS; SOUTHEAST-ASIA; REEF FISHERIES; ROVING BANDITS; SUSTAINABILITY","Categories":"Environmental Sciences & Ecology; International Relations Web of Science Categories:Environmental Studies; International Relations","Journal Information":"MARINE POLICY Volume: 36 Issue: 1 Pages: 42-53 DOI: 10.1016/j.marpol.2011.03.007 Published: JAN 2012","Abstract":"Environment and development agendas are increasingly being characterised by regional-scale initiatives. This trend is in part motivated by recognition of the need to account for global drivers of change (e.g., climate change, migration, and globalisation), the aspirations of achieving large-scale ecological goals (such as maintaining ecosystem processes), and reconciling potentially conflicting priorities in multi-use planning. However, regional-scale governance is challenging and there is little theoretical guidance or empirical evidence to suggest how it can be achieved. This paper uses the Institutional Analysis and Development framework to highlight the diverse contextual factors that challenge governance of a large-scale marine common, using an example of the Coral Triangle Initiative. The analysis points to the need for a critical, reflexive approach to the Coral Triangle Initiative if it is to effectively navigate diverse contexts and reconcile multiple objectives in the region. Recognising the heterogeneous, multi-scale and interlinked nature of large-scale marine systems is critical. Coping with contextual complexity will require innovative approaches that strive to be inclusive of varied perspectives and actors, enable and support effective collective-choice arrangements at lower levels of organisation, and organise and link diverse institutional arrangements at multiple scales. Large-scale marine governance will also involve a great deal of experimentation and regular adjustments to governance arrangements to account for the dynamic nature of regional commons. (C) 2011 Elsevier Ltd. All rights reserved.","Authors":"Fidelman, P (Fidelman, Pedro) ; Evans, L (Evans, Louisa) ; Fabinyi, M (Fabinyi, Michael) ; Foale, S (Foale, Simon) ; Cinner, J (Cinner, Josh) ; Rosen, F (Rosen, Franciska) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Fabinyi, Michael  G-5198-2012 http://orcid.org/0000-0001-5293-4081 Cinner, Joshua  E-8966-2011   Foale, Simon  M-5872-2014 http://orcid.org/0000-0003-0809-872X Research ID, CTBCC   O-3564-2014   Fidelman, Pedro  N-1466-2014 http://orcid.org/0000-0001-7780-0952 Cinner, Joshua  http://orcid.org/0000-0003-2675-9317","Title":"Governing large-scale marine commons: Contextual challenges in the Coral Triangle"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000292803300006 ISSN: 0921-8009 eISSN: 1873-6106","Keywords":"Common pool resources; Experimental economics; Asymmetry; Irrigation KeyWords Plus:SYSTEMS","Categories":"Environmental Sciences & Ecology; Business & Economics Web of Science Categories:Ecology; Economics; Environmental Sciences; Environmental Studies","Journal Information":"ECOLOGICAL ECONOMICS Volume: 70 Issue: 9 Pages: 1590-1598 Special Issue: SI DOI: 10.1016/j.ecolecon.2011.01.006 Published: JUL 15 2011","Abstract":"The emergence of large-scale irrigation systems has puzzled generations of social scientists, since they are particularly vulnerable to selfish rational actors who might exploit inherent asymmetries in the system (e.g. simply being the head-ender) or who might free ride on the provision of public infrastructure. As part of two related research projects that focus on how subtle social and environmental contextual variables affect the evolution and performance of institutional rules, several sets of experiments have been performed in laboratory settings at Arizona State University and in field settings in rural villages in Thailand and Colombia. In these experiments, participants make both a decision about how much to invest in public infrastructure and how much to extract from the resources generated by that public infrastructure. With both studies we find that head-enders act as stationary bandits. They do take unequal shares of the common-pool resource but if their share is very large relative to downstream participants' shares, the latter will revolt. Therefore for groups to be successful, head-enders must restrain themselves in their use of their privileged access to the common-pool resource. The comparative approach shows that this result is robust across different social and ecological contexts. (C) 2011 Elsevier B.V. All rights reserved.","Authors":"Janssen, MA (Janssen, Marco A.) ; Anderies, JM (Anderies, John M.) ; Cardenas, JC (Cardenas, Juan-Camilo) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number cardenas, juan-camilo  B-4658-2009 http://orcid.org/0000-0003-0005-7595","Title":"Head-enders as stationary bandits in asymmetric commons: Comparing irrigation experiments in the laboratory and the field"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000297588000004 ISSN: 1012-2443","Keywords":"Computational learning theory; Multi-armed bandits; Contextual bandits; UCB; PUCB; Computer Go KeyWords Plus:COMPUTER GO; EXPLORATION; AI","Categories":"Computer Science; Mathematics Web of Science Categories:Computer Science, Artificial Intelligence; Mathematics, Applied","Journal Information":"ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE Volume: 61 Issue: 3 Pages: 203-230 DOI: 10.1007/s10472-011-9258-6 Published: MAR 2011","Abstract":"A multi-armed bandit episode consists of n trials, each allowing selection of one of K arms, resulting in payoff from a distribution over [0,1] associated with that arm. We assume contextual side information is available at the start of the episode. This context enables an arm predictor to identify possible favorable arms, but predictions may be imperfect so that they need to be combined with further exploration during the episode. Our setting is an alternative to classical multi-armed bandits which provide no contextual side information, and is also an alternative to contextual bandits which provide new context each individual trial. Multi-armed bandits with episode context can arise naturally, for example in computer Go where context is used to bias move decisions made by a multi-armed bandit algorithm. The UCB1 algorithm for multi-armed bandits achieves worst-case regret bounded by O(root Knlog(n). We seek to improve this using episode context, particularly in the case where K is large. Using a predictor that places weight M (i) > 0 on arm i with weights summing to 1, we present the PUCB algorithm which achieves regret O (1/M, root n log(n) where M (auaEuro parts per thousand) is the weight on the optimal arm. We illustrate the behavior of PUCB with small simulation experiments, present extensions that provide additional capabilities for PUCB, and describe methods for obtaining suitable predictors for use with PUCB.","Authors":"Rosin, C (Rosin, Christopher D.)","Title":"Multi-armed bandits with episode context"}, {"Document Information":"Document Type:Article; Book Chapter Language:English Accession Number: WOS:000277622300003 ISBN:978-3-642-11687-2 ISSN: 1860-949X","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"INTERACTIVE COLLABORATIVE INFORMATION SYSTEMS Book Series: Studies in Computational Intelligence Volume: 281 Pages: 65-84 Published: 2010","Abstract":"This chapter presents and evaluates an online representation selection method for factored Markov decision processes (MDPs). The method addresses a special case of the feature selection problem that only considers certain subsets of features, which we call candidate representations. A motivation for the method is that it can potentially deal with problems where other structure learning algorithms are infeasible due to a large degree of the associated dynamic Bayesian network. Our method uses switch actions to select a representation and uses off-policy updating to improve the policy of representations that were not selected. We demonstrate the validity of the method by showing for a contextual bandit task and a regular MDP that given a feature set containing only a single relevant feature, we can find this feature very efficiently using the switch method. We also show for a contextual bandit task that switching between a set of relevant features and a subset of these features can outperform each of the individual representations. The reason for this is that the switch method combines the fast performance increase of the small representation with the high asymptotic performance of the large representation.","Authors":"van Seijen, H (van Seijen, Harm) ; Whiteson, S (Whiteson, Shimon) ; Kester, L (Kester, Leon) Edited by:Babuska, R; Groen, FCA","Title":"Switching between Representations in Reinforcement Learning"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000270922000012 ISBN:978-1-60558-495-9","Keywords":"Contextual bandits; associative reinforcement learning; interactive learning KeyWords Plus:CLASSIFICATION; BANDIT","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING Pages: 129-137 Published: 2009","Abstract":"We present an algorithm, called the Offset Tree, for learning to make decisions in situations where the payoff of only one choice is observed, rather than all choices. The algorithm reduces this setting to binary classification, allowing one to reuse any existing, fully supervised binary classification algorithm in this partial information setting. We show that the Offset Tree is an optimal reduction to binary classification. In particular, it has regret at most (k - 1) times the regret of the binary classifier it uses (where k is the number of choices); and no reduction to binary classification call do better. This reduction is also computationally optimal, both at training and test time, requiring just O(log(2)k) work to train on an example or make a prediction. Experiments with the Offset Tree show that it generally performs better than several alternative approaches.","Authors":"Beygelzimer, A (Beygelzimer, Alina) ; Langford, J (Langford, John) Book Group Author(s):ACM","Title":"The Offset Tree for Learning with Partial Labels"}]