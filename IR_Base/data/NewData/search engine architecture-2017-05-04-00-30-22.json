[{"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396696000003 ISSN: 2155-6377 eISSN: 2155-6385","Keywords":"Domain Specific Hidden Web Crawler (DSHWC); Domain Term Analyzer; Frequency Calculator; Indexing; Repository; Web Log","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"INTERNATIONAL JOURNAL OF INFORMATION RETRIEVAL RESEARCH Volume: 7 Issue: 2 Pages: 19-33 Special Issue: SI DOI: 10.4018/IJIRR.2017040102 Published: APR-JUN 2017","Abstract":"Now days with the advent of internet technologies and ecommerce the need for smart search engine for human life is rising. The traditional search engines are not intelligent as well as smart and thus lead to the rise in searching costs. In this paper, architecture of a vertical search engine based on the domain specific hidden web crawler is proposed. To make a least cost vertical search engine improvement in the following techniques like: searching, indexing, ranking, transaction and query interface are suggested. The domain term analyzer filters the useless information to the maximum extent and finally provides the users with high precision information. Through the experimental result it is shown that the system works on accelerating the access, computation, storage, communication time, increased efficiency and work professionally.","Authors":"Ranjan, S (Ranjan, Sudhakar) ; Bhatia, KK (Bhatia, Komal Kumar)","Title":"Design of a Least Cost (LC) Vertical Search Engine based on Domain Specific Hidden Web Crawler"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395894000015 ISSN: 1063-8210 eISSN: 1557-9999","Keywords":"Deep packet inspection or virus detection; memory architecture; network security; nonvolatile ternary content-addressable memory (TCAM); priority-decision in memory (PDM) KeyWords Plus:CONTENT-ADDRESSABLE MEMORY","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Hardware & Architecture; Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS Volume: 25 Issue: 3 Pages: 962-973 DOI: 10.1109/TVLSI.2016.2624990 Published: MAR 2017","Abstract":"Ternary content-addressable memory (TCAM)-based search engines generally need a priority encoder (PE) to select the highest priority match entry for resolving the multiple match problem due to the don't care (X) features of TCAM. In contemporary network security, TCAM-based search engines are widely used in regular expression matching across multiple packets to protect against attacks, such as by viruses and spam. However, the use of PE results in increased energy consumption for pattern updates and search operations. Instead of using PEs to determine the match, our solution is a three-phase search operation that utilizes the length information of the matched patterns to decide the longest pattern match data. This paper proposes a promising memory technology called priority-decision in memory (PDM), which eliminates the need for PEs and removes restrictions on ordering, implying that patterns can be stored in an arbitrary order without sorting their lengths. Moreover, we present a sequential input-state (SIS) scheme to disable the mass of redundant search operations in state segments on the basis of an analysis distribution of hex signatures in a virus database. Experimental results demonstrate that the PDM-based technology can improve update energy consumption of nonvolatile TCAM (nvTCAM) search engines by 36%-67%, because most of the energy in these search engines is used to reorder. By adopting the SIS-based method to avoid unnecessary search operations in a TCAM array, the search energy reduction is around 64% of nvTCAM search engines.","Authors":"Tsai, HJ (Tsai, Hsiang-Jen) ; Yang, KH (Yang, Keng-Hao) ; Peng, YC (Peng, Yin-Chi) ; Lin, CC (Lin, Chien-Chen) ; Tsao, YH (Tsao, Ya-Han) ; Chen, MF (Chen, Meng-Fan) ; Chen, TF (Chen, Tien-Fu)","Title":"Energy- Efficient TCAM Search Engine Design Using Priority- Decision in Memory Technology"}, {"Document Information":"Document Type:Review Language:English Accession Number: WOS:000393790100003 ISSN: 1556-603X eISSN: 1556-6048","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE Volume: 12 Issue: 1 Pages: 42-55 DOI: 10.1109/MCI.2016.2627670 Published: FEB 2017","Abstract":"ames are becoming increasingly indispensable, not only for fun but also to support tasks that are more serious, such as education, strategic planning, and understanding of complex phenomena. Computational intelligence- based methods are contributing significantly to this development. Learning Classifier Systems (LCS) is a pioneering computational intelligence approach that combines machine learning methods with evolutionary computation, to learn problem solutions in the form of interpretable rules. These systems offer several advantages for game applications, including a powerful and flexible agent architecture built on a knowledgebased symbolic modeling engine; modeling flexibility that allows integrating domain knowledge and different machine learning mechanisms under a single computational framework; an ability to adapt to diverse game requirements; and an ability to learn and generate creative agent behaviors in real-time dynamic environments. We present a comprehensive and dedicated survey of LCS in computer games. The survey highlights the versatility and advantages of these systems by reviewing their application in a variety of games. The survey is organized according to a general game classification and provides an opportunity to bring this important research direction into the public eye. We discuss the strengths and weaknesses of the existing approaches and provide insights into important future research directions.","Authors":"Shafi, K (Shafi, Kamran) ; Abbass, HA (Abbass, Hussein A.)","Title":"A Survey of Learning Classifier Systems in Games"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393689700006 PubMed ID: 27561754 ISSN: 0897-1889 eISSN: 1618-727X","Keywords":"Content-based image retrieval; Computer systems; Graphical user interface (GUI); Information storage and retrieval; PACS; Reproducibility of results; Software design; Multimodal information retrieval; Query fusion; Web services KeyWords Plus:RETRIEVAL; FUSION","Categories":"Radiology, Nuclear Medicine & Medical Imaging Web of Science Categories:Radiology, Nuclear Medicine & Medical Imaging","Journal Information":"JOURNAL OF DIGITAL IMAGING Volume: 30 Issue: 1 Pages: 39-48 DOI: 10.1007/s10278-016-9903-z Published: FEB 2017","Abstract":"The use of digital medical imaging systems in healthcare institutions has increased significantly, and the large amounts of data in these systems have led to the conception of powerful support tools: recent studies on content-based image retrieval (CBIR) and multimodal information retrieval in the field hold great potential in decision support, as well as for addressing multiple challenges in healthcare systems, such as computer-aided diagnosis (CAD). However, the subject is still under heavy research, and very few solutions have become part of Picture Archiving and Communication Systems (PACS) in hospitals and clinics. This paper proposes an extensible platform for multimodal medical image retrieval, integrated in an open-source PACS software with profile-based CBIR capabilities. In this article, we detail a technical approach to the problem by describing its main architecture and each sub-component, as well as the available web interfaces and the multimodal query techniques applied. Finally, we assess our implementation of the engine with computational performance benchmarks.","Authors":"Pinho, E (Pinho, Eduardo) ; Godinho, T (Godinho, Tiago) ; Valente, F (Valente, Frederico) ; Costa, C (Costa, Carlos)","Title":"A Multimodal Search Engine for Medical Imaging Studies"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396575500030 PubMed ID: 27899674 ISSN: 0305-1048 eISSN: 1362-4962","Categories":"Biochemistry & Molecular Biology Web of Science Categories:Biochemistry & Molecular Biology","Journal Information":"NUCLEIC ACIDS RESEARCH Volume: 45 Issue: D1 Pages: D200-D203 DOI: 10.1093/nar/gkw1129 Published: JAN 4 2017","Abstract":"NCBI's Conserved Domain Database (CDD) aims at annotating biomolecular sequences with the location of evolutionarily conserved protein domain footprints, and functional sites inferred from such footprints. An archive of pre-computed domain annotation is maintained for proteins tracked by NCBI's Entrez database, and live search services are offered as well. CDD curation staff supplements a comprehensive collection of protein domain and protein family models, which have been imported from external providers, with representations of selected domain families that are curated in-house and organized into hierarchical classifications of functionally distinct families and sub-families. CDD also supports comparative analyses of protein families via conserved domain architectures, and a recent curation effort focuses on providing functional characterizations of distinct subfamily architectures using SPARCLE: Subfamily Protein Architecture Labeling Engine.","Authors":"Marchler-Bauer, A (Marchler-Bauer, Aron) ; Bo, Y (Bo, Yu) ; Han, LY (Han, Lianyi) ; He, JE (He, Jane) ; Lanczycki, CJ (Lanczycki, Christopher J.) ; Lu, SN (Lu, Shennan) ; Chitsaz, F (Chitsaz, Farideh) ; Derbyshire, MK (Derbyshire, Myra K.) ; Geer, RC (Geer, Renata C.) ; Gonzales, NR (Gonzales, Noreen R.) ; Gwadz, M (Gwadz, Marc) ; Hurwitz, DI (Hurwitz, David I.) ; Lu, F (Lu, Fu) ; Marchler, GH (Marchler, Gabriele H.) ; Song, JS (Song, James S.) ; Thanki, N (Thanki, Narmada) ; Wang, ZX (Wang, Zhouxi) ; Yamashita, RA (Yamashita, Roxanne A.) ; Zhang, DC (Zhang, Dachuan) ; Zheng, CJ (Zheng, Chanjuan) ; Geer, LY (Geer, Lewis Y.) ; Bryant, SH (Bryant, Stephen H.) ...More...Less","Title":"CDD/SPARCLE: functional classification of proteins via subfamily domain architectures"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393642100005 ISSN: 1069-2509 eISSN: 1875-8835","Keywords":"Railway electric provisioning; infrastructure optimization; simulation model; cloud computing KeyWords Plus:MULTIOBJECTIVE DESIGN; DISTRIBUTION-SYSTEMS; GENETIC ALGORITHM; OPTIMIZATION; RECONFIGURATION; METHODOLOGY","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Engineering, Multidisciplinary","Journal Information":"INTEGRATED COMPUTER-AIDED ENGINEERING Volume: 24 Issue: 1 Pages: 57-72 DOI: 10.3233/ICA-160532 Published: 2017","Abstract":"Nowadays, railway infrastructure designers rely heavily on computer simulators and expert systems to model, analyze and evaluate potential deployments prior to their installation. This paper presents the railway power consumption simulator model (RPCS), a cloud-based model for the design, simulation and evaluation of railway electric infrastructures. This model integrates the parameters of an infrastructure within a search engine that generates and evaluates a set of simulations to achieve optimal designs, according to a given set of objectives and restrictions. The knowledge of the domain is represented as an ontology that translates the elements in the infrastructure into an electric circuit, which is simulated to obtain a wide range of electric metrics. In order to support the execution of thousands of scenarios in a scalable, efficient and fault-tolerant manner, this paper introduces an architecture to deploy the model in a cloud environment, and a dimensioning model to find the types and number of instances that maximize performance while minimizing the externalization costs. The resulting model is applied to a particular case study, allowing the execution of over one thousand concurrent experiments in a virtual cluster on the Amazon Elastic Compute Cloud.","Authors":"Caino-Lores, S (Caino-Lores, Silvina) ; Garcia, A (Garcia, Alberto) ; Garcia-Carballeira, F (Garcia-Carballeira, Felix) ; Carretero, J (Carretero, Jesus)","Title":"Efficient design assessment in the railway electric infrastructure domain using cloud computing"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000389100100004 ISSN: 0164-1212 eISSN: 1873-1228","Keywords":"Multi-cloud capacity allocation; Optimization; MILP KeyWords Plus:PALLADIO COMPONENT MODEL; PERFORMANCE PREDICTION; RESOURCE-MANAGEMENT; SYSTEMS; SEARCH","Categories":"Computer Science Web of Science Categories:Computer Science, Software Engineering; Computer Science, Theory & Methods","Journal Information":"JOURNAL OF SYSTEMS AND SOFTWARE Volume: 123 Pages: 64-78 DOI: 10.1016/j.jss.2016.10.001 Published: JAN 2017","Abstract":"The large success of the Cloud computing, its strong impact on the ICT world and on everyday life testifies the maturity and effectiveness this paradigm achieved in the last few years. Presently, the Cloud market offers a multitude of heterogeneous solutions. However, despite the undeniable advantages, Cloud computing introduced new issues and challenges. In particular, the heterogeneity of the available Cloud services and their pricing models makes the identification of a configuration that minimizes the operating costs of a Cloud application, guaranteeing at the same time the Quality of Service, a challenging task. This situation requires new processes and models to design software architectures and predict costs and performance considering together the large variability in price models and the intrinsic dynamism and multi-tenancy of the Cloud environments. This work aims at providing a novel mathematical approach to this problem presenting a queuing theory based Mixed Integer Linear Program (MILP) to find a promising multi-cloud configuration for a given software architecture. The effectiveness of the proposed model has been favorably evaluated against first principle heuristics currently adopted by practitioners. Furthermore, the configuration returned by the model has been also used as initial solution for a local-search based optimization engine, which exploits more accurate but time-consuming performance models. This combined approach has been shown to improve the quality of the returned solutions by 37% on average and reducing the overall search time by 50% with respect to state-of-the-art heuristics based on tiers utilization thresholds. (C) 2016 Elsevier Inc. All rights reserved.","Authors":"Ciavotta, M (Ciavotta, Michele) ; Ardagna, D (Ardagna, Danilo) ; Gibilisco, GP (Gibilisco, Giovanni Paolo) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Ardagna, Danilo  http://orcid.org/0000-0003-4224-927X Ciavotta, Michele  http://orcid.org/0000-0002-2480-966X","Title":"A mixed integer linear programming optimization approach for multi-cloud capacity allocation"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000386321800014 ISSN: 0957-4174 eISSN: 1873-6793","Keywords":"Vehicle routing problem; Simultaneous pickup and delivery; Time limit; Ant colony system; Variable neighborhood search; Metaheuristics KeyWords Plus:METAHEURISTIC ALGORITHM; HEURISTIC ALGORITHMS; DEPOT; BACKHAULS; SERVICE; POINTS; SINGLE","Categories":"Computer Science; Engineering; Operations Research & Management Science Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science","Journal Information":"EXPERT SYSTEMS WITH APPLICATIONS Volume: 66 Pages: 163-175 DOI: 10.1016/j.eswa.2016.09.017 Published: DEC 30 2016","Abstract":"Along with the progress in computer hardware architecture and computational power, in order to overcome technological bottlenecks, software applications that make use of expert and intelligent systems must race against time where nanoseconds matter in the long-awaited future. This is possible with the integration of excellent solvers to software engineering methodologies that provide optimization-based decision support for planning. Since the logistics market is growing rapidly, the optimization of routing systems is of primary concern that motivates the use of vehicle routing problem (VRP) solvers as software components integrated as an optimization engine. A critical success factor of routing optimization is quality vs. response time performance. Less time-consuming and more efficient automated processes can be achieved by employing stronger solution algorithms. This study aims to solve the Vehicle Routing Problem with Simultaneous Pickup and Delivery (VRPSPD) which is a popular extension of the basic Vehicle Routing Problem arising in real world applications where pickup and delivery operations are simultaneously taken into account to satisfy the vehicle capacity constraint with the objective of total travelled distance minimization. Since the problem is known to be NP-hard, a hybrid metaheuristic algorithm based on an ant colony system (ACS) and a variable neighborhood search (VNS) is developed for its solution. VNS is a powerful optimization algorithm that provides intensive local search. However, it lacks a memory structure. This weakness can be minimized by utilizing long term memory structure of ACS and hence the overall performance of the algorithm can be boosted. In the proposed algorithm, instead of ants, VNS releases pheromones on the edges while ants provide a perturbation mechanism for the integrated algorithm using the pheromone information in order to explore search space further and jump from local optima. The performance of the proposed ACS empowered VNS algorithm is studied on well-known benchmarks test problems taken from the open literature of VRPSPD for comparison purposes. Numerical results confirm that the developed approach is robust and very efficient in terms of both solution quality and CPU time since better results provided in a shorter time on benchmark data sets is a good performance indicator. (C) 2016 Elsevier Ltd. All rights reserved.","Authors":"Kalayci, CB (Kalayci, Can B.) ; Kaya, C (Kaya, Can) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Kalayci, Can  http://orcid.org/0000-0003-2355-7015","Title":"An ant colony system empowered variable neighborhood search algorithm for the vehicle routing problem with simultaneous pickup and delivery"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000382008700001 ISSN: 0926-8782 eISSN: 1573-7578","Keywords":"Cloud; Heterogeneous databases; SQL and NoSQL integration; Multistore query language KeyWords Plus:XML","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems; Computer Science, Theory & Methods","Journal Information":"DISTRIBUTED AND PARALLEL DATABASES Volume: 34 Issue: 4 Pages: 463-503 DOI: 10.1007/s10619-015-7185-y Published: DEC 2016","Abstract":"The blooming of different cloud data management infrastructures, specialized for different kinds of data and tasks, has led to a wide diversification of DBMS interfaces and the loss of a common programming paradigm. In this paper, we present the design of a cloud multidatastore query language (CloudMdsQL), and its query engine. CloudMdsQL is a functional SQL-like language, capable of querying multiple heterogeneous data stores (relational and NoSQL) within a single query that may contain embedded invocations to each data store's native query interface. The query engine has a fully distributed architecture, which provides important opportunities for optimization. The major innovation is that a CloudMdsQL query can exploit the full power of local data stores, by simply allowing some local data store native queries (e.g. a breadth-first search query against a graph database) to be called as functions, and at the same time be optimized, e.g. by pushing down select predicates, using bind join, performing join ordering, or planning intermediate data shipping. Our experimental validation, with three data stores (graph, document and relational) and representative queries, shows that CloudMdsQL satisfies the five important requirements for a cloud multidatastore query language.","Authors":"Kolev, B (Kolev, Boyan) ; Valduriez, P (Valduriez, Patrick) ; Bondiombouy, C (Bondiombouy, Carlyna) ; Jimenez-Peris, R (Jimenez-Peris, Ricardo) ; Pau, R (Pau, Raquel) ; Pereira, J (Pereira, Jose)","Title":"CloudMdsQL: querying heterogeneous cloud data stores with a common language"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000387897700030 ISSN: 0001-0782 eISSN: 1557-7317","Categories":"Computer Science Web of Science Categories:Computer Science, Hardware & Architecture; Computer Science, Software Engineering; Computer Science, Theory & Methods","Journal Information":"COMMUNICATIONS OF THE ACM Volume: 59 Issue: 11 Pages: 114-122 DOI: 10.1145/2996868 Published: NOV 2016","Abstract":"Datacenter workloads demand high computational capabilities, flexibility, power efficiency, and low cost. It is challenging to improve all of these factors simultaneously. To advance datacenter capabilities beyond what commodity server designs can provide, we designed and built a composable, reconfigurable hardware fabric based on field programmable gate arrays (FPGA). Each server in the fabric contains one FPGA, and all FPGAs within a 48-server rack are interconnected over a low-latency, high-bandwidth network. We describe a medium-scale deployment of this fabric on a bed of 1632 servers, and measure its effectiveness in accelerating the ranking component of the Bing web search engine. We describe the requirements and architecture of the system, detail the critical engineering challenges and solutions needed to make the system robust in the presence of failures, and measure the performance, power, and resilience of the system. Under high load, the large-scale reconfigurable fabric improves the ranking throughput of each server by 95% at a desirable latency distribution or reduces tail latency by 29% at a fixed throughput. In other words, the reconfigurable fabric enables the same throughput using only half the number of servers.","Authors":"Putnam, A (Putnam, Andrew) ; Caulfield, AM (Caulfield, Adrian M.) ; Chung, ES (Chung, Eric S.) ; Chiou, D (Chiou, Derek) ; Constantinides, K (Constantinides, Kypros) ; Demme, J (Demme, John) ; Esmaeilzadeh, H (Esmaeilzadeh, Hadi) ; Fowers, J (Fowers, Jeremy) ; Gopal, GP (Gopal, Gopi Prashanth) ; Gray, J (Gray, Jan) ; Haselman, M (Haselman, Michael) ; Hauck, S (Hauck, Scott) ; Heil, S (Heil, Stephen) ; Hormati, A (Hormati, Amir) ; Kim, JY (Kim, Joo-Young) ; Lanka, S (Lanka, Sitaram) ; Larus, J (Larus, James) ; Peterson, E (Peterson, Eric) ; Pope, S (Pope, Simon) ; Smith, A (Smith, Aaron) ; Thong, J (Thong, Jason) ; Xiao, PY (Xiao, Phillip Yi) ; Burger, D (Burger, Doug) ...More...Less","Title":"A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000383308500006 ISSN: 0167-6423 eISSN: 1872-7964","Keywords":"Exploratory search; Information retrieval; Bag-of-concepts (BoC) representation; Software architecture KeyWords Plus:LATENT SEMANTIC ANALYSIS","Categories":"Computer Science Web of Science Categories:Computer Science, Software Engineering","Journal Information":"SCIENCE OF COMPUTER PROGRAMMING Volume: 129 Pages: 72-91 Special Issue: SI DOI: 10.1016/j.scico.2016.05.005 Published: NOV 1 2016","Abstract":"Internet searches that occur in learning contexts are very different in nature from traditional \"lookup\" or \"known item\" searches: students usually perform searches to gather information about or master a certain topic, and the search engine is used as an aid in the exploration of a domain of knowledge. This paper presents SDE (Search Discover Explore), an exploratory search engine for educational resources that was built on top of the knowledge provided by Wikipedia: the set of its articles provides the search space (the set of topics that users can investigate), and the relationships between Wikipedia articles inform the suggestions that the search engine provides to students to go deeper in the exploration of a certain domain of knowledge. SDE indexes several hundreds of thousands of educational resources from high-quality Web sources, such as Project Gutenberg and Open Education Europe, among many others. This paper also reports the results of the evaluation of SDE by experts in Technology Enhanced Learning in several workshops that took place across Europe in the context of the European FP7 project iTEC. These results enable us to conclude that the exploratory search paradigm, making use of knowledge mined from Wikipedia, is a very promising approach for building information retrieval systems to be used in learning contexts. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Perez-Rodriguez, R (Perez-Rodriguez, Roberto) ; Anido-Rifon, L (Anido-Rifon, Luis) ; Gomez-Carballa, M (Gomez-Carballa, Miguel) ; Mourino-Garcia, M (Mourino-Garcia, Marcos)","Title":"Architecture of a concept-based information retrieval system for educational resources"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000380068900012 ISSN: 0020-0255 eISSN: 1872-6291","Keywords":"Deep representation; 3D Convolutional Neural Networks; 3D object understanding KeyWords Plus:MODEL RETRIEVAL; SHAPE DESCRIPTOR; RELEVANCE FEEDBACK; SEARCH ENGINE; RECOGNITION; SIMILARITY; VIEWS; MECHANISM; DATABASES; MACHINE","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"INFORMATION SCIENCES Volume: 366 Pages: 188-201 DOI: 10.1016/j.ins.2015.08.007 Published: OCT 20 2016","Abstract":"Feature engineering plays an important role in object understanding. Expressive discriminative features can guarantee the success of object understanding tasks. With remarkable ability of data abstraction, deep hierarchy architecture has the potential to represent objects. For 3D objects with multiple views, the existing deep learning methods can not handle all the views with high quality. In this paper, we propose a 3D convolutional neural network, a deep hierarchy model which has a similar structure with convolutional neural network. We employ stochastic gradient descent (SGD) method to pretrain the convolutional layer, and then a back-propagation method is proposed to fine-tune the whole network. Finally, we use the result of the two phases for 3D object retrieval. The proposed method is shown to out-perform the state-of-the-art approaches by experiments conducted on publicly available 3D object datasets. (C) 2015 Elsevier Inc. All rights reserved.","Authors":"Leng, BA (Leng, Biao) ; Liu, Y (Liu, Yu) ; Yu, K (Yu, Kai) ; Zhang, XY (Zhang, Xiangyang) ; Xiong, Z (Xiong, Zhang)","Title":"3D object understanding with 3D Convolutional Neural Networks"}, {"Document Information":"Document Type:Review Language:English Accession Number: WOS:000386879600003 ISSN: 2214-9147","Keywords":"Simulation; Virtual reality; Diver training; Ordnance disposal; Mine clearance","Categories":"Engineering Web of Science Categories:Engineering, Multidisciplinary","Journal Information":"DEFENCE TECHNOLOGY Volume: 12 Issue: 5 Pages: 367-379 DOI: 10.1016/j.dt.2016.06.001 Published: OCT 2016","Abstract":"This paper presents the results of a concept capability demonstration pilot study, the aim of which was to investigate how inexpensive gaming software and hardware technologies could be exploited in the development and evaluation of a simulator prototype for training Royal Navy mine clearance divers, specifically focusing on the detection and accurate reporting of the location and condition of underwater ordnance. The simulator was constructed using the Blender open source 3D modelling toolkit and game engine, and featured not only an interactive 3D editor for underwater scenario generation by instructors, but also a real-time, 3D After Action Review (AAR) system for formative assessment and feedback. The simulated scenarios and AAR architecture were based on early human factors observations and briefings conducted at the UK's Defence Diving School (DDS), an organisation that provides basic military diving training for all Royal Navy and Army (Royal Engineers) divers. An experimental pilot study was undertaken to determine whether or not basic navigational and mine detection components of diver performance could be improved as a result of exposing participants to the AAR system, delivered between simulated diving scenarios. The results suggest that the provision of AAR was accompanied by significant performance improvements in the positive identification of simulated underwater ordnance (in contrast to non-ordnance objects) and on participants' description of their location, their immediate in-water or seabed context and their structural condition. Only marginal improvements were found with participants' navigational performance in terms of their deviation accuracies from a pre-programmed expert search path. Overall, this project contributes to the growing corpus of evidence supporting the development of simulators that demonstrate the value of exploiting open source gaming software and the significance of adopting established games design techniques in delivering highly engaging scenarios to defence training communities. (C) 2016 The Authors. Production and hosting by Elsevier B. V. on behalf of China Ordnance Society.","Authors":"Stone, R (Stone, Robert) ; Snell, T (Snell, Timothy) ; Cooke, N (Cooke, Neil)","Title":"An inexpensive underwater mine countermeasures simulator with real-time 3D after action review"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000386554200008 PubMed ID: 27942248 ISSN: 1387-3326 eISSN: 1572-9419","Keywords":"Data models; Interactive systems; Reusability; Information architecture","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems; Computer Science, Theory & Methods","Journal Information":"INFORMATION SYSTEMS FRONTIERS Volume: 18 Issue: 5 Pages: 953-965 DOI: 10.1007/s10796-016-9658-6 Published: OCT 2016","Abstract":"Faceted browsing has become ubiquitous with modern digital libraries and online search engines, yet the process is still difficult to abstractly model in a manner that supports the development of interoperable and reusable interfaces. We propose category theory as a theoretical foundation for faceted browsing and demonstrate how the interactive process can be mathematically abstracted. Existing efforts in facet modeling are based upon set theory, formal concept analysis, and light-weight ontologies, but in many regards, they are implementations of faceted browsing rather than a specification of the basic, underlying structures and interactions. We will demonstrate that category theory allows us to specify faceted objects and study the relationships and interactions within a faceted browsing system. Resulting implementations can then be constructed through a category-theoretic lens using these models, allowing abstract comparison and communication that naturally support interoperability and reuse.","Authors":"Harris, DR (Harris, Daniel R.)","Title":"Foundations of reusable and interoperable facet models using category theory"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000384807400005 ISSN: 0334-0082 eISSN: 2191-0332","Keywords":"turbofan engine; performance diagnosis; sensor fault diagnosis; extended Kalman filter; adaptive genetic algorithm","Categories":"Engineering Web of Science Categories:Engineering, Aerospace","Journal Information":"INTERNATIONAL JOURNAL OF TURBO & JET-ENGINES Volume: 33 Issue: 3 Pages: 253-264 DOI: 10.1515/tjj-2015-0029 Published: SEP 2016","Abstract":"A hybrid diagnostic method utilizing Extended Kalman Filter (EKF) and Adaptive Genetic Algorithm (AGA) is presented for performance degradation estimation and sensor anomaly detection of turbofan engine. The EKF is used to estimate engine component performance degradation for gas path fault diagnosis. The AGA is introduced in the integrated architecture and applied for sensor bias detection. The contributions of this work are the comparisons of Kalman Filters (KF)-AGA algorithms and Neural Networks (NN)-AGA algorithms with a unified framework for gas path fault diagnosis. The NN needs to be trained off-line with a large number of prior fault mode data. When new fault mode occurs, estimation accuracy by the NN evidently decreases. However, the application of the Linearized Kalman Filter (LKF) and EKF will not be restricted in such case. The crossover factor and the mutation factor are adapted to the fitness function at each generation in the AGA, and it consumes less time to search for the optimal sensor bias value compared to the Genetic Algorithm (GA). In a word, we conclude that the hybrid EKF-AGA algorithm is the best choice for gas path fault diagnosis of turbofan engine among the algorithms discussed.","Authors":"Lu, F (Lu, Feng) ; Wang, YF (Wang, Yafan) ; Huang, JQ (Huang, Jinquan) ; Wang, QH (Wang, Qihang)","Title":"A Comparison of Hybrid Approaches for Turbofan Engine Gas Path Fault Diagnosis"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000384270800006 ISSN: 1936-7406 eISSN: 1936-7414","Keywords":"Routing; graph traversal; optimization","Categories":"Computer Science Web of Science Categories:Computer Science, Hardware & Architecture","Journal Information":"ACM TRANSACTIONS ON RECONFIGURABLE TECHNOLOGY AND SYSTEMS Volume: 9 Issue: 4 Article Number: 29 DOI: 10.1145/2892640 Published: SEP 2016","Abstract":"In addition to optimizing for long-path timing and routability, commercial FPGA routing engines must also optimize for various timing constraints, enabling users to fine tune their designs. These timing constraints involve both long-and short-path timing requirements. The intricacies of commercial FPGA architectures add difficulty to the problem of supporting such constraints. In this work, we introduce specific delay window routing as a general method for optimization during the routing stage of the FPGA design flow, which can be applied to various timing constraints constituting both long-and short-path requirements. Furthermore, we propose a key adjustment to standard FPGA routing technology for the purposes of specific delay window routing. By using dual-wave expansion instead of traditional single-wave expansion, we solve the critical issue of inaccurate delay estimation in our wave search, which would otherwise make routing according to a specific delay window difficult. Our results show that this dual-wave method can support stricter timing constraints than the standard single-wave method. For a suite of designs with constraints requiring connections to meet a target delay within 250ps, our dual-wave method could satisfy the requirement for all designs, whereas the single-wave method failed for more than two thirds of the designs. CCS Concepts: . Hardware -> Wire routing; Static timing analysis;","Authors":"Wegley, E (Wegley, Evan) ; Yi, YH (Yi, Yanhua) ; Zhang, QH (Zhang, Qinhai)","Title":"Application of Specific Delay Window Routing for Timing Optimization in FPGA Designs"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000382635400017 ISSN: 1386-7857 eISSN: 1573-7543","Keywords":"Big data analysis; Public security; Video surveillance system KeyWords Plus:WEB SEARCH ENGINES; SPARSE REPRESENTATION; IMAGE SUPERRESOLUTION; HALLUCINATION","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems; Computer Science, Theory & Methods","Journal Information":"CLUSTER COMPUTING-THE JOURNAL OF NETWORKS SOFTWARE TOOLS AND APPLICATIONS Volume: 19 Issue: 3 Pages: 1283-1292 DOI: 10.1007/s10586-016-0581-x Published: SEP 2016","Abstract":"Recently, the video data has very huge volume, taking one city for example, thousands of cameras are built of which each collects high-definition video over 24-48 GB every day with the rapidly growth; secondly, data collected includes variety of formats involving multimedia, images and other unstructured data; furthermore the valuable information contains in only a few frames called key frames of massive video data; and the last problem caused is how to improve the processing velocity of a large amount of original video with computers, so as to enhance the crime prediction and detection effectiveness of police and users. In this paper, we conclude a novel architecture for next generation public security system, and the \"front + back\" pattern is adopted to address the problems brought by the redundant construction of current public security information systems which realizes the resource consolidation of multiple IT resources, and provides unified computing and storage environment for more complex data analysis and applications such as data mining and semantic reasoning. Under the architecture, we introduce cloud computing technologies such as distributed storage and computing, data retrieval of huge and heterogeneous data, provide multiple optimized strategies to enhance the utilization of resources and efficiency of tasks. This paper also presents a novel strategy to generate a super-resolution image via multi-stage dictionaries which are trained by a cascade training process. Extensive experiments on image super-resolution validate that the proposed solution can get much better results than some state-of-the-arts ones.","Authors":"Xu, Z (Xu, Zheng) ; Mei, L (Mei, Lin) ; Hu, CP (Hu, Chuanping) ; Liu, YH (Liu, Yunhuai)","Title":"The big data analytics and applications of the surveillance system using video structured description technology"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000382349100017 ISSN: 0198-9715 eISSN: 1873-7587","Keywords":"PolarHub; Big data access; Geospatial interoperability; Scalability; Cyberinfrastructure KeyWords Plus:GEOSPATIAL CYBERINFRASTRUCTURE; LONG-TAIL; SCIENCE; INTEROPERABILITY; INFRASTRUCTURE; FUTURE; ACCESS; GIS","Categories":"Computer Science; Engineering; Environmental Sciences & Ecology; Geography; Operations Research & Management Science Web of Science Categories:Computer Science, Interdisciplinary Applications; Engineering, Environmental; Environmental Studies; Geography; Operations Research & Management Science","Journal Information":"COMPUTERS ENVIRONMENT AND URBAN SYSTEMS Volume: 59 Pages: 195-207 DOI: 10.1016/j.compenvurbsys.2016.07.004 Published: SEP 2016","Abstract":"The advancement of geospatial interoperability research has fostered the proliferation of geospatial resources that are shared and made publicly available on the Web. However, their increasingly availability has made the identification of the web signature of voluminous geospatial resources a major challenge. In this paper, we introduce our solution of a new cyberinfrastructure platform, the PolarHub, that conducts large-scale web crawling to discover distributed geospatial data and service resources and accomplish this goal efficiently and effectively. The PolarHub is built-upon a service-oriented architecture (SOA) and adopts Data Access Object (DAO)-based software design pattern to ensure the extendibility of the software system. The proposed meta-search-based seed selection and pattern-matching based crawling strategy facilitates the rapid resource identification and discovery through constraining the search scope on the Web. In addition, PolarHub introduces the Use of advanced asynchronous communication strategy, which combines client-pull and server-push to ensure high efficiency of the crawling system. These unique design features of PolarHub enable a high performance, scalable, sustainable, collaborative, and interactive platform for active geospatial data discovery. Because of OGC's widespread adoption, OGC-compliant web services become the primary search target of PolarHub. Currently, the PolarHub system is up and running and is serving various scientific community that demands geospatial data. We consider PolarHub a significant contribution to the field of information retrieval and geospatial interoperability. (C) 2016 Elsevier Ltd. All rights reserved.","Authors":"Li, WW (Li, Wenwen) ; Wang, SZ (Wang, Sizhe) ; Bhatia, V (Bhatia, Vidit)","Title":"PolarHub: A large-scale web crawling engine for OGC service discovery in cyberinfrastructure"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000375507700031 ISSN: 0957-4174 eISSN: 1873-6793","Keywords":"Evolutionary computation; Neural networks; Grammatical evolution","Categories":"Computer Science; Engineering; Operations Research & Management Science Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science","Journal Information":"EXPERT SYSTEMS WITH APPLICATIONS Volume: 56 Pages: 368-384 DOI: 10.1016/j.eswa.2016.03.012 Published: SEP 1 2016","Abstract":"This paper proposes a hybrid neuro-evolutive algorithm (NEA) that uses a compact indirect encoding scheme (IES) for representing its genotypes (a set of ten production rules of a Lindenmayer System with memory), moreover has the ability to reuse the genotypes and automatically build modular, hierarchical and recurrent neural networks. A genetic algorithm (GA) evolves a Lindenmayer System (L-System) that is used to design the neural network's architecture. This basic neural codification confers scalability and search space reduction in relation to other methods. Furthermore, the system uses a parallel genome scan engine that increases both the implicit parallelism and convergence of the GA. The fitness function of the NEA rewards economical artificial neural networks (ANNs) that are easily implemented. The NEA was tested on five real-world classification datasets and three well-known datasets for time series forecasting (TSF). The results are statistically compared against established state-of-the-art algorithms and various forecasting methods (ADANN, ARIMA, UCM, and Forecast Pro). In most cases, our NEA outperformed the other methods, delivering the most accurate classification and time series forecasting with the least computational effort. These superior results are attributed to the improved effectiveness and efficiency of NEA in the decision-making process. The result is an optimized neural network architecture for solving classification problems and simulating dynamical systems. (C) 2016 Elsevier Ltd. All rights reserved.","Authors":"de Campos, LML (Lima de Campos, Lidio Mauro) ; de Oliveira, RCL (Limao de Oliveira, Roberto Celio) ; Roisenberg, M (Roisenberg, Mauro)","Title":"Optimization of neural networks through grammatical evolution and a genetic algorithm"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000382751800001 ISSN: 1559-1131 eISSN: 1559-114X","Keywords":"Web search engines; distributed result diversification KeyWords Plus:RETRIEVAL; RANKING; IR","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems; Computer Science, Software Engineering","Journal Information":"ACM TRANSACTIONS ON THE WEB Volume: 10 Issue: 3 Article Number: 15 DOI: 10.1145/2907948 Published: AUG 2016","Abstract":"It has been shown that top-k retrieval quality can be considerably improved by taking not only relevance but also diversity into account. However, currently proposed diversification approaches have not put much attention on practical usability in large-scale settings, such as modern web search systems. In this work, we make two contributions toward this goal. First, we propose a combination of optimizations and heuristics for an implicit diversification algorithm based on the desirable facility placement principle, and present two algorithms that achieve linear complexity without compromising the retrieval effectiveness. Instead of an exhaustive comparison of documents, these algorithms first perform a clustering phase and then exploit its outcome to compose the diverse result set. Second, we describe and analyze two variants for distributed diversification in a computing cluster, for large-scale IR where the document collection is too large to keep in one node. Our contribution in this direction is pioneering, as there exists no earlier work in the literature that investigates the effectiveness and efficiency of diversification on a distributed setup. Extensive evaluations on a standard TREC framework demonstrate a competitive retrieval quality of the proposed optimizations to the baseline algorithm while reducing the processing time by more than 80% and up to 97%, and shed light on the efficiency and effectiveness tradeoffs of diversification when applied on top of a distributed architecture.","Authors":"Naini, KD (Naini, Kaweh Djafari) ; Altingovde, IS (Altingovde, Ismail Sengor) ; Siberski, W (Siberski, Wolf)","Title":"Scalable and Efficient Web Search Result Diversification"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000380112900026 ISSN: 0018-9340 eISSN: 1557-9956","Keywords":"Flash; TCAM; circuit design; routing; internet KeyWords Plus:SEARCH ENGINE; MEMORY; CIRCUITS","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Hardware & Architecture; Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON COMPUTERS Volume: 65 Issue: 8 Pages: 2652-2658 DOI: 10.1109/TC.2015.2493535 Published: AUG 1 2016","Abstract":"This paper presents a Ternary Content-addressable Memory (TCAM) design which is based on the use of floating-gate (flash) transistors. TCAMs are extensively used in high speed IP networking, and are commonly found in routers in the internet core. Traditional TCAM ICs are built using CMOS devices, and a single TCAM cell utilizes 17 transistors. In contrast, our TCAM cell utilizes only two flash transistors, thereby significantly reducing circuit area. We cover the chip-level architecture of the TCAM IC briefly, focusing mainly on the TCAM block which does fast parallel IP routing table lookup. Our flash-based TCAM (FTCAM) block is simulated in SPICE, and we show that it has a significantly lowered area compared to a CMOS based TCAM block, with a speed that can meet current (similar to 400 Gb/s) data rates that are found in the internet core.","Authors":"Fedorov, VV (Fedorov, Viacheslav V.) ; Abusultan, M (Abusultan, Monther) ; Khatri, SP (Khatri, Sunil P.) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Fedorov, Viacheslav  http://orcid.org/0000-0002-2916-7694","Title":"FTCAM: An Area-Efficient Flash-Based Ternary CAM Design"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000378050500005 PubMed ID: 27318069 ISSN: 1386-5056 eISSN: 1872-8243","Keywords":"Prediction market system; Logarithmic market scoring rules; Infectious diseases; Epidemic prediction; Real-time update; Web-based system; Wisdom of crowds KeyWords Plus:TIME-SERIES METHODS; GOOGLE FLU TRENDS; SIMULATION-MODEL; GRANULOMA-FORMATION; SOCIAL MEDIA; INFECTION; SURVEILLANCE; INFORMATION; HEALTH; TUBERCULOSIS","Categories":"Computer Science; Health Care Sciences & Services; Medical Informatics Web of Science Categories:Computer Science, Information Systems; Health Care Sciences & Services; Medical Informatics","Journal Information":"INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS Volume: 92 Pages: 35-43 DOI: 10.1016/j.ijmedinf.2016.04.014 Published: AUG 2016","Abstract":"Background: The quest for an effective system capable of monitoring and predicting the trends of epidemic diseases is a critical issue for communities worldwide. With the prevalence of Internet access, more and more researchers today are using data from both search engines and social media to improve the prediction accuracy. In particular, a prediction market system (PMS) exploits the wisdom of crowds on the Internet to effectively accomplish relatively high accuracy. Objective: This study presents the architecture of a PMS and demonstrates the matching mechanism of logarithmic market scoring rules. The system was implemented to predict infectious diseases in Taiwan with the wisdom of crowds in order to improve the accuracy of epidemic forecasting. Methods: The PMS architecture contains three design components: database clusters, market engine, and Web applications. The system accumulated knowledge from 126 health professionals for 31 weeks to predict five disease indicators: the confirmed cases of dengue fever, the confirmed cases of severe and complicated influenza, the rate of enterovirus infections, the rate of influenza-like illnesses, and the confirmed cases of severe and complicated enterovirus infection. Results: Based on the winning ratio, the PMS predicts the trends of three out of five disease indicators more accurately than does the existing system that uses the five-year average values of historical data for the same weeks. In addition, the PMS with the matching mechanism of logarithmic market scoring rules is easy to understand for health professionals and applicable to predict all the five disease indicators. Conclusions: The PMS architecture of this study affords organizations and individuals to implement it for various purposes in our society. The system can continuously update the data and improve prediction accuracy in monitoring and forecasting the trends of epidemic diseases. Future researchers could replicate and apply the PMS demonstrated in this study to more infectious diseases and wider geographical areas, especially the under-developed countries across Asia and Africa. (C) 2016 Elsevier Ireland Ltd. All rights reserved.","Authors":"Li, EY (Li, Eldon Y.) ; Tung, CY (Tung, Chen-Yuan) ; Chang, SH (Chang, Shu-Hsun)","Title":"The wisdom of crowds in action: Forecasting epidemic diseases with a web-based prediction market system"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000387787000013 ISSN: 1017-9909 eISSN: 1560-229X","Keywords":"object detection; depth extraction; edge detection; motion detection; field programmable gate array; hardware architecture KeyWords Plus:FACE DETECTION; FPGA; ARCHITECTURE; VISION","Categories":"Engineering; Optics; Imaging Science & Photographic Technology Web of Science Categories:Engineering, Electrical & Electronic; Optics; Imaging Science & Photographic Technology","Journal Information":"JOURNAL OF ELECTRONIC IMAGING Volume: 25 Issue: 4 Article Number: 041013 DOI: 10.1117/1.JEI.25.4.041013 Published: JUL 2016","Abstract":"Object detection is a major step in several computer vision applications and a requirement for most smart camera systems. Recent advances in hardware acceleration for real-time object detection feature extensive use of reconfigurable hardware [field programmable gate arrays (FPGAs)], and relevant research has produced quite fascinating results, in both the accuracy of the detection algorithms as well as the performance in terms of frames per second (fps) for use in embedded smart camera systems. Detecting objects in images, however, is a daunting task and often involves hardware-inefficient steps, both in terms of the datapath design and in terms of input/output and memory access patterns. We present how a visual-feature-directed search cascade composed of motion detection, depth computation, and edge detection, can have a significant impact in reducing the data that needs to be examined by the classification engine for the presence of an object of interest. Experimental results on a Spartan 6 FPGA platform for face detection indicate data search reduction of up to 95%, which results in the system being able to process up to 50 1024 x 768 pixels images per second with a significantly reduced number of false positives. (C) 2016 SPIE and IS&T","Authors":"Kyrkou, C (Kyrkou, Christos) ; Theocharides, T (Theocharides, Theocharis)","Title":"Accelerating object detection via a visual-feature-directed search cascade: algorithm and field programmable gate array implementation"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000380029700011 ISSN: 1063-8210 eISSN: 1557-9999","Keywords":"Content addressable memory (CAM); dual bit associative memory (DBAM); energy-efficient memory designs; low-power design; ternary CAM (TCAM) KeyWords Plus:CONTENT-ADDRESSABLE MEMORY; TERNARY CAM; SCHEME; LINE; TCAM; ARCHITECTURE; DESIGN; LOOKUP; ENGINE; SRAM","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Hardware & Architecture; Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS Volume: 24 Issue: 6 Pages: 2142-2151 DOI: 10.1109/TVLSI.2015.2503005 Published: JUN 2016","Abstract":"A ternary content addressable memory (TCAM) speeds up the search process in the memory by searching through prestored contents rather than addresses. The additional don't care (X) state makes the TCAM suitable for many network applications but the large amount of cell requirement for storage consumes high power and takes a large design area. This paper presents a novel architecture of TCAM, which prestores 2 bits of data in an up-down manner and provides multiple masking operations through a single control multimasking circuit. The proposed dual bit associative memory with match error and mask control (EMDBAM) consumes low power and selects the valid value on matchline through match error controller. The proposed design has been implemented using a standard 45-nm CMOS technology, and the extracted layout has been simulated using SPECTRE with the supply voltage at 1 V. The proposed EMDBAM can reduce the cell area by 39% compared with a basic TCAM design with a reduction of 9.6% in the energy-delay product.","Authors":"Mishra, S (Mishra, Sandeep) ; Dandapat, A (Dandapat, Anup) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Mishra, Sandeep  A-6524-2014 http://orcid.org/0000-0002-5893-9243 Dandapat, Anup  http://orcid.org/0000-0002-0997-1220","Title":"EMDBAM: A Low-Power Dual Bit Associative Memory With Match Error and Mask Control"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000374802600001 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Relation classification; Convolution neural network; Dropout; Data-driven KeyWords Plus:RELATION EXTRACTION","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 190 Pages: 1-9 DOI: 10.1016/j.neucom.2015.12.091 Published: MAY 19 2016","Abstract":"In industry, relation classification plays a significant role in today's search engine. Up to now, the state-of-the-art systems have the problems of over-reliance on the quality of handcrafted features annotated by experts and linguistic knowledge derived from linguistic analysis modules, which is costly and leads to the issue of error propagation. Currently, with the data-driven approaches attracting wide attention, deep learning achieves impressive performance in semantic processing tasks without much effort on costly features. In this work, we deal with the relation classification task utilizing a convolutional neural network (CNN) approach to automatically control feature learning from raw sentences and minimize the application of external toolkits and resources. Our proposed method has several distinct features. First, we exploit a simple but rational way to specify which input tokens are the target nominals in the input sentence, instead of Position Feature that used in other neural network relation classification systems. Secondly, a most suitable dropout strategy is used to prevent units in the neural network from co-adapting too much, which significantly reduces over-fitting and improves the performance. Eventually, using only word embedding as input features is sufficient to achieve desirable performance. Our experiments on the SemEval-2010 Task-8 dataset show that our CNN architecture without using any additional extracted features significantly outperforms the state-of-the-art systems and achieves an F1-score of 84.8% only considering the context between the two target nominals. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Qin, PD (Qin, Pengda) ; Xu, WR (Xu, Weiran) ; Guo, J (Guo, Jun)","Title":"An empirical convolutional neural network approach for semantic relation classification"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000375705500015 ISSN: 0360-0300 eISSN: 1557-7341","Keywords":"Algorithms; Performance; Parallel algorithms; biological sequence comparison; FPGA; GPU; multicores; CellBE; Intel Phi KeyWords Plus:PROTEIN DATABASE SEARCH; SMITH-WATERMAN; LINEAR-SPACE; SPEED-UP; ALIGNMENT; GPU; ARCHITECTURES; ACCELERATION; INSTRUCTIONS; SUBSEQUENCES","Categories":"Computer Science Web of Science Categories:Computer Science, Theory & Methods","Journal Information":"ACM COMPUTING SURVEYS Volume: 48 Issue: 4 Article Number: 63 DOI: 10.1145/2893488 Published: MAY 2016","Abstract":"Many bioinformatics applications, such as the optimal pairwise biological sequence comparison, demand a great quantity of computing resource, thus are excellent candidates to run in high-performance computing (HPC) platforms. In the last two decades, a large number of HPC-based solutions were proposed for this problem that run in different platforms, targeting different types of comparisons with slightly different algorithms and making the comparative analysis of these approaches very difficult. This article proposes a classification of parallel optimal pairwise sequence comparison solutions, in order to highlight their main characteristics in a unifiedway. We then discuss several HPC-based solutions, including clusters ofmulticores and accelerators such as Cell Broadband Engines (CellBEs), Field-Programmable Gate Arrays (FPGAs), Graphics Processing Units (GPUs) and Intel Xeon Phi, as well as hybrid solutions, which combine two or more platforms, providing the actual landscape of the main proposals in this area. Finally, we present open questions and perspectives in this research field.","Authors":"Sandes, EFD (De Oliveira Sandes, Edans Flavius) ; Boukerche, A (Boukerche, Azzedine) ; De Melo, ACMA (Magalhaes Alves De Melo, Alba Cristina) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Melo, Alba  A-9010-2008 http://orcid.org/0000-0001-5191-5209","Title":"Parallel Optimal Pairwise Biological Sequence Comparison: Algorithms, Platforms, and Classification"}, {"Document Information":"Document Type:Review Language:English Accession Number: WOS:000375153900009 ISSN: 1424-8220","Keywords":"search; Web of Things; Internet of Things; linked data; streaming data; observation and measurement data; sensors; entities KeyWords Plus:SENSOR WEB; NEIGHBOR DISCOVERY; SERVICE DISCOVERY; SEMANTIC WEB; LINKED DATA; INTERNET; ENGINE; MOBILE; WORLD; ARCHITECTURE","Categories":"Chemistry; Electrochemistry; Instruments & Instrumentation Web of Science Categories:Chemistry, Analytical; Electrochemistry; Instruments & Instrumentation","Journal Information":"SENSORS Volume: 16 Issue: 5 Article Number: 600 DOI: 10.3390/s16050600 Published: MAY 2016","Abstract":"The Web of Things aims to make physical world objects and their data accessible through standard Web technologies to enable intelligent applications and sophisticated data analytics. Due to the amount and heterogeneity of the data, it is challenging to perform data analysis directly; especially when the data is captured from a large number of distributed sources. However, the size and scope of the data can be reduced and narrowed down with search techniques, so that only the most relevant and useful data items are selected according to the application requirements. Search is fundamental to the Web of Things while challenging by nature in this context, e.g., mobility of the objects, opportunistic presence and sensing, continuous data streams with changing spatial and temporal properties, efficient indexing for historical and real time data. The research community has developed numerous techniques and methods to tackle these problems as reported by a large body of literature in the last few years. A comprehensive investigation of the current and past studies is necessary to gain a clear view of the research landscape and to identify promising future directions. This survey reviews the state-of-the-art search methods for the Web of Things, which are classified according to three different viewpoints: basic principles, data/knowledge representation, and contents being searched. Experiences and lessons learned from the existing work and some EU research projects related to Web of Things are discussed, and an outlook to the future research is presented.","Authors":"Zhou, YC (Zhou, Yuchao) ; De, S (De, Suparna) ; Wang, W (Wang, Wei) ; Moessner, K (Moessner, Klaus) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number De, Suparna  http://orcid.org/0000-0001-7439-6077","Title":"Search Techniques for the Web of Things: A Taxonomy and Survey"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000375211700042 PubMed ID: 27123592 ISSN: 1932-6203","Categories":"Science & Technology - Other Topics Web of Science Categories:Multidisciplinary Sciences","Journal Information":"PLOS ONE Volume: 11 Issue: 4 Article Number: e0154046 DOI: 10.1371/journal.pone.0154046 Published: APR 28 2016","Abstract":"Tackling protein interfaces with small molecules capable of modulating protein-protein interactions remains a challenge in structure-based ligand design. Particularly arduous are cases in which the epitopes involved in molecular recognition have a non-structured and discontinuous nature. Here, the basic strategy of translating continuous binding epitopes into mimetic scaffolds cannot be applied, and other innovative approaches are therefore required. We present a structure-based rational approach involving the use of a regular expression syntax inspired in the well established PROSITE to define minimal descriptors of geometric and functional constraints signifying relevant functionalities for recognition in protein interfaces of non-continuous and unstructured nature. These descriptors feed a search engine that explores the currently available three-dimensional chemical space of the Protein Data Bank (PDB) in order to identify in a straightforward manner regular architectures containing the desired functionalities, which could be used as templates to guide the rational design of small natural-like scaffolds mimicking the targeted recognition site. The application of this rescaffolding strategy to the discovery of natural scaffolds incorporating a selection of functionalities of interleukin-10 receptor-1 (IL-10R1), which are relevant for its interaction with interleukin-10 (IL-10) has resulted in the de novo design of a new class of potent IL-10 peptidomimetic ligands.","Authors":"Ruiz-Gomez, G (Ruiz-Gomez, Gloria) ; Hawkins, JC (Hawkins, John C.) ; Philipp, J (Philipp, Jenny) ; Kunze, G (Kuenze, Georg) ; Wodtke, R (Wodtke, Robert) ; Loser, R (Loeser, Reik) ; Fahmy, K (Fahmy, Karim) ; Pisabarro, MT (Pisabarro, M. Teresa) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Loser, Reik  I-9469-2016   Fahmy, Karim  B-2708-2017 http://orcid.org/0000-0002-8752-5824 Pisabarro, M. Teresa  D-4270-2012  ","Title":"Rational Structure-Based Rescaffolding Approach to De Novo Design of Interleukin 10 (IL-10) Receptor-1 Mimetics"}, {"Document Information":"Document Type:Article Language:Spanish Accession Number: WOS:000385670500003 ISSN: 2340-9711 eISSN: 2386-7027","Keywords":"Research; Architecture; web 2.0 tools; learning management systems; curation tools","Categories":"Architecture Web of Science Categories:Architecture","Journal Information":"RITA-REVISTA INDEXADA DE TEXTOS ACADEMICOS Issue: 5 Pages: 80-+ Published: APR 2016","Abstract":"Mastering the resources and tools to access information, especially that available on the internet, is a key issue for researchers in architecture. The use of visual and collaborative curation tools, enables students to organize the vast amount of online data. Moreover, they can drag and collect digital objects, filter web contents and results, thereby avoiding over-reliance on popular search engines. The aim is to enable students to familiarize themselves with an increasingly flexible and open work environment. In short, it is essential for them to incorporate technology, digital tools and online learning activities as key elements of their training, in a radical move away from outdated routines and uses.","Authors":"Agueira, SB (Blanco Agueira, Silvia)","Title":"Organization and refinement of online contents in research duties. Teaching methodology in the field of architecture"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000379055800018 ISSN: 2146-5193","Keywords":"optimization; louver-driving mechanism; high-performance architecture; energy efficient architecture; innovative daylighting system; deep-plan building KeyWords Plus:SHADING DEVICES; SOLAR; IMPACT","Categories":"Art Web of Science Categories:Art","Journal Information":"TURKISH ONLINE JOURNAL OF DESIGN ART AND COMMUNICATION Volume: 6 Pages: 174-187 DOI: 10.7456/1060ASE/018 Published: APR 2016","Abstract":"Optimum usage of daylight plays a an importatnt role in high-performance architecture and planning. Innovative daylighting systems in contemporary buildings can produce various benefits such as maximizing daylight penetration, optimizing visual comfort and reducing energy consumption. This paper is to develop a new louver-driving mechanism to optimize adoption of daylight in deep-plan building in order to meet requirements of high-performance architecture. Methodology of the paper emphasize on a experimental and quasiexperimental approach for examination of different types of louver-driving mechanism. The greater city of Tehran adopted as case study of the paper. The main goal is based on searching standards to better louver design in order to optimize energy demands by using this shading device. Thus 45 different cases with different louver width and angles and different louvers' distance from facade were chosen to analysis. Visual comfort and energy efficiency are analysed in an integrated approach. Moreover the combination of daylight and energy performances has always been an issue, as different software packages are needed to perform detailed calculations. A simplified method to overcome both issues using recent advances in software integration is explored here. All daylight, thermal and glare analysis were done in DIVA plug-in for Rhinoceros/Grasshopper which has ability to effectively calculate daylight metrics (using the Radiance/Daysim engine) and energy consumption (using the EnergyPlus engine). It can be concluded from experiments that generally, the various size of the louvers were not much different from each other and custom sizes can be selected according to the plan but any alteration in louver's angle and distance from facade were followed by changes in lighting and thermal load of room. In the case of the using fixed louver in design in Tehran's climate, distance between 0 and 13 centimeters and angle between 22.5 to 45 degrees are introduced as the optimum mode.","Authors":"Mahdavinejad, M (Mahdavinejad, Mohammadjavad) ; Mohammadi, S (Mohammadi, Sahar)","Title":"SYNTHESIS AND OPTIMIZATION OF LOUVER-DRIVING MECHANISM FOR INNOVATIVE DAYLIGHTING SYSTEM IN DEEP-PLAN BUILDING"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000371615600011 ISSN: 2095-2228 eISSN: 2095-2236","Keywords":"unstructured data; storage; analysis; index; search; RAISE process modeling KeyWords Plus:INFORMATION-RETRIEVAL; DIGITAL LIBRARY; MULTIMEDIA; ACCESS","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods","Journal Information":"FRONTIERS OF COMPUTER SCIENCE Volume: 10 Issue: 2 Pages: 353-369 DOI: 10.1007/s11704-015-5045-6 Published: APR 2016","Abstract":"Together with the big datamovement,many organizations collect their own big data and build distinctive applications. In order to provide smart services upon big data, massive variable data should be well linked and organized to form Data Ocean, which specially emphasizes the deep exploration of the relationships among unstructured data to support smart services. Currently, almost all of these applications have to deal with unstructured data by integrating various analysis and search techniques upon massive storage and processing infrastructure at the application level, which greatly increase the difficulty and cost of application development. This paper presents D-Ocean, an unstructured data management system for data ocean environment. D-Ocean has an open and scalable architecture, which consists of a core platform, pluggable components and auxiliary tools. It exploits a unified storage framework to store data in different kinds of data stores, integrates batch and incremental processing mechanisms to process unstructured data, and provides a combined search engine to conduct compound queries. Furthermore, a so-called RAISE process modeling is proposed to support the whole process of Repository, Analysis, Index, Search and Environment modeling, which can greatly simplify application development. The experiments and use cases in production demonstrate the efficiency and usability of D-Ocean.","Authors":"Zhuang, YT (Zhuang, Yueting) ; Wang, YG (Wang, Yaoguang) ; Shao, J (Shao, Jian) ; Chen, L (Chen, Ling) ; Lu, WM (Lu, Weiming) ; Sun, JL (Sun, Jianling) ; Wei, BG (Wei, Baogang) ; Wu, JQ (Wu, Jiangqin)","Title":"D-Ocean: an unstructured data management system for data ocean environment"}, {"Document Information":"Document Type:Review Language:English Accession Number: WOS:000369644500001 ISSN: 0218-1266 eISSN: 1793-6454","Keywords":"Charge sharing; content addressable memory; current-race; feedback; match line; NAND-type cell; NOR-type cell; sensing scheme KeyWords Plus:MATCH-LINE; PACKET CLASSIFICATION; LOOKUP ALGORITHM; SEARCH ENGINES; SENSING SCHEME; TERNARY CAM; HIGH-SPEED; IP LOOKUP; TCAM; ARCHITECTURE","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Hardware & Architecture; Engineering, Electrical & Electronic","Journal Information":"JOURNAL OF CIRCUITS SYSTEMS AND COMPUTERS Volume: 25 Issue: 4 Article Number: 1630002 DOI: 10.1142/S0218126616300026 Published: APR 2016","Abstract":"Content addressable memory (CAM) can perform high-speed table look-up with bit level masking capability. This feature makes CAMs extremely attractive for high-speed packet forwarding and classification in network routers. High-speed look-up implies all the CAM word entries to be accessed and compared with a search word to find a suitable match in a single clock cycle. This parallel search activity requires large energy consumption which needs to be reduced. In this paper, a review of the energy reduction techniques of CAM is presented. A comparative study of some popular techniques has been made with the help of simulations carried out in this work and published results.","Authors":"Ali, SI (Ali, Syed Iftekhar) ; Islam, MS (Islam, Md Shafiqul) ; Islam, MR (Islam, Mohammad Rakibul)","Title":"A Comprehensive Review of Energy Efficient Content Addressable Memory Circuits for Network Applications"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000370459000016 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Big data; Data storage; SSD; iSearch","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 181 Pages: 147-152 Special Issue: SI DOI: 10.1016/j.neucom.2015.06.103 Published: MAR 12 2016","Abstract":"The data storage system is central in determining the performance and cost in data mining or ITS. As the computing power of servers has increased so have the problems caused by the bottlenecks from slower storage protocol interfaces, which restrict data throughput and the accessing raw data from the physical storage systems. This paper presented new big data storage architecture to optimize the efficiency of data mining or mass surveillance by integrating a distributed and embedded searching engine inside each storage drive. By integrating the intrinsic search engine (iSearch) into the core controller chip some of the work of searching for patterns and keywords takes place inside the drive freeing up resources of a higher level host and ultimately the server. Only those drives, in which the expected pattern or keywords were detected, are analyzed by the higher level host. Not only does iSearch free up the server for other high level computing tasks it also helps preserve as the bandwidth of the big data storage interface. (C) 2015 Elsevier B.V. All rights reserved.","Authors":"Luo, JJ (Luo, Jianjun) ; Fan, LY (Fan, Lingyan) ; Li, ZH (Li, Zhenhua) ; Tsu, C (Tsu, Chris)","Title":"A new big data storage architecture with intrinsic search engines"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000377220600032 ISSN: 2158-107X eISSN: 2156-5570","Keywords":"Search engine; Data mining; Multi agent systems (MAS); Semantic mapping; Hozo","Categories":"Computer Science Web of Science Categories:Computer Science, Theory & Methods","Journal Information":"INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS Volume: 7 Issue: 3 Pages: 224-229 Published: MAR 2016","Abstract":"The process of retrieving information is becoming ambiguous day by day due to huge collection of documents present on web. A single keyword produces millions of results related to given query but these results are not up to user expectations. The search results produced from traditional text search engines may be relevant or irrelevant. The underlying reason is Web documents are HTML documents that do not contain semantic descriptors and annotations. The paper proposes multi agent architecture to produce fewer but personalized results. The purpose of the research is to provide platform for domain specific personalized search. Personalized search allows delivering web pages in accordance with user's interest and domain. The proposed architecture uses client side as well server side personalization to provide user with personalized fever but more accurate results. Multi agent search engine architecture uses the concept of semantic descriptors for acquiring knowledge about given domain and leading to personalized search results. Semantic descriptors are represented as network graph that holds relationship between given problem in form of hierarchy. This hierarchical classification is termed as Taxonomy.","Authors":"Verma, D (Verma, Disha) ; Kochar, B (Kochar, Barjesh)","Title":"Multi Agent Architecture for Search Engine"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000376268700004 ISSN: 1611-2776 eISSN: 2196-7032","Keywords":"Search engines; handwriting image retrieval; continuous learning; crowd sourcing; high-performance computing KeyWords Plus:HANDWRITTEN DOCUMENTS; WRITER IDENTIFICATION; MODEL","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"IT-INFORMATION TECHNOLOGY Volume: 58 Issue: 2 Pages: 80-88 DOI: 10.1515/itit-2015-0049 Published: MAR 2016","Abstract":"This article gives an overview of design considerations for a handwriting search engine based on pattern recognition and high-performance computing, \"Monk\". In order to satisfy multiple and often conflicting technological requirements, an architecture is used which heavily relies on high-performance computing, interactivity, and a Posix file-access model for the scientific programmers. The resulting system is able to handle billions of image files, in the order of petabytes of storage capacity, with a single mount point. Monk is operational since the year 2009.","Authors":"Schomaker, L (Schomaker, Lambert) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Schomaker, Lambert  A-9489-2008 http://orcid.org/0000-0003-2351-930X","Title":"Design considerations for a large-scale image-based text search engine in historical manuscript collections"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000365360100025 ISSN: 0956-7135 eISSN: 1873-7129","Keywords":"e-commerce; Internet; e-food; Food control; Food law enforcement; Automatization KeyWords Plus:WEB SEARCH ENGINES; HIT COUNTS","Categories":"Food Science & Technology Web of Science Categories:Food Science & Technology","Journal Information":"FOOD CONTROL Volume: 61 Pages: 204-212 DOI: 10.1016/j.foodcont.2015.09.039 Published: MAR 2016","Abstract":"The online marketplace for food products is continually expanding and all types of food and beverages can now be purchased over the internet. It is primarily the responsibility of the food business operator to ensure compliance with food safety law. However, the competent authorities are tasked with controlling the e-food sector as part of their regulatory duties to protect consumer health and to prevent fraud, regardless of the sales channel being used. For this purpose, a new software prototype concept was developed that automatically identifies and evaluates potentially non-compliant e-food products. The prototype was developed using a modular architecture comprising a research tool, an image analysis tool, and a monitoring tool. User-defined thresholds are applied to assess the reliability of the retrieved data. Results that are not deemed reliable enough can be reworked using a computer-aided evaluation interface. The research tool utilizes both internet search engines and customized search algorithms. A multi-stage filtering process is performed to limit the sites according to defined criteria (e.g. food product merchants only). The data acquisition module stores all matching data from webpages for later analysis and preservation of evidence. In another module, automatic recognition of a site's legal notice (impressum) is carried out for the respective vendor within whose online shop a potentially noncompliant food product is being offered. The image analysis tool performs logo recognition to enrich the text-based information of websites, thus providing additional visual information. The monitoring tool performs regular automated monitoring of e-food vendors, products and ingredients. The proof of principle of the prototype was achieved by conducting a web search for hazardous food products containing synephrine and caffeine. In total, 1242 product offerings on the internet for suspicious food products were identified among the 8683 search results. The software prototype has potential to enhance consumer protection and food safety with respect to e-foods. (C) 2015 Elsevier Ltd. All rights reserved.","Authors":"Krewinkel, A (Krewinkel, Alexandra) ; Sunkler, S (Suenkler, Sebastian) ; Lewandowski, D (Lewandowski, Dirk) ; Finck, N (Finck, Niklas) ; Tolg, B (Tolg, Boris) ; Kroh, LW (Kroh, Lothar W.) ; Schreiber, GA (Schreiber, Georg A.) ; Fritsche, J (Fritsche, Jan) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Lewandowski, Dirk  http://orcid.org/0000-0002-2674-9509 Sunkler, Sebastian  http://orcid.org/0000-0001-9848-1137","Title":"Concept for automated computer-aided identification and evaluation of potentially non-compliant food products traded via electronic commerce"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000369479500016 ISSN: 1063-8210 eISSN: 1557-9999","Keywords":"Circuit; convolutional codes; joint detection and decoding (JDD); multi-input multioutput (MIMO) detection; VLSI; wireless KeyWords Plus:VLSI IMPLEMENTATION; SYSTEMS","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Hardware & Architecture; Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS Volume: 24 Issue: 2 Pages: 587-599 DOI: 10.1109/TVLSI.2015.2419234 Published: FEB 2016","Abstract":"This paper presents an algorithm and a VLSI architecture of a configurable joint detection and decoding (CJDD) scheme for multi-input multioutput (MIMO) wireless communication systems with convolutional codes. A novel tree-enumeration strategy is proposed such that the MIMO detection and decoding of convolutional codes can be conducted in single stage using a tree-searching engine. Moreover, this design can be configured to support different combinations of quadrature amplitude modulation (QAM) schemes as well as encoder code rates, and thus can be more practically deployed to real-world MIMO wireless systems. A formal outline of the proposed algorithm will be given and simulation results for 16-QAM and 64-QAM with rate-1/2 and rate-1/3 codes will be presented showing that, compared with the conventional separate scheme, the CJDD algorithm can greatly improve bit error rate (BER) performance with different system settings. In addition, the VLSI architecture and implementation of the CJDD approach will be illustrated. The architectures and circuits are designed to support configurability and flexibility while maintaining high efficiency and low complexity. The postlayout experimental results for 16-QAM and 64-QAM with rate-1/2 and rate-1/3 codes show that, compared with the previous configurable design, this architecture can achieve reduced or comparable complexity with improved BER performance.","Authors":"Shen, CA (Shen, Chung-An) ; Yu, CP (Yu, Chia-Po) ; Huang, CH (Huang, Chien-Hao)","Title":"Algorithm and Architecture of Configurable Joint Detection and Decoding for MIMO Wireless Communications With Convolutional Codes"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000398536300043 ISBN:978-1-5090-1445-3 ISSN: 2330-2194","Keywords":"Feedback Control System; Bandpass Filter; ARIMA; Dynamic Resource Provisioning; Multi-tier Applications","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods","Journal Information":"2016 8TH IEEE INTERNATIONAL CONFERENCE ON CLOUD COMPUTING TECHNOLOGY AND SCIENCE (CLOUDCOM 2016) Book Series: International Conference on Cloud Computing Technology and Science Pages: 326-335 DOI: 10.1109/CloudCom.2016.56 Published: 2016","Abstract":"In the competitive world of modern web applications, performance plays a crucial role. An e-commerce company estimated that every 100ms delay reduces sales by 1 percent, and a popular search engine reported that every 500ms delay in search reduces earnings by 20 percent. The demands from users for these services can vary widely based on factors such as the time-of-day and unexpected events that can trigger flash crowds. To meet these demands web applications can be organized using a multi-tier architecture to make them modular and scalable in a cloud environment. However, a highly dynamic workload and different types of resource requirements in each tier can make it difficult to model the behavior of these applications. This presents two significant challenges to infrastructure providers: 1) to model the behavior of an application workload and provide responsive resources using dynamic resource provisioning; and 2) to maintain performance-based (response time) Service Level Agreements (SLAs). In this paper, we formulate a convex optimization problem for resource allocation, and offer a strict SLA for performance. We adopt an SLA violation cost model to formulate our optimization problem and derive the solution for dynamic resource provisioning. To achieve a strict SLA for the response time of an application, we propose a predictive model that seeks to dynamically provision resources using a Feedbackbased Control System (FCS). Our model is applicable for a broad range of multi-tier applications. We demonstrate the effective use of our model through experiments that analyze the behavior of an online auction application using a common workload benchmark.","Authors":"Nanda, S (Nanda, Saurav) ; Hacker, TJ (Hacker, Thomas J.) ; Lu, YH (Lu, Yung-Hsiang) Book Group Author(s):IEEE","Title":"Predictive Model for Dynamically Provisioning Resources in Multi-Tier Web Applications"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000389792300021 ISBN:978-3-319-50182-6; 978-3-319-50181-9 ISSN: 0302-9743","Keywords":"Software assets; Serious games; Asset repository; Asset development; Taxonomy tools; Metadata editor; Applied games; Reuse","Categories":"Computer Science Web of Science Categories:Computer Science, Interdisciplinary Applications; Computer Science, Software Engineering","Journal Information":"GAMES AND LEARNING ALLIANCE, GALA 2016 Book Series: Lecture Notes in Computer Science Volume: 10056 Pages: 235-245 DOI: 10.1007/978-3-319-50182-6_21 Published: 2016","Abstract":"This paper describes the structural architecture of the RAGE repository, which is a unique and dedicated infrastructure that provides access to a wide variety of advanced technologies (RAGE software assets) for applied game development. These software assets are reusable across a wide diversity of game engines, game platforms and programming languages. The RAGE repository allows applied game developers and studios to search for software assets for inclusion in applied games. The repository is designed as an asset life-cycle management system for defining, publishing, updating, searching and packaging for distribution of these assets. The RAGE repository provides storage space for assets and their artefacts. It will be embedded in a social platform for networking among asset developers and other users. A dedicated Asset Repository Manager provides the main functionality of the repository and its integration with other systems. Tools supporting the Asset Manager are presented and discussed. When the RAGE repository is in full operation, applied game developers will be able to easily enhance the quality of their games by including advanced game technology assets.","Authors":"Georgiev, A (Georgiev, Atanas) ; Grigorov, A (Grigorov, Alexander) ; Bontchev, B (Bontchev, Boyan) ; Boytchev, P (Boytchev, Pavel) ; Stefanov, K (Stefanov, Krassen) ; Westera, W (Westera, Wim) ; Prada, R (Prada, Rui) ; Hollin, P (Hollin, Paul) ; Ger, PM (Moreno Ger, Pablo) Edited by:Bottino, R; Jeuring, J; Veltkamp, RC","Title":"The RAGE Advanced Game Technologies Repository for Supporting Applied Game Development"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393122700002 ISSN: 1741-847X eISSN: 1741-8488","Keywords":"support vector machines; information extraction; graphical models; entity classification; relation extraction; relation classification; knowledge integration; ontological constraints; Markov random fields; distributed approach KeyWords Plus:DOCUMENTS; TEXT","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"INTERNATIONAL JOURNAL OF GRID AND UTILITY COMPUTING Volume: 7 Issue: 4 Pages: 245-256 Published: 2016","Abstract":"In this work we propose a solution for the problem of the entities and relations extraction from textual documents to build an index for a semantically oriented search engine. The approach we propose is based on the integration of statistical classifiers and ontological constraints through Markov random fields. Owing to the high computational complexity of the approach, the architecture of our system is distributed and exploits parallelisation to lower processing time. In the experimental assessment we show how the proposed system can be effectively applied to a large data set, namely BioNLP-ST 2013. While the experimental results provided in the paper refer to a biomedical application, the approach is very general and can be ported to different domains.","Authors":"Alicante, A (Alicante, Anita) ; Benerecetti, M (Benerecetti, Massimo) ; Corazza, A (Corazza, Anna) ; Silvestri, S (Silvestri, Stefano)","Title":"A distributed architecture to integrate ontological knowledge into information extraction"}, {"Categories":"Automation & Control Systems; Energy & Fuels; Engineering; Mechanics Web of Science Categories:Automation & Control Systems; Energy & Fuels; Engineering, Mechanical; Mechanics","Journal Information":"PROCEEDINGS OF THE ASME INTERNAL COMBUSTION ENGINE FALL TECHNICAL CONFERENCE, 2016 Article Number: V001T02A010 Published: 2016","Abstract":"The next generation of alternative fuels is being investigated through advanced chemical and biological production techniques for the purpose of finding suitable replacements to diesel and gasoline while lowering production costs and increasing process yields. Chemical conversion of biomass to fuels provides a plethora of pathways with a variety of fuel molecules, both novel and traditional, which may be targeted. In the search for new fuels, an initial, intuition-driven evaluation of fuel compounds with desired properties is required. Due to the high cost and significant production time needed to synthesize these materials at a scale sufficient for exhaustive testing, a predictive model would allow chemists to preemptively screen fuel properties of potentially desirable fuel candidates. Recent work has shown that predictive models, in this case artificial neural networks (ANN's) analyzing quantitative structure property relationships (QSPR's), can predict the cetane number (CN) of a proposed fuel molecule with relatively small error. A fuel's CN is a measure of its ignition quality, typically defined using prescribed ASTM standards and a cetane testing engine. Alternatively, the analogous derived cetane number (DCN), obtained using an Ignition Quality Tester (IQT), is a direct measurement alternative to the CN that uses an empirical inverse relationship to the ignition delay found in the constant volume combustion chamber apparatus. DCN data points acquired using an IQT were utilized for model validation and expansion of the experimental database used in this study. The present work improves on an existing model by optimizing the model architecture along with the key learning variables of the ANN and by making the model more generalizable to a wider variety of fuel candidate types, specifically the class of furans and furan derivatives, by including specific molecules for the model to incorporate. The new molecules considered include tetrahydrofuran, 2-methylfuran, 2-methyltetrahydrofuran, 5,5'-(furan-2-ylmethylene)bis(2-methylfuran), 5,5'-((tetrahydrofuran-2-yl)methylene)bis(2-methyltetrahydrofuran), tris(5-methylfuran-2-yl)methane, and tris(5-methyltetrahydrofuran-2-yl)methane. Model architecture adjustments improved the overall root-mean-squared error (RMSE) of the base database predictions by 5.54%. Additionally, through the targeted database expansion, it is shown that the predicted cetane number of the furan-based molecules improves on average by 49.21% (3.74 CN units) and significantly for a few of the individual molecules. This indicates that a selected subset of representative molecules can be used to extend the model's predictive accuracy to new molecular classes. The approach, bolstered by the improvements presented in this paper, enables chemists to focus on promising molecules by eliminating less favorable candidates in relation to their ignition quality.","Authors":"Kessler, T (Kessler, Travis) ; Sacia, ER (Sacia, Eric R.) ; Bell, AT (Bell, Alexis T.) ; Mack, JH (Mack, J. Hunter) Book Group Author(s):ASME","Title":"PREDICTING THE CETANE NUMBER OF FURANIC BIOFUEL CANDIDATES USING AN IMPROVED ARTIFICIAL NEURAL NETWORK BASED ON MOLECULAR STRUCTURE"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392833800006 ISSN: 1742-7185 eISSN: 1742-7193","Keywords":"cloud computing; Hadoop; MapReduce; virtual machine; search engine; cluster","Categories":"Computer Science Web of Science Categories:Computer Science, Interdisciplinary Applications","Journal Information":"International Journal of Computational Science and Engineering Volume: 12 Issue: 1 Pages: 53-57 Published: 2016","Abstract":"Recently, more and more researches have indicated that the personalised and parallelised search engine can provide users with fast and correct information from the internet. Hadoop is a software framework to process the huge dataset with more than petabyte size. Virtualisation technology can fully utilise the resources of physical machines. In this paper, we construct a virtual cluster as a Hadoop cluster by multiple virtual machines to perform multiple Nutch simultaneously. From the experimental results, the proposed virtual cluster architecture for Nutch can retrieval data rapidly and the performance enhancement is proportional to the number of virtual machines.","Authors":"Hung, CL (Hung, Che Lun) ; Lin, CY (Lin, Chun-Yuan)","Title":"Efficient parallelised search engine based on virtual cluster"}, {"Keywords":"clustering web video; video representation; Convolutional Neural Networks","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"2016 3RD NATIONAL FOUNDATION FOR SCIENCE AND TECHNOLOGY DEVELOPMENT CONFERENCE ON INFORMATION AND COMPUTER SCIENCE (NICS) Pages: 135-140 Published: 2016","Abstract":"Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems giving state-of-the-art results on recognition, detection, segmentation, classification and retrieval. Encouraged by these results, we develop our previous work [14] by implementing deep neural network architecture for extracting and representing visual features to improve the clustering quality of web video search results. Experiments were conducted on a dataset published in [14]. This dataset includes 1580 videos from 18 queries issued to the YouTube search engine. Our method exhibits significant performance improvements over the previously published result. evaluated by Entropy measure (23.27% vs. 39.46%) and Purity measure (77.09% vs. 61.50%).","Authors":"Nguyen, PQ (Phuc Quang Nguyen) ; Do, T (Tien Do) ; Nguyen-Thi, AT (Anh-Thu Nguyen-Thi) ; Ngo, TD (Thanh Duc Ngo) ; Le, DD (Le, Duy-Dinh) ; Nguyen, TAB (Tu-Anh Hoang Nguyen) Edited by:Bao, VNQ; LeHung, N; Duy, TT","Title":"Clustering Web Video Search Results with Convolutional Neural Networks"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000392504700032 ISBN:978-1-5090-0635-9","Keywords":"partial image matching; accelerator; FPGA","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"2016 IEEE SYMPOSIUM ON VLSI CIRCUITS (VLSI-CIRCUITS) Book Series: Symposium on VLSI Circuits-Digest of Papers Published: 2016","Abstract":"We propose and demonstrate an FPGA-accelerated partial-image-matching engine for massive media-data searching systems. To take advantage of FPGA, a highly parallelized and pipelined architecture with an application-specific calculation was adopted. Our prototype system achieves 32 times better runtime performance than a CPU-based solution.","Authors":"Shimizu, T (Shimizu, Takashi) ; Tomita, Y (Tomita, Yasumoto) ; Matsumura, H (Matsumura, Hidetoshi) ; Sugimura, M (Sugimura, Masahiko) ; Yamasaki, H (Yamasaki, Hironobu) ; Thach, D (Thach, David) ; Miyoshi, T (Miyoshi, Takashi) ; Baba, T (Baba, Takayuki) ; Watanabe, Y (Watanabe, Yasuhiro) ; Ike, A (Ike, Atsushi) Book Group Author(s):IEEE","Title":"An FPGA-accelerated Partial Image Matching Engine for Massive Media Data Searching Systems"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000391430100487 ISBN:978-1-5090-0396-9","Keywords":"Associative Memory; Content Addressable Memory; Sparse Clustered Networks; CAM Matrix and B-CAM KeyWords Plus:ARCHITECTURES; DESIGN","Categories":"Engineering; Telecommunications Web of Science Categories:Engineering, Electrical & Electronic; Telecommunications","Journal Information":"2016 INTERNATIONAL CONFERENCE ON COMMUNICATION AND SIGNAL PROCESSING (ICCSP), VOL. 1 Pages: 2319-2322 Published: 2016","Abstract":"One of the functional memories which stores the huge amount of data and returns the matching data address after comparing the input search data with the stored data. CAM has a single clock cycle throughput so it is faster than other search systems. CAM is also said to be as associative memory, associative storage or associative array. CAM is one type of search engine which used in associative computing, lookup tables, databases and networking. In this paper, the CAM matrix has been proposed. The proposed CAM matrix architecture is designed with BCAM. When searching, the design process is used to eliminate the parallel comparisons. While comparing with conventional design, the proposed design reduces the power consumption and increases the speed.","Authors":"Vijayalakshmi, S (Vijayalakshmi, S.) ; Elango, B (Elango, B.) ; Nagarajan, V (Nagarajan, V.) Book Group Author(s):IEEE","Title":"Content Addressable Memory Using XNOR CAM Matrix"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000391380901175 ISBN:978-1-4673-9939-5","Keywords":"Intelligent Research community; IOT; Sensor Network; Manet; Web of Things; Make in India; Made India","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"2016 INTERNATIONAL CONFERENCE ON ELECTRICAL, ELECTRONICS, AND OPTIMIZATION TECHNIQUES (ICEEOT) Pages: 1825-1828 Published: 2016","Abstract":"Research is redrawing the same picture with more clarity and resolution, a better way of view. Which is different from Invention and Discovery. Invention is outcome of Innovation and ideas that build Entity which is not in Existence, whereas Discovery is Finding and Searching hidden Technology based on some existing Innovation. Major Question Today is which should have more efforts to be taken and but obviously the Answer is Innovation. Considering Current Scenario a large Amount of Research is been done by Scientists, Research scholar in Computer Science Discovery of new species and Physics equation changing particles like neutron is done but invention is for behind even though Research done is fruitful is it so fruitful to yield results, research Question to asked is research on proper Track, do we evaluate research done in correct and better way. In this Research manuscript we have evaluated Ten Research which has been accepted in International Journals and Come with conclusion that we need for better Process to Research Evaluation. Internet of Things The innovation we present is Intelligent Research Community. Idea of intelligent Research Community is platform where research scholar presents their research performance, we propose to bring research community on web and make hardware more physically embedded with Internet of things or so called web of things as platform. The Layered Architecture of Intelligent research community is present where performance of Research becomes more physical to real world with hardware of research system like Sensor nodes research on manet, search engines search technique on web platform for everyone to test. Every layer in proposed system has a dedicated group of user who might be students, research scholar, professors and Scientists. A supervisor monitors the internal work of performance is each layer which has hierarchy of levels and a view that every layer has in seven layers within its own layer. This environment would lead user on path of effective research which definitely would be effective to cast Sweet non virtual fruits. This research work would make vision of make in India and made in India presented by India prim-minister. This Research articles emphasize on solving and turning effort in research taken at various levels to discovery making and making discovery an invention. This research presents seven research artciles that have Been accepted in International Journal for publication. The a deep analysis show that this research work donot have better performance parameter and evaluation they used in falsifying misleading user who read their research article. A presenting false positive Analysis.","Authors":"Nikam, SB (Nikam, S. B.) ; Jagtap, P (Jagtap, Prajakta) ; Pawar, SS (Pawar, Sanket S.) ; Mane, M (Mane, Madhavi) ; Manepatil, A (Manepatil, Abhijeet) ; Kadam, A (Kadam, Aniket) Book Group Author(s):IEEE","Title":"Idea of Intelligent Smart Research Community Development (Idea Innovation Invention)"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000391380901176 ISBN:978-1-4673-9939-5","Keywords":"Search Engine; Reinforment learning; agent; Image information retriveal; re-ranking; web search; contenet based image retriveal(CBIR); semi-supervised; slef organized mapping(SOM)","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"2016 INTERNATIONAL CONFERENCE ON ELECTRICAL, ELECTRONICS, AND OPTIMIZATION TECHNIQUES (ICEEOT) Pages: 1829-1837 Published: 2016","Abstract":"Information retrieval and web search present a Challenging Question to researches. Today users urge for accurate and precise hands on information from Search Machine. Interpreting of user query goal is major challenge in past and present. Numerous algorithms and Frameworks have be proposed, but fail to incorporate user aims, as query without proper intent processing retrieves irrelevant information pattern discovery has ability to solve in limitations of keyword and image disambiguates with phrase learning ie, pattern discovery. Today's search machines are based on ranking model eliminating Boolean retrieval constraint and boosting natural language use. Even though word sense and concept extraction is major challenge which comes up with keywords. Information can be presented in better way with image presentation, which is been used in news portals to communicate fastly happing news and social websites instagram Facebook, flicker. user purchase goods by sighting product images on flipkart. So today uses have sifted their approach from text based information to image based, which has given rise to research domain of image information retrieval (IIR) but large number of image attributes also give rise to Image classification ambiguity. Relevance is major factor that influence information retrieval system performance with impact precision and recall. Relevance re-ranking is methodology opted in to retrieve most optimized relevant results eliminating non-relevant. Large amount of image with associated word annotations are present on different web portals. In this research we build a semantic search engine which selects network design pattern and integrate reinformant learning approach (Agent based learning) that help in selecting information from various networks and help in network structuring with WAIR (Web Agents for Information Retrieval) Architecture at core. Agent helping in retrieving precise objects from different portals and linking them. A optimized procedure E-SimRank is been implemented to count in link semantic in network and content based knowledge learning for reinforcing better results. Performance evaluation show that proposed architecture and algorithm design present faster and relevance result. A image based recommendation system is our research outcome which contributes to image retrieval domain. The research work is been developed by studying 24 core vital articles on image retrieval and find research scope with major challenges which have common ground and need to be addressed. The found Research Analysis Query (RAQ) help in directing to study better techniques to overcome problem. Our research innovation is reinforment learning algorithm agent based system development. Existing state of art of present algorithms have been optimized with this innovation integration. Future scope of research lies to image to image base retrieval or video recommendation system.","Authors":"Malgaonkar, R (Malgaonkar, Rohit) ; Kadam, A (Kadam, Aniket) ; Prakash, D (Prakash, Devale) ; Gawali, SZ (Gawali, S. Z.) ; Pawar, SS (Pawar, Sanket S.) Book Group Author(s):IEEE","Title":"Image Re-Ranking Semantic Search Engine : Reinforcement Learning Methodology"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000391380903147 ISBN:978-1-4673-9939-5","Keywords":"Elasticsearch; Indexing; Restful","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"2016 INTERNATIONAL CONFERENCE ON ELECTRICAL, ELECTRONICS, AND OPTIMIZATION TECHNIQUES (ICEEOT) Pages: 3624-3628 Published: 2016","Abstract":"The fact that technology have changed the lives of human beings cannot be denied. It has drastically reduced the effort needed to perform a particular task and has increased the productivity and efficiency. Computers especially have been playing a very important role in almost all fields in today's world. They are used to store large amount of data in almost all sectors, be it business and industrial sectors, personal lives or any other. The research areas of science and technology uses computers to solve complex and critical problems. Information is the most important requirement of each individual. In this era of quick-growing and huge data, it has become increasingly illogical to analyse it with the help of traditional techniques or relational databases. New big data instruments, architectures and designs have come into existence to give better support to the requirements of organizations/institutions in analysing large data. Specifically, Elasticsearch, a full-text java based search engine, designed keeping cloud environment in mind solves issues of scalability, search in real time, and efficiency that relational databases were not able to address. In this paper, we present our own experience with Elasticsearch an open source, Apache Lucene based, full-text search engine that provides near real-time search ability, as well as a RESTful API for the ease of user in the field of research.","Authors":"Thacker, U (Thacker, Urvi) ; Pandey, M (Pandey, Manjusha) ; Rautaray, SS (Rautaray, Siddharth S.) Book Group Author(s):IEEE","Title":"Performance of Elasticsearch in Cloud Environment with nGram and non-nGram indexing"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000391829200062 ISBN:978-1-5090-5142-7 ISSN: 1063-6404","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Hardware & Architecture; Engineering, Electrical & Electronic","Journal Information":"PROCEEDINGS OF THE 34TH IEEE INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD) Book Series: Proceedings IEEE International Conference on Computer Design Pages: 412-415 Published: 2016","Abstract":"Historically, microprocessor instructions were designed in order to obtain high performance on integer and floating point computations. Today's applications, however, demand high performance for cloud computing, web-based search engines, network applications, and social media tasks. Such software applications involve an extensive use of hashing in their computation. Hashing can reduce the complexity of search and lookup from O(n) to O(n/k), where k bins are used. In modern microprocessors hashing is done in software. In this paper, we propose a novel hardware hash unit design for use in modern microprocessors. We present the design of the Hash Unit (HU) at the micro-architecture level. We simulate the new HU to compare its performance with a software-based hash implementation. We demonstrate a significant speed-up (up to 12X) for the HU. Furthermore, the performance scales elegantly with increasing database size and application diversity, without increasing the hardware cost.","Authors":"Fairouz, A (Fairouz, Abbas) ; Abusultan, M (Abusultan, Monther) ; Khatri, SP (Khatri, Sunil P.) Book Group Author(s):IEEE","Title":"A Novel Hardware Hash Unit Design for Modern Microprocessors"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000391391100031 ISBN:978-1-5090-3601-1 ISSN: 2375-821X","Keywords":"Search Engine; Unit Test; Test Reuse; Open Source","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Software Engineering; Engineering, Electrical & Electronic","Journal Information":"2016 IEEE 27TH INTERNATIONAL SYMPOSIUM ON SOFTWARE RELIABILITY ENGINEERING WORKSHOPS (ISSREW) Book Series: IEEE International Symposium on Software Reliability Engineering Workshops Pages: 153-159 DOI: 10.1109/ISSREW.2016.41 Published: 2016","Abstract":"Software testing as a means of early fault detection is increasingly practiced and integrated into software development activities. Despite its importance, many junior developers lack sufficient testing education and expertise. When faced with a task they are unfamiliar with, developers often use online resources to find code for adaptation or reference. Code search engines and repository mining tools have been created to assist developers, particularly less experienced ones, with finding solutions to common coding problems. However, many of these search engines lack the capability to facilitate test case searches. This has prevented developers from taking advantage of the massive analytical knowledge embodied in test cases written by other developers. In this paper we introduce TestEX, a web-based test case search engine designed to make finding useful test cases easier for developers. The strength of TestEX is the combination of three unique strategies for providing developers with useful and relevant test cases: (i) test-case identification within source code, (ii) test case categorization, and (iii) semantic mapping to the concern being searched via TestEX. Through the paper we present TestEX's Architecture, features and potential use cases. Furthermore, we report a study which was conducted to evaluate the effectiveness of our test case search engine.","Authors":"Gonzalez, D (Gonzalez, Danielle) ; Popovich, A (Popovich, Andrew) ; Mirakhorli, M (Mirakhorli, Mehdi) Book Group Author(s):IEEE","Title":"TestEX: A Search Tool for Finding and Retrieving Example Unit Tests from Open Source Projects"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000391237900045 ISBN:978-1-5090-4052-0","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"2016 IEEE 4TH INTERNATIONAL CONFERENCE ON FUTURE INTERNET OF THINGS AND CLOUD (FICLOUD 2016) Pages: 327-335 DOI: 10.1109/FiCloud.2016.53 Published: 2016","Abstract":"The Cloud age, characterized by a revolution of Data center infrastructures with powerful resources, has leveraged the \"as-a-service\" paradigms. Nowadays, the Web of Things (WoT) proposes the abstraction of real world entities in a virtual Web avatar, to acquire, process and present real-time information with the ability to connect with the real-world, and to control real things. A new generation of services can arise from these complementary paradigms. One of them is crucial for a daily basis interaction with this new smart world, Information Retrieval (IR), mostly in the form of search engines can also evolve into more powerful tools. Based on that, a new architecture of this kind of services has to be defined by the synergy and challenges that WoT imposes. We propose an IR as a service IRaaS architecture for the new era of the Web of Things. It considers the high dynamics in WoT which leverages a significant amount of changes in the IR collection to merge efficiently conventional IR concepts and the cloud power. Our IRaaS approach takes into account the big-data characteristics of WoT concerning velocity, volume, volatility, and the variety of data. The architecture has two main pillars: the indexing and analysis block, and the querying and retrieval block. We propose building it driven in three dimensions: search scope, resources, and data type. From these perspectives, we show and compare the related works and researches.","Authors":"Manta-Caro, C (Manta-Caro, Cristyan) ; Fernandez-Luna, JM (Fernandez-Luna, Juan M.) Edited by:Younas, M; Awan, I; Seah, W","Title":"Information Retrieval as-a-Service for the Web of Things: A Survey and a Proposal of IRaaS Architecture"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000391133400062 ISBN:978-1-4503-4218-6","Keywords":"Binary Code Searching; Vulnerability Matching KeyWords Plus:CODE","Categories":"Computer Science Web of Science Categories:Computer Science, Software Engineering","Journal Information":"FSE'16: PROCEEDINGS OF THE 2016 24TH ACM SIGSOFT INTERNATIONAL SYMPOSIUM ON FOUNDATIONS OF SOFTWARE ENGINEERING Pages: 678-689 DOI: 10.1145/2950290.2950350 Published: 2016","Abstract":"Binary code search has received much attention recently due to its impactful applications, e.g., plagiarism detection, malware detection and software vulnerability auditing. However, developing an effective binary code search tool is challenging due to the gigantic syntax and structural differences in binaries resulted from different compilers, architectures and OSs. In this paper, we propose BINGO-a scalable and robust binary search engine supporting various architectures and OSs. The key contribution is a selective inlining technique to capture the complete function semantics by inlining relevant library and user-defined functions. In addition, architecture and OS neutral function filtering is proposed to dramatically reduce the irrelevant target functions. Besides, we introduce length variant partial traces to model binary functions in a program structure agnostic fashion. The experimental results show that BINGO can find semantic similar functions across architecture and OS boundaries, even with the presence of program structure distortion, in a scalable manner. Using BINGO, we also discovered a zero-day vulnerability in Adobe PDF Reader, a COTS binary.","Authors":"Chandramohan, M (Chandramohan, Mahinthan) ; Xue, YX (Xue, Yinxing) ; Xu, ZZ (Xu, Zhengzi) ; Liu, Y (Liu, Yang) ; Cho, CY (Cho, Chia Yuan) ; Kuan, THB (Kuan, Tan Hee Beng) Edited by:Zimmermann, T; ClelandHuang, J; Su, Z Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Liu, Yang  D-2306-2013  ","Title":"BinGo: Cross-Architecture Cross-OS Binary Search"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390946500004 ISBN:978-1-5090-2569-5","Keywords":"Component-Based Software Engineering; Cyber-Physical Systems; Systematic Literature Review","Categories":"Computer Science Web of Science Categories:Computer Science, Software Engineering","Journal Information":"PROCEEDINGS 2016 19TH INTERNATIONAL ACM SIGSOFT SYMPOSIUM ON COMPONENT-BASED SOFTWARE ENGINEERING Pages: 23-32 DOI: 10.1109/CBSE.2016.9 Published: 2016","Abstract":"By focussing on Cyber Physical Systems (CPS), this paper investigates how component-based principles and practices are used and support the activity of architecting CPS. For doing so, by running a systematic process, we selected 49 primary studies from the most important publishers search engines. Those papers have been analyzed and their contents classified according to the Classification Framework for Component Models proposed in our previous work. The results show that the main concerns handled by CPS component models are those of integration, performance, and maintainability. The instruments to satisfy those concerns, while architecting CPS, are ad-hoc software/system architecture, model-based approaches, architectural and component languages, and design. The IEC 61499 standard with its functions block is remarkably used to drive the work on six papers. Java is the most frequently used programming language used for implementing the components. Components are deployed mostly at compile time. Interfaces are almost equally distributed into port-based and operation-based. Overall, the results show a transition of technologies and approaches used in Embedded Systems to CPS, but still lacking methods for integrated architecting, in particular in incremental development.","Authors":"Crnkovic, I (Crnkovic, Ivica) ; Malavolta, I (Malavolta, Ivano) ; Muccini, H (Muccini, Henry) ; Sharaf, M (Sharaf, Mohammad) Book Group Author(s):IEEE Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Malavolta, Ivano  E-9018-2017 http://orcid.org/0000-0001-5773-8346","Title":"On the Use of Component-Based Principles and Practices for Architecting Cyber-Physical Systems"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390690700025 ISBN:978-1-5090-2535-0","Keywords":"Question Answering; Answer Extraction; Sentence Translation; Natural Language Processing KeyWords Plus:RANKING; MODEL; COMMUNITIES; WEBSITES; NETWORK; SYSTEM","Categories":"Automation & Control Systems; Computer Science Web of Science Categories:Automation & Control Systems; Computer Science, Theory & Methods","Journal Information":"2016 3RD INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CONTROL ENGINEERING (ICISCE) Pages: 115-119 DOI: 10.1109/ICISCE.2016.35 Published: 2016","Abstract":"Question Answering provides a new manner for information query. Compared to existed search engines, Q&A systems are able to return answer for the questions represented by natural language. Hence, Q&A is always one of core reseraches in the field of information processing. In this paper, we present survey targets the state of the art in Q&A under systems and approaches. Firstly, we introduce the basic architecture of a typical Q&A system. Then, we present ten typical Q&A systems in different views including developer, supporting language, information source, answer type, etc. In detail, we investigate the key research endeavors on Q&A. We summary the exploited approaches to questions classification, questions similarity, answer extraction, answer sorting and question translation. Finally, we will also present our perspective on future directions of Q&A.","Authors":"Liu, YQ (Liu, Yaqing) ; Yi, XK (Yi, Xiaokai) ; Chen, R (Chen, Rong) ; Song, YJ (Song, Yingjie) Edited by:Li, SZ; Dai, Y; Cheng, Y","Title":"A Survey on Frameworks and Methods of Question Answering"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390717500040 ISBN:978-1-5090-2552-7","Keywords":"Ranking; Ontology; Data Mining","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"2016 INTERNATIONAL CONFERENCE ON INFORMATION COMMUNICATION AND EMBEDDED SYSTEMS (ICICES) Published: 2016","Abstract":"This paper shows the user search results with an images. Based on the given query it retrieves the image from huge database, first we give the importance for content concepts and location concepts. And also users locations (positioned by GPS) are used insert the location concepts. For the user preference using ontology but it take into consideration the semantic meaning of each keyword that expected to upgrade the retrieval accuracy. Query results with an image based search sorted by the method of ranking to access more accurate results. We present a detailed architecture and design for implementation of search engine. Here the client collects and stores locally the clickthrough data to protect privacy of the users.","Authors":"GunaNandhini, S (GunaNandhini, S.) ; Nagendran, MM (Nagendran, M. Muthu) Book Group Author(s):IEEE","Title":"AN EFFECTIVE CONTENT-BASED IMAGE RETRIEVAL FROM HUGE DATBASE SETS"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390600700039 ISBN:978-1-4503-4822-5","Keywords":"Search user interfaces; multimedia information retrieval; information visualization; Sound effects KeyWords Plus:AUDIO","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"PROCEEDINGS OF AUDIO MOSTLY 2016 - A CONFERENCE ON INTERACTION WITH SOUND IN COOPERATION WITH ACM Pages: 272-277 DOI: 10.1145/2986416.2986436 Published: 2016","Abstract":"Sound designers select the sounds they use among massive collections of recordings. They usually rely on textbased queries to narrow down a subset from these collections when looking for specific content. However, when it comes to unknown collections, this approach can fail to precisely retrieve files according to their content. We investigate an audio search engine that associates content-based features and semantic meta-data using Apache Solr deployed in a fully integrated server architecture. In order to facilitate the task of browsing the sounds, we also propose a search user interface in which the user can perform both text-based queries and visual browsing in a window where sounds are organized according to their audio features. A preliminary evaluation of the performances helped to optimize the parameters of the system.","Authors":"Urbain, G (Urbain, Gabriel) ; Moinet, A (Moinet, Alexis) ; Frisson, C (Frisson, Christian) ; Dutoit, T (Dutoit, Thierry) Book Group Author(s):ACM Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Urbain, Gabriel  http://orcid.org/0000-0003-0449-5146","Title":"A Semantic and Content-Based Search User Interface for Browsing Large Collections of Foley Sounds"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390720000058 ISBN:978-94-6252-210-7 ISSN: 2352-5401","Keywords":"Electronic commerce; network marketing; search engine optimization","Categories":"Engineering; Science & Technology - Other Topics Web of Science Categories:Engineering, Multidisciplinary; Multidisciplinary Sciences","Journal Information":"PROCEEDINGS OF THE 2016 6TH INTERNATIONAL CONFERENCE ON MACHINERY, MATERIALS, ENVIRONMENT, BIOTECHNOLOGY AND COMPUTER (MMEBC) Book Series: AER-Advances in Engineering Research Volume: 88 Pages: 277-281 Published: 2016","Abstract":"E-commerce sites using search engine optimization to improve web site in the search results in the index ranking, and thus get more sales opportunities. Among them, in the station optimization is the enterprise's own site architecture optimization, need to meet the search engine crawlers search habits; standing outside the optimization is search through the analysis of competitors and provide support for the development of enterprises and the decision-making.","Authors":"Wang, YP (Wang Yaping) Edited by:Zhang, L; Xu, D","Title":"Research on the value of search engine optimization based on Electronic Commerce"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000390093900068 ISBN:978-1-5090-0669-4","Keywords":"Query-adaptive image search; scalability; hash codes; weighted Hamming distance; relevance feedback; client server","Categories":"Computer Science Web of Science Categories:Computer Science, Hardware & Architecture; Computer Science, Information Systems; Computer Science, Software Engineering","Journal Information":"2016 SYMPOSIUM ON COLOSSAL DATA ANALYSIS AND NETWORKING (CDAN) Published: 2016","Abstract":"Scalable image search based on similarity matching has been an active topic in recent years. Currently use of web has been increased significantly for information recovery and it is challenging to extract the relevance information in less time. For this the State-of-the-art methods usually use hashing approaches to embed high-dimensional image features into given Hamming space, where result search may be executed in real time based on Hamming distance of compact binary hash codes. There are various methods based on account of query adaptive method to recover the image searching. Now days, with the increasing scale of image databases, incorporated image recovery system no longer provide sufficient quick search. In this, we work on a scalable client server architecture, which is equivalent to web search engine, for effective large scale image retrieval. In our client server architecture, images are partitioned to multiple clients and an index is built. Administrated by a controlling one server, each client matches query image in its own image sub-collection in parallel and returns the intermediate search results, a list of images related to query image. An evaluation of the results shows that our client server architecture removes the limitation of an integrated image recovery system and this can produce better results. Analysis on a Flickr image dataset and relevance feedback with client server architecture for given output illustrates perfect improvements from our projected approach.","Authors":"Mali, SP (Mali, Sapana Prakash) ; Patil, NN (Patil, Nitin N.) Book Group Author(s):IEEE","Title":"Image Searching Using Client Server Architecture"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000389720200020 ISBN:978-3-319-39380-3; 978-3-319-39381-0 ISSN: 0302-9743","Keywords":"Interactive search engine; Homemade explosives; Dark web KeyWords Plus:DARK-WEB","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems; Computer Science, Theory & Methods","Journal Information":"HUMAN ASPECTS OF INFORMATION SECURITY, PRIVACY, AND TRUST Book Series: Lecture Notes in Computer Science Volume: 9750 Pages: 221-233 DOI: 10.1007/978-3-319-39381-0_20 Published: 2016","Abstract":"This work investigates the effectiveness of a novel interactive search engine in the context of discovering and retrieving Web resources containing recipes for synthesizing Home Made Explosives (HMEs). The discovery of HME Web resources both on Surface and Dark Web is addressed as a domain-specific search problem; the architecture of the search engine is based on a hybrid infrastructure that combines two different approaches: (i) a Web crawler focused on the HME domain; (ii) the submission of HME domain-specific queries to general-purpose search engines. Both approaches are accompanied by a user-initiated post-processing classification for reducing the potential noise in the discovery results. The design of the application is built based on the distinctive nature of law enforcement agency user requirements, which dictate the interactive discovery and the accurate filtering of Web resources containing HME recipes. The experiments evaluating the effectiveness of our application demonstrate its satisfactory performance, which in turn indicates the significant potential of the adopted approaches on the HME domain.","Authors":"Kalpakis, G (Kalpakis, George) ; Tsikrika, T (Tsikrika, Theodora) ; Iliou, C (Iliou, Christos) ; Mironidis, T (Mironidis, Thodoris) ; Vrochidis, S (Vrochidis, Stefanos) ; Middleton, J (Middleton, Jonathan) ; Williamson, U (Williamson, Una) ; Kompatsiaris, I (Kompatsiaris, Ioannis) Edited by:Tryfonas, T","Title":"Interactive Discovery and Retrieval of Web Resources Containing Home Made Explosive Recipes"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000389202500024 ISSN: 1553-877X","Keywords":"Regular expression matching; deep packet inspection; pattern matching; content inspection; survey KeyWords Plus:TRAFFIC CLASSIFICATION; INTRUSION DETECTION; FINITE AUTOMATON; EFFICIENT; ARCHITECTURE; GPUS; SEARCH; ENGINE; SPACE; DFA","Categories":"Computer Science; Telecommunications Web of Science Categories:Computer Science, Information Systems; Telecommunications","Journal Information":"IEEE COMMUNICATIONS SURVEYS AND TUTORIALS Volume: 18 Issue: 4 Pages: 2991-3029 DOI: 10.1109/COMST.2016.2566669 Published: 2016","Abstract":"Deep packet inspection (DPI) is widely used in content-aware network applications such as network intrusion detection systems, traffic billing, load balancing, and government surveillance. Pattern matching is a core and critical step in DPI, which checks the payload of each packet for known signatures (patterns) in order to identify packets with certain characteristics (e.g., malicious packets that carry viruses or worms). Regular expression is the major tool for signature description due to its powerful and flexible expressive ability. However, this flexibility also brings great challenges for efficient implementation in practice. Despite of hundreds to thousands of empirical proposals, wire-speed matching for large scale regular expressions still remains a big challenge. The gap between the matching throughput and the link speed is widening with the ever-increasing network link speed and pattern scale. This survey begins with a full-scale application background of DPI and technical background of regular expression matching in order to provide a global view and essential knowledge for readers. We then analyze the challenges in regular expression matching originated from the state explosion of finite state automaton used for regular expression matching. The nature of state explosion is analyzed in details, and the state-of-the-art solutions are grouped into categories of methods to relieve state expansion and methods to avoid state explosion, suggestions are also provided for building compact and efficient automata in different scenarios. Furthermore, proposals employing parallel platforms, including field-programmable gate array, GPU, general multi-processors, and ternary content addressable memory, to accelerate the matching process are introduced and thoroughly discussed. We also provide guidelines for efficient deployment for each of these platforms.","Authors":"Xu, CC (Xu, Chengcheng) ; Chen, SH (Chen, Shuhui) ; Su, JS (Su, Jinshu) ; Yiu, SM (Yiu, S. M.) ; Hui, LCK (Hui, Lucas C. K.)","Title":"A Survey on Regular Expression Matching for Deep Packet Inspection: Applications, Algorithms, and Hardware Platforms"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000389532200013 ISBN:978-1-4673-8845-0 ISSN: 0730-3157","Keywords":"e-health Mobile System; MapReduce; Safe Food Consumption; Hadoop environment","Categories":"Computer Science Web of Science Categories:Computer Science, Interdisciplinary Applications; Computer Science, Software Engineering","Journal Information":"PROCEEDINGS 2016 IEEE 40TH ANNUAL COMPUTER SOFTWARE AND APPLICATIONS CONFERENCE WORKSHOPS (COMPSAC), VOL 2 Book Series: Proceedings International Computer Software and Applications Conference Pages: 77-82 DOI: 10.1109/COMPSAC.2016.35 Published: 2016","Abstract":"Many mobile applications on safe food consumption and e-health have been developed recently. Health conscious users highly regard such applications for safe food consumption, especially for avoiding offending foods and additives. However, there is the lack of a comprehensive database containing structured or unstructured data to support such applications. In this paper we propose the MSS, a healthy food consumption search service for mobile applications utilizing Hadoop(1) and MapReduce(2) (MR). The MSS may work as a process behind a mobile application to provide a search service for information on food and food additives. MSS works by the same logic as a search engine (SE); it crawls over Web sources cataloguing relevant information for eventual use in responding to queries from mobile applications. MSS design and development are highlighted in this paper through its system architecture, query understanding, its use of the Hadoop/MapReduce Environment, and action scripts. A case study helps displaying the virtues of MSS.","Authors":"Cifci, MA (Cifci, Mehmet Akif) ; Ertugrul, DC (Ertugrul, Duygu Celik) ; Elci, A (Elci, Atilla) Edited by:Reisman, S; Ahamed, SI; Liu, L; Milojicic, D; Claycomb, W; Matskin, M; Sato, H; Nakamura, M; Cimato, S; Lung, CH; Zhang, Z","Title":"A Search Service for Food Consumption Mobile Applications via Hadoop and MapReduce Technology"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000388117501088 ISBN:978-9-3805-4419-9","Keywords":"Knowledge Retrieval; Ontology; Search engines; Semantic web and Semantic similarity","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"PROCEEDINGS OF THE 10TH INDIACOM - 2016 3RD INTERNATIONAL CONFERENCE ON COMPUTING FOR SUSTAINABLE GLOBAL DEVELOPMENT Pages: 1357-1362 Published: 2016","Abstract":"Web documents are growing exponentially and the results retrieved by traditional search engines are marked by irrelevant and inconsistent results. In order to alleviate this problem and increase degree of relevance, there is need to move towards development of semantic search engines. Such engines produce results by focusing on meaning of context rather than structure of content. The paper describes proposed architecture of semantic search engine that takes web search results as input and refines them with notion of semantics. The output produced would be precise and relevant results in context of user's query.","Authors":"Yadav, U (Yadav, Usha) ; Narula, GS (Narula, Gagandeep Singh) ; Duhan, N (Duhan, Neelam) ; Jain, V (Jain, Vishal) Edited by:Hoda, MN Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Jain, Vishal  E-4675-2017 http://orcid.org/0000-0003-1126-7424","Title":"A Novel Approach for Precise Search Results Retrieval Based on Semantic Web Technologies"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000388117503104 ISBN:978-9-3805-4419-9","Keywords":"Stemming; Extraction; classification decision","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"PROCEEDINGS OF THE 10TH INDIACOM - 2016 3RD INTERNATIONAL CONFERENCE ON COMPUTING FOR SUSTAINABLE GLOBAL DEVELOPMENT Pages: 3430-3434 Published: 2016","Abstract":"A web crawler is a software program that scans the hypertext layout of the web pages, starting from a set of seed pages. The crawler retrieves these pages, indexed them and extracts the hyperlinks inside these pages to find out the addresses for more pages to be crawled. There is a set of problems emerging in the Automatic publication data gatherer, this paper gives a solution to the problems. A proposed architecture is given to improve the performance of the crawler by using the different feature extraction technique and the classification technique and also use search engine queries.","Authors":"Gupta, S (Gupta, Shilpi) Edited by:Hoda, MN","Title":"Design of Focused Crawler Based On Feature Extraction, Classification and Term Extraction"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000389808900004 ISBN:978-1-4503-4183-7","Keywords":"Computer network security; Deep packet inspection; Intrusion detection systems; String matching","Categories":"Computer Science; Telecommunications Web of Science Categories:Computer Science, Hardware & Architecture; Telecommunications","Journal Information":"PROCEEDINGS OF THE 2016 SYMPOSIUM ON ARCHITECTURES FOR NETWORKING AND COMMUNICATIONS SYSTEMS (ANCS'16) Pages: 37-42 DOI: 10.1145/2881025.2881031 Published: 2016","Abstract":"The increasing complexity of cyber-attacks necessitates the design of more efficient hardware architectures for real-time Intrusion Detection Systems (IDSs). String matching is the main performance-demanding component of an IDS. An effective technique to design high-performance string matching engines is to partition the target set of strings into multiple subgroups and to use a parallel string matching hardware unit for each subgroup. This paper introduces a novel pattern grouping algorithm for heterogeneous bit-split string matching architectures. The proposed algorithm presents a reliable method to estimate the correlation between strings. The correlation factors are then used to find a preferred group for each string in a seed growing approach. Experimental results demonstrate that the proposed algorithm achieves an average of 41% reduction in memory consumption compared to the best existing approach found in the literature, while offering orders of magnitude faster execution time compared to an exhaustive search.","Authors":"Vakili, S (Vakili, Shervin) ; Langlois, JMP (Langlois, J. M. Pierre) ; Boughzala, B (Boughzala, Bochra) ; Savaria, Y (Savaria, Yvon) Book Group Author(s):ACM Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Vakili, Shervin  http://orcid.org/0000-0002-4791-9298","Title":"Memory-Efficient String Matching for Intrusion Detection Systems using a High-Precision Pattern Grouping Algorithm"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000389484800043 ISBN:978-94-6252-188-9 ISSN: 2352-5401","Keywords":"Search engine; Lucene; web spider; Chinese word segmentation","Categories":"Automation & Control Systems; Engineering Web of Science Categories:Automation & Control Systems; Engineering, Multidisciplinary; Engineering, Manufacturing; Engineering, Mechanical","Journal Information":"PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON ADVANCES IN MECHANICAL ENGINEERING AND INDUSTRIAL INFORMATICS (AMEII 2016) Book Series: AER-Advances in Engineering Research Volume: 73 Pages: 212-218 Published: 2016","Abstract":"From in-depth research on the basic principles and architecture of search engine, through secondary development for Lucene development package, this paper designs an entire search engine system framework, and realizes its core modules. This system can make up for the deficiency of the existing Lucene framework, and enhance the accuracy of the search engine system, so has higher real and commercial value.","Authors":"Wan, P (Wan Pu) ; Wang, LS (Wang Lisha) Edited by:Xu, M; Zhang, K","Title":"Research and implementation of search engine based on Lucene"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000389314400007 ISBN:978-1-4673-8203-8","Keywords":"cognitive computing; natural language processing; question answering systems; questions; answers; cognitive learning","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods","Journal Information":"2016 6th International Conference - Cloud System and Big Data Engineering (Confluence) Pages: 34-39 Published: 2016","Abstract":"Technology has advanced so much that people in recent times use internet search engines as primary medium of reference for understanding of any concept and knowledge level doubts. In this paper the main idea is to present an architecture which is the new way of searching for the information present on the internet and it is based on the new era of computing i.e. cognitive computing. Cognitive computing resolves the situation where ambiguity and uncertainty is present and attempts to mimic the mechanism of human brain. Recent information requirements need an efficient mechanism which should be capable of interacting with users in a more natural way. Our system is intended to manage large amount of textual data and mining suitable knowledge. Thus resulting in a new level of refinement to search results for the user by providing exact answer to the natural language query that are specified by users. Our system will be helpful to the users because system is able to provide succinct answers which do not require end users to wade through a huge amount of data present on the internet. This paper briefly summarizes the characteristics of cognitive computing and how it is implemented in our open domain question answering system developed using open source software.","Authors":"Bhati, R (Bhati, Ruby) ; Prasad, SS (Prasad, S. S.) Edited by:Bansal, A; Singhal, A","Title":"Open Domain Question Answering System using Cognitive Computing"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000389348200043 ISBN:978-94-6252-186-5 ISSN: 2352-5401","Keywords":"Web information search; Keyword search; Ontology knowledge base; Networked automatic reasoning KeyWords Plus:ALGORITHM; PAGERANK","Categories":"Engineering; Materials Science Web of Science Categories:Engineering, Multidisciplinary; Materials Science, Multidisciplinary","Journal Information":"Proceedings of the 2016 6th International Conference on Applied Science, Engineering and Technology (ICASET) Book Series: AER-Advances in Engineering Research Volume: 77 Pages: 217-223 Published: 2016","Abstract":"With the advent of the era of big data, people face Web information overload problem. In order to efficiently and accurately obtain information on the network, Web information search technology has been developed rapidly. First in this paper, the working principle of the keyword search engines that are the traditional Web information search technology is reviewed, and then from the recall ratio and precision, reasoning search, information in a timely manner and the user experience etc, we analyze its existence disadvantages and Improvement strategy. Finally we present the train of thought and key technologies of a new Web information search that is based on networked automatic reasoning. This paper provides a train of thought for building automatic reasoning system which is adapted to the network of open, distributional and dynamic, and also puts forward a new direction for the development of Web information search technology and the architecture of next generation of search engine.","Authors":"Tang, L (Tang, Lin) Edited by:Xu, P; Si, H; Xiao, X; Zhao, XSG","Title":"Present Situation of Web Information Search and Its Improvement Strategy"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000388804600017 ISBN:978-1-5106-0205-2; 978-1-5106-0206-9 ISSN: 0277-786X","Keywords":"Archives; ground-based telescopes; software development; data management; metrics; software architecture","Categories":"Astronomy & Astrophysics; Engineering; Optics Web of Science Categories:Astronomy & Astrophysics; Engineering, Electrical & Electronic; Optics","Journal Information":"SOFTWARE AND CYBERINFRASTRUCTURE FOR ASTRONOMY IV Book Series: Proceedings of SPIE Volume: 9913 Article Number: UNSP 99130I DOI: 10.1117/12.2231719 Published: 2016","Abstract":"The Keck Observatory Archive (KOA) (https://koa.ipac.caltech.edu) curates all observations acquired at the W. M. Keck Observatory (WMKO) since it began operations in 1994, including data from eight active instruments and two decommissioned instruments. The archive is a collaboration between WMKO and the NASA Exoplanet Science Institute (NExScI). Since its inception in 2004, the science information system used at KOA has adopted an architectural approach that emphasizes software re-use and adaptability. This paper describes how KOA is currently leveraging and extending open source software components to develop new services and to support delivery of a complete set of instrument metadata, which will enable more sophisticated and extensive queries than currently possible. In August 2015, KOA deployed a program interface to discover public data from all instruments equipped with an imaging mode. The interface complies with version 2 of the Simple Imaging Access Protocol (SIAP), under development by the International Virtual Observatory Alliance (IVOA), which defines a standard mechanism for discovering images through spatial queries. The heart of the KOA service is an R-tree-based, database-indexing mechanism prototyped by the Virtual Astronomical Observatory (VAO) and further developed by the Montage Image Mosaic project, designed to provide fast access to large imaging data sets as a first step in creating wide-area image mosaics (such as mosaics of subsets of the 4.7 million images of the SDSS DR9 release). The KOA service uses the results of the spatial R-tree search to create an SQLite data database for further relational filtering. The service uses a JSON configuration file to describe the association between instrument parameters and the service query parameters, and to make it applicable beyond the Keck instruments. The images generated at the Keck telescope usually do not encode the image footprints as WCS fields in the FITS file headers. Because SIAP searches are spatial, much of the effort in developing the program interface involved processing the instrument and telescope parameters to understand how accurately we can derive the WCS information for each instrument. This knowledge is now being fed back into the KOA databases as part of a program to include complete metadata information for all imaging observations. The R-tree program was itself extended to support temporal (in addition to spatial) indexing, in response to requests from the planetary science community for a search engine to discover observations of Solar System objects. With this 3D-indexing scheme, the service performs very fast time and spatial matches between the target ephemerides, obtained from the JPL SPICE service. Our experiments indicate these matches can be more than 100 times faster than when separating temporal and spatial searches. Images of the tracks of the moving targets, overlaid with the image footprints, are computed with a new command-line visualization tool, mViewer, released with the Montage distribution. The service is currently in test and will be released in late summer 2016.","Authors":"Berriman, GB (Berriman, G. Bruce) ; Cohen, RW (Cohen, Richard W.) ; Colson, A (Colson, Andrew) ; Gelino, CR (Gelino, Christopher R.) ; Good, JC (Good, John C.) ; Kong, M (Kong, Mihseh) ; Laity, AC (Laity, Anastasia C.) ; Mader, JA (Mader, Jeffrey A.) ; Swain, MA (Swain, Melanie A.) ; Tran, HD (Tran, Hien D.) ; Wang, SY (Wang, Shin-Ywan) ...More...Less Edited by:Chiozzi, G; Guzman, JC","Title":"A case study in adaptable and reusable infrastructure at the Keck Observatory Archive: VO interfaces, moving targets, and more"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000387733800100 ISBN:978-1-4503-3603-1","Keywords":"Content-based multimedia retrieval; multimedia search","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE Pages: 1183-1186 DOI: 10.1145/2964284.2973797 Published: 2016","Abstract":"vitrivr is an open source full-stac content-based multimedia retrieval system with focus on video. Unlike the majority of the existing multimedia search solutions, vitrivr is not limited to searching in metadata, but also provides content based search and thus offers a large variety of different query modes which can be seamlessly combined: Query by sketch, which allows the user to draw a sketch of a query image and/or sketch motion paths, Query by example, keyword search, and relevance feedback. The vitrivr architecture is self-contained and addresses all aspects of multimedia search, from offline feature extraction, database management to frontend user interaction. The system is composed of three modules: a web based frontend which allows the user to input the query (e.g., add a sketch) and browse the retrieved results (vitrivr-ui), a database system designed for interactive search in large-scale multimedia collections (ADAM), and a retrieval engine that handles feature extraction and feature based retrieval (Cineast). The vitrivr source is available on GitHub under the MIT open source (and similar) licenses and is currently undergoing several upgrades as part of the Coogle Summer of Code 2016.","Authors":"Rossetto, L (Rossetto, Luca) ; Giangreco, I (Giangreco, Ivan) ; Tanase, C (Tanase, Claudiu) ; Schuldt, H (Schuldt, Heiko) Book Group Author(s):ACM","Title":"vitrivr - A Flexible Retrieval Stack Supporting Multiple Query Modes for Searching in Multimedia"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000387293800062 ISSN: 1877-0509","Keywords":"Advanced Persistent Threat; FOCA; OSINT; PII","Categories":"Computer Science; Telecommunications Web of Science Categories:Computer Science, Theory & Methods; Telecommunications","Journal Information":"11TH INTERNATIONAL CONFERENCE ON FUTURE NETWORKS AND COMMUNICATIONS (FNC 2016) / THE 13TH INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS AND PERVASIVE COMPUTING (MOBISPC 2016) / AFFILIATED WORKSHOPS Book Series: Procedia Computer Science Volume: 94 Pages: 459-464 DOI: 10.1016/j.procs.2016.08.071 Published: 2016","Abstract":"Doxing is a term derived from documents, and hence consists of collecting information on an organization or individual through social media websites, search engines, password cracking methods, social engineering tools and other sources of publicly displayed information. The main purpose of doxing attacks is to threaten, embarrass, harass and humiliate the organization or individual. Various tools are used to perform doxing. Tools such as Maltego visualize organization's architecture which helps in determining weak links within the organization. This paper discusses limitations of Maltego Chlorine CE 3.6.0 and suggests measures as to how organizations can use these tools to protect themselves from doxing attacks. (C) 2016 Published by Elsevier B.V.","Authors":"Khanna, P (Khanna, Parul) ; Zavarsky, P (Zavarsky, Pavol) ; Lindskog, D (Lindskog, Dale) Edited by:Shakshuki, E","Title":"Experimental Analysis of Tools Used for Doxing and Proposed New Transforms to Help Organizations Protect against Doxing Attacks"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000386323900006 ISBN:978-3-319-41000-5; 978-3-319-40999-3 ISSN: 0302-9743","Keywords":"Autonomous Search; Constraint programming; Constraint satisfaction; Optimization; Choice functions","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Theory & Methods","Journal Information":"ADVANCES IN SWARM INTELLIGENCE, ICSI 2016, PT I Book Series: Lecture Notes in Computer Science Volume: 9712 Pages: 56-65 DOI: 10.1007/978-3-319-41000-5_6 Published: 2016","Abstract":"Autonomous Search is a modern technique aimed at introducing self-adjusting features to problem-solvers. In the context of constraint satisfaction, the idea is to let the solver engine to autonomously replace its solving strategies by more promising ones when poor performances are identified. The replacement is controlled by a choice function, which takes decisions based on information collected during solving time. However, the design of choice functions can be done in very different ways, leading of course to very different resolution processes. In this paper, we present a performance evaluation of 16 rigorously designed choice functions. Our goal is to provide new and interesting knowledge about the behavior of such functions in autonomous search architectures. To this end, we employ a set of well-known benchmarks that share general features that may be present on most constraint satisfaction and optimization problems. We believe this information will be useful in order to design better autonomous search systems for constraint satisfaction.","Authors":"Soto, R (Soto, Ricardo) ; Crawford, B (Crawford, Broderick) ; Olivares, R (Olivares, Rodrigo) ; Niklander, S (Niklander, Stefanie) ; Olguin, E (Olguin, Eduardo) Edited by:Tan, Y; Shi, Y; Niu, B","Title":"Autonomous Search in Constraint Satisfaction via Black Hole: A Performance Evaluation Using Different Choice Functions"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000383945000036 ISSN: 1877-0509","Keywords":"Intellectual Property (IP) core; Floorplanning; Simulated Annealing (SA); Network on chip (NoC); Application specific NoC (ASNoC); System-on-chip (SoC); Network Interface; Switch; Network Component (NC)","Categories":"Computer Science; Engineering; Telecommunications Web of Science Categories:Computer Science, Theory & Methods; Engineering, Electrical & Electronic; Telecommunications","Journal Information":"PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING AND COMMUNICATIONS Book Series: Procedia Computer Science Volume: 93 Pages: 259-268 DOI: 10.1016/j.procs.2016.07.209 Published: 2016","Abstract":"In the past, shared bus based architecture was used as a communication architecture in SoC. They consume more area, power and do not meet the proper bandwidth requirement. Network on chip (NoC) is evolving as a viable communication architecture because they offer better scalability, modularity, design predictability, lower power consumption and shorter latency compared to bus based systems. Application Specific Network on chip (ASNoC) topologies are found to be superior than regular NoC topologies for designing SoC with known communication demands. In this paper, a communication centric floorplanning algorithm is proposed in which communication architecture is synthesized along with the floor plan. This is achieved by grouping IP cores based on their communication requirements in pre floorplanning stage and placing network components judiciously in the post floorplanning stage. Simulated annealing is used as a search engine to obtain the optimal location of IP cores and network components in the floorplan. (C) 2016 The Authors. Published by Elsevier B.V.","Authors":"John, JT (John, Jyothi Thomas) ; Dahare, N (Dahare, Nikhil) ; Chaithanya, B (Chaithanya, Budamagunta) ; Reuben, J (Reuben, John) Edited by:Mathew, J; DasKrishna, D; Jose, J","Title":"Communication centric floorplanning of NoC based System On Chip"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000383745100036 ISBN:978-1-61499-619-4; 978-1-61499-618-7 ISSN: 0922-6389","Keywords":"Linux; search engine; TCP; Redis; text-mining","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods","Journal Information":"FUZZY SYSTEM AND DATA MINING Book Series: Frontiers in Artificial Intelligence and Applications Volume: 281 Pages: 264-270 DOI: 10.3233/978-1-61499-619-4-264 Published: 2016","Abstract":"The search engine discussed in this paper is a directory search engine. It is used to solve issues of cache and performance optimization. It provides website search services for education resources of primary and secondary schools. Optimizations from the architecture design to underlying details of each level are as follows: TCP protocol and epoll multiplexer were used, search words correction mechanism was adopted for error correction and association recommendation, multiple index coordination mechanisms were used instead of the traditional single index, Redis was used to handle query history cache instead of traditional memcached systems. The experimental data is from Fudan University corpus. The test method is a comparison between longitudinal and traversal tests. It is found that the average service time has been reduced from 5ms to less than 1ms; the actual performance has been improved nearly 100 times when compared with the previous average results.","Authors":"Liu, C (Liu, Chao) ; Yin, SQ (Yin, Shi Qun) ; Sun, MM (Sun, Meng Meng) ; Gao, S (Gao, Sheng) Edited by:Chen, G; Liu, F; Shojafar, M","Title":"Lightweight Search Engine Based on Text-Mining"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000382042200073 ISBN:978-1-4673-9292-1","Categories":"Engineering; Telecommunications Web of Science Categories:Engineering, Electrical & Electronic; Telecommunications","Journal Information":"2016 13TH IEEE ANNUAL CONSUMER COMMUNICATIONS & NETWORKING CONFERENCE (CCNC) Published: 2016","Abstract":"Packet classification has become increasingly complex and important to network equipment intended for future use. A recent trend to achieve complex packet classification is to use software-based methods, which tend to be slower than hardwarebased methods. For search, this typically means using ternary content-addressable memory (TeAM) to make classification feasible. However, TeAM is not well-suited to the long (in bits) and sparse rules used for running advanced applications that require complicated classification. We propose a multi-dimension search engine (MDSE) that is optimized for use with long, sparse rules, and we propose a multi-dimensional TeAM scheme, which is an MDSE constructed to operate on TeAM. Through fine-grained simulations with real traffic, we show that our proposed search engine can reduce the power consumed by network equipment by about 85%.","Authors":"Nawa, M (Nawa, Masami) ; Okuda, K (Okuda, Kenzo) ; Ata, S (Ata, Shingo) ; Kuroda, Y (Kuroda, Yasuto) ; Yano, Y (Yano, Yuji) ; Iwamoto, H (Iwamoto, Hisashi) ; Inoue, K (Inoue, Kazunari) ; Oka, I (Oka, Ikuo) Book Group Author(s):IEEE","Title":"Energy-efficient High-speed Search Engine Using a Multi-dimensional TeAM Architecture with Parallel Pipelined Subdivided Structure"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000382670200116 ISBN:978-1-5090-0641-0 ISSN: 2472-6737","Categories":"Computer Science; Engineering; Optics Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Optics","Journal Information":"2016 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV 2016) Book Series: IEEE Winter Conference on Applications of Computer Vision Published: 2016","Abstract":"In this paper, we introduce an FPGA-accelerated partial image retrieval engine, suitable for a visualized document search system. To achieve efficient sharing and reuse of digitized documents, this system has the function of partial duplicate image retrieval. To meet the demand for stability and speed, we introduced a brute-force matching of the BRIEF descriptor with two step scheme and an FPGA accelerator with a thoroughly parallelized and pipelined architecture to educe the potential of the FPGA. The FPGA accelerator significantly improves the runtime performance of the engine. It is about 65 times better than a CPU-based solution on average. Ultimately, we developed a prototype document search system, and the proposed engine contributes to the intuitive retrieval and quick response of the system.","Authors":"Matsumura, H (Matsumura, Hidetoshi) ; Sugimura, M (Sugimura, Masahiko) ; Yamasaki, H (Yamasaki, Hironobu) ; Tomita, Y (Tomita, Yasumoto) ; Baba, T (Baba, Takayuki) ; Watanabe, Y (Watanabe, Yasuhiro) Book Group Author(s):IEEE","Title":"An FPGA-accelerated Partial Duplicate Image Retrieval Engine for a Document Search System"}, {"Document Information":"Document Type:Article Language:Portuguese Accession Number: WOS:000379627300001 ISSN: 1562-4730","Keywords":"Information architecture; Data base; PubMed","Categories":"Information Science & Library Science Web of Science Categories:Information Science & Library Science","Journal Information":"BIBLIOS-REVISTA DE BIBLIOTECOLOGIA Y CIENCIAS DE LA INFORMACION Issue: 63 Pages: 1-12 Published: 2016","Abstract":"Objective. Based on principles proposed by Rosenfeld and Morville (2006), the present study examined the PubMed database interface, since a well-structured information architecture contributes to good usability in any digital environment. Method. The research development occurred through the use of literature techniques and empirical study on the analysis of information architecture based on organization, navigation, recommended labeling and search for Rosenfeld and Morville (2006) for the sake of usability base PubMed. For better understanding and description of these principles, we used the technique of content analysis, Results. The results showed that the database interface meets the criteria established by the elements of Information Architecture, such as organization based on hypertext structure, horizontal menu and local content divided into categories, identifying active links, global navigation, breadcrumb, textual labeling and iconographic and highlight the search engine. Conclusions. This research showed that the PubMed database interface is well structured, friendly and objective, with numerous possibilities of search and information retrieval. However, there is a need to adopt accessibility standards on this website, so that it reaches more efficiently its purpose of facilitating access to information organized and stored in the PubMed database.","Authors":"Sales, OMM (Sales, Odete Mayra Mesquita) ; Pinto, VB (Pinto, Virginia Bentes) ; de Sousa, MRF (Ferreira de Sousa, Marckson Roberto)","Title":"Information architecture: study and analysis of data Public Medical PDF (Portugues base (PubMed)"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000379296100039 ISBN:978-3-319-39937-9; 978-3-319-39936-2 ISSN: 0302-9743","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods","Journal Information":"WEB-AGE INFORMATION MANAGEMENT, PT I Book Series: Lecture Notes in Computer Science Volume: 9658 Pages: 504-516 DOI: 10.1007/978-3-319-39937-9_39 Published: 2016","Abstract":"The most popular open source projects for text searching have been designed to support many features. These projects are well-written in Java for cross-platform using. But when conducting research, the execution efficiency of program should be more essential, which is a problem for applications written in Java. It is also difficult for Java to use parallel mechanisms in the modern computer system like SIMD and GPUs. To this end, we expand an open source text searching project written in C++ for research purpose. Our approach is to define a flexible and efficient search engine architecture which consists of extensible application programming interfaces. We aim to provide a flexible architecture to enable researchers to readily implement and modify search engine algorithms and strategies. Moreover, we integrate one generic mathematical encoding library which can be used to compress inverted index. We also implement an integral framework for result summarization, including snippet generation and cache strategies. Experiment results show that the new architecture makes a significant improvement versus original work.","Authors":"Zhang, ZH (Zhang, Zhaohua) ; Ye, BJ (Ye, Benjun) ; Huang, JY (Huang, Jiayi) ; Stones, R (Stones, Rebecca) ; Wang, G (Wang, Gang) ; Liu, XG (Liu, Xiaoguang) Edited by:Cui, B; Zhang, N; Xu, J; Lian, X; Liu, D","Title":"NBLucene: Flexible and Efficient Open Source Search Engine"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000375769000043 ISBN:978-3-319-34099-9 ISSN: 1865-0929","Keywords":"Knowledge base; Expert system shell; Web application","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Interdisciplinary Applications; Computer Science, Theory & Methods","Journal Information":"BEYOND DATABASES, ARCHITECTURES AND STRUCTURES, BDAS 2016 Book Series: Communications in Computer and Information Science Volume: 613 Pages: 558-570 DOI: 10.1007/978-3-319-34099-9_43 Published: 2016","Abstract":"Web applications have developed rapidly and have had a significant impact on the application of systems in many domains. The migration of information systems from classic desktop software to web applications can be seen as a permanent trend. This trend also applies to the knowledge based systems. This work is a part of the KBExplorator project -the main goal of this project is to provide a complete and easy to use web-based tool for the development of expert systems. The evaluation of the rules searching effectiveness in the proposed physical rule base model is the first experimental aim of this work. Experiments will be conducted to determine the duration of retrieving a single rule or group of rules in large rules sets. Decomposition of the rule knowledge base into the relational database is also a crucial issue of this work and therefore the presentation of the data model is the second goal of this work. The usage of a relational database in the web-based application is obvious, but its usage as the physical storage for the rule base is described in relatively small number of publications. Proposed decomposition conception and the model presented in this work has not been previously described. The positive results of experiments presented in this work allow us to continue the development of the system - in the next revision, the database interface layer will be implemented with the usage of a specialized API. This proposed software architecture allow us to transparently change the database engine as well as the programming language currently used in the application layer of the system.","Authors":"Siminski, R (Siminski, Roman) ; Xieski, T (Xieski, Tomasz) Edited by:Kozielski, S; Mrozek, D; Kasprowski, P; Malysiak-Mrozek, B; Kostrzewa, D","Title":"Physical Knowledge Base Representation for Web Expert System Shell"}, {"Document Information":"Document Type:Proceedings Paper Language:English Accession Number: WOS:000373970600060 ISBN:978-0-7918-5660-4","Categories":"Energy & Fuels; Engineering Web of Science Categories:Energy & Fuels; Engineering, Mechanical","Journal Information":"PROCEEDINGS OF THE ASME POWER CONFERENCE, 2015 Article Number: V001T07A003 Published: 2016","Abstract":"Using software automation technology can significantly improve the quality and productivity of software development. Based on the tree data structure, this paper proposed the Breadth First Search(BFS) based software framework automatic generation algorithm called CFAA (Code Framework Automation Algorithm). The algorithm uses tree data structure to represent scientific computing software architecture, then uses the BFS to traverse all nodes of the tree to generate code framework of software. CFAA enable programmers to focus more on computing software architecture design and optimization, and then automatically generate the skeleton code. CFAA has been applied to COSINE (Core and System Integrated Engine for design and analysis) software development process. Practice proved that CFAA can improve the efficiency of building software framework, while reducing the defect rate of software development.","Authors":"Ren, H (Ren, Hao) ; Mo, WT (Mo, Wentao) ; Zhao, G (Zhao, Guang) ; Ren, DP (Ren, Dangpei) ; Liu, S (Liu, Shuo) Book Group Author(s):ASME","Title":"BREADTH FIRST SEARCH BASED COSINE SOFTWARE CODE FRAMEWORK AUTOMATION ALGORITHM"}, {"Document Information":"Document Type:Article; Book Chapter Language:English Accession Number: WOS:000371923600015 Book DOI: 10.1007/978-3-319-14194-7 ISBN:978-3-319-14194-7; 978-3-319-14193-0 ISSN: 1860-949X","Categories":"Computer Science; Robotics Web of Science Categories:Computer Science, Artificial Intelligence; Robotics","Journal Information":"NOVEL APPLICATIONS OF INTELLIGENT SYSTEMS Book Series: Studies in Computational Intelligence Volume: 586 Pages: 269-285 DOI: 10.1007/978-3-319-14194-7_14 Published: 2016","Abstract":"The paper presents the system-\"Exactus Expert\"-search and analytical engine. The system aims to provide comprehensive tools for analysis of large-scale collections of scientific documents for experts and researchers. The system challenges many tasks, among them full-text search, search for similar documents, automatic quality assessment, term and definition extraction, results extraction and comparison, detection of scientific directions and analysis of references. These features help to aggregate information about different sides of scientific activity and can be useful for evaluation of research projects and groups. The paper discusses general architecture of the system, implemented methods of scientific publication analysis and some experimental results.","Authors":"Osipov, G (Osipov, Gennady) ; Smirnov, I (Smirnov, Ivan) ; Tikhomirov, I (Tikhomirov, Ilya) ; Sochenkov, I (Sochenkov, Ilya) ; Shelmanov, A (Shelmanov, Artem) Edited by:Hadjiski, M; Kasabov, N; Filev, D; Jotsov, V Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Tikhomirov, Ilya  I-3771-2016 http://orcid.org/0000-0003-0698-7689 Sochenkov, Ilya  D-4770-2014   Smirnov, Ivan  G-9456-2014 http://orcid.org/0000-0003-4490-2017 Osipov, Gennady  http://orcid.org/0000-0002-5329-6234","Title":"Exactus Expert-Search and Analytical Engine for Research and Development Support"}, {"Document Information":"Document Type:Review Language:English Accession Number: WOS:000371098400012 ISSN: 1084-8045","Keywords":"Grid-based; Large scale dataset; Information retrieval; Data searching; Digital libraries; Multimedia retrieval; Geospatial search KeyWords Plus:KNOWLEDGE MANAGEMENT; DIGITAL LIBRARIES; ARCHITECTURE; SYSTEM; INFRASTRUCTURE; RETRIEVAL; SERVICES; ANATOMY; ENGINE; LAYER","Categories":"Computer Science Web of Science Categories:Computer Science, Hardware & Architecture; Computer Science, Interdisciplinary Applications; Computer Science, Software Engineering","Journal Information":"JOURNAL OF NETWORK AND COMPUTER APPLICATIONS Volume: 60 Pages: 170-179 DOI: 10.1016/j.jnca.2015.10.010 Published: JAN 2016","Abstract":"The large-scale distributed dataset searching faced dynamicity, heterogeneity, and latency issues that emphasize the importance of approach to orchestrate the search operations. The growth of grid computing gives a reliable environment for the effective usage of this distributed huge dataset. In order to build effective and efficient searching technique, varieties of researches were developed based on grid technology. The goal of such researches is to solve interoperability and heterogeneous of the data sets, and increase the efficiency of search techniques by harnessing the grid computing capabilities. This paper reviews and analyzes the search techniques carried out in the domain of large-scale dataset search based on a grid. Several searching approaches are studied to identify which attributes are ignored and which are considered. The techniques are evaluated depending on the aforementioned attributes and are presented in a table. The contribution of this study is to illustrate the characteristics, capabilities and limitations of existing solutions that can guide the researchers in this evolving area. (C) 2015 Elsevier Ltd. All rights reserved.","Authors":"Bashir, MB (Bashir, Mohammed Bakri) ; Bin Abd Latiff, MS (Bin Abd Latiff, Muhammad Shafie) ; Coulibaly, Y (Coulibaly, Yahaya) ; Yousif, A (Yousif, Adil) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Abd Latiff, Muhammad Shafie  P-1648-2015 http://orcid.org/0000-0002-0741-2257","Title":"A survey of grid-based searching techniques for large scale distributed data"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000368208100011 ISSN: 0045-7906 eISSN: 1879-0755","Keywords":"String matching; Intrusion detection; FPGA; Binary tree; NIDS","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Hardware & Architecture; Computer Science, Interdisciplinary Applications; Engineering, Electrical & Electronic","Journal Information":"COMPUTERS & ELECTRICAL ENGINEERING Volume: 49 Pages: 117-133 DOI: 10.1016/j.compeleceng.2015.11.025 Published: JAN 2016","Abstract":"Network intrusion detection systems (NIDSs) monitor Internet Protocol (IP) traffic to detect anomalous and malicious activities on a network. Despite the plethora of studies in this field, hardware-based string matching engines that can accommodate the advancements in optical networking technology are still in high demand. Furthermore, memory efficient data structures to store intrusion patterns have recently received a great deal of research attention. In this paper, we introduce a tree-based pattern matching (TPM) scheme that comprises a forest of Binary Search Tree (BST) data structures and an accommodating high-throughput multi-pipelined architecture for scalable string matching on hardware. To improve the resource efficiency in hardware implementations, we enhanced TPM scheme (extended-TPM) with two novel tree structures, namely BST-epsilon (BST epsilon) and hierarchical BST (H-BST). Our entire design accomplishes a memory efficiency of 1.07 bytes/char for the latest Snort dictionary. Utilizing a state-of-the-art Field Programmable Gate Arrays (FPGAs), TPM architecture can sustain a throughput of 2.7 Gbps. (C) 2015 Elsevier Ltd. All rights reserved.","Authors":"Erdem, O (Erdem, Oguzhan)","Title":"Tree-based string pattern matching on FPGAs"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000367696200014 ISSN: 1936-6442 eISSN: 1936-6450","Keywords":"Peer-to-peer; Cloud computing; Web search; Query spikes","Categories":"Computer Science; Telecommunications Web of Science Categories:Computer Science, Information Systems; Telecommunications","Journal Information":"PEER-TO-PEER NETWORKING AND APPLICATIONS Volume: 9 Issue: 1 Pages: 193-208 DOI: 10.1007/s12083-014-0322-y Published: JAN 2016","Abstract":"In search engines, popular news events cause huge spikes in the queries related to the events. In this work, we consider the stability issues caused by these spikes in a peer-to-peer web search engine formed using voluntary resources shared by peers. The requirement of providing top-ranking results from a dynamic index distinguishes web search from other classes of peer-to-peer search like object lookup and file search. This makes the traditional methods of load reduction based on caching and replication, proposed for peer-to-peer object lookup and file search, insufficient for containing interest spikes in peer-to-peer web search. We propose transient use of public cloud to maintain the stability of peer-to-peer web search engines during interest spikes. To the best of our knowledge, this is the first work proposing transient use of public cloud to handle spikes in peer-to-peer search. In the proposed architecture, CAPS, the responsibility of handling spiking queries is dynamically offloaded to public clouds during the spike period. The peer bandwidth to be used for transfer of relevant index from peers to cloud is decided considering the impact on other applications in the peer as well as the requirements of the search application. We show that transient use of public cloud can be performed without major adverse impact on the desirable properties of peer-to-peer search like privacy and decentralized control. Experimental evaluation under realistic settings show that cloud-assistance can be used effectively to handle spikes.","Authors":"Haridas, H (Haridas, Harisankar) ; Dharanipragada, J (Dharanipragada, Janakiram)","Title":"CAPS: A cloud-assisted approach to handle spikes in peer-to-peer web search"}, {"Document Information":"Document Type:Review Language:English Accession Number: WOS:000367250300010 ISSN: 0018-9219 eISSN: 1558-2256","Keywords":"Decision making; design of experiments; optimization; response surface methodology; statistical learning KeyWords Plus:EFFICIENT GLOBAL OPTIMIZATION; SPATIAL COVARIANCE STRUCTURE; GAUSSIAN PROCESS REGRESSION; PARAMETER OPTIMIZATION; EXPERIMENTAL-DESIGN; MULTIARMED BANDIT; DEFORMATIONS; ALGORITHMS; SEARCH; MODELS","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"PROCEEDINGS OF THE IEEE Volume: 104 Issue: 1 Pages: 148-175 Special Issue: SI DOI: 10.1109/JPROC.2015.2494218 Published: JAN 2016","Abstract":"Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.","Authors":"Shahriari, B (Shahriari, Bobak) ; Swersky, K (Swersky, Kevin) ; Wang, ZY (Wang, Ziyu) ; Adams, RP (Adams, Ryan P.) ; de Freitas, N (de Freitas, Nando)","Title":"Taking the Human Out of the Loop: A Review of Bayesian Optimization"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000363348100001 ISSN: 0920-5489 eISSN: 1872-7018","Keywords":"Focused Web crawler; T-Graph; HTML data; Information retrieval; Search engine KeyWords Plus:WEB","Categories":"Computer Science Web of Science Categories:Computer Science, Hardware & Architecture; Computer Science, Software Engineering","Journal Information":"COMPUTER STANDARDS & INTERFACES Volume: 43 Pages: 1-11 DOI: 10.1016/j.csi.2015.07.001 Published: JAN 2016","Abstract":"The two significant tasks of a focused Web crawler are finding relevant documents and prioritizing them for effective download. For the first task, we propose an algorithm to fetch and analyze the most effective HTML elements of the page to predict and elicit the topical focus of each unvisited page with high accuracy. For the second task, we propose a scoring function of the relevant URLs through the use of T-Graph to prioritize each unvisited link. Thus, our novel method uniquely combines these approaches, giving precision and recall values close to 50%, which indicate the significance of the proposed architecture. (C) 2015 Elsevier B.V. All rights reserved.","Authors":"Seyfi, A (Seyfi, Ali) ; Patel, A (Patel, Ahmed)","Title":"A focused crawler combinatory link and content model based on T-Graph principles"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000367463200003 ISSN: 1392-8716","Keywords":"mistuning blade arrangement; nonlinear friction; annealing evolution algorithm; tabu list; GPU; CUDA KeyWords Plus:VIBRATION; DISKS","Categories":"Engineering Web of Science Categories:Engineering, Biomedical; Engineering, Mechanical","Journal Information":"JOURNAL OF VIBROENGINEERING Volume: 17 Issue: 8 Pages: 4078-4095 Published: DEC 2015","Abstract":"This paper sets up a lumped parameter model of engine bladed disk system when considering the nonlinear friction damping based on mistuned parameters which is obtained from the blade modal experiment. A bladed arrangement optimization method, namely annealing evolutionary algorithm with tabu list is presented which combines the local search ability of SA (simulated annealing) and the global searching ability of GA (genetic algorithm) introducing tabu list as the search memory list. Parallel TAEA (tabu annealing evolutionary algorithm) is presented based on CUDA (Compute Unified Device Architecture) combining GPU (Graphics Processing Unit) and its performance is analyzed. The results show that optimization based on CUDA framework can improve computing speed. At the same time using optimization results can reduce the amplitude of forced vibration response of bladed disk system and make it in the range of allowable engineering.","Authors":"Yuan, HQ (Yuan, Huiqun) ; Zhao, TY (Zhao, Tianyu) ; Yang, WJ (Yang, Wenjun) ; Pan, HG (Pan, Honggang)","Title":"Annealing evolutionary parallel algorithm analysis of optimization arrangement on mistuned blades with non-linear friction"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000365890700027 PubMed ID: 26414014 ISSN: 1548-7091 eISSN: 1548-7105","Categories":"Biochemistry & Molecular Biology Web of Science Categories:Biochemical Research Methods","Journal Information":"NATURE METHODS Volume: 12 Issue: 12 Pages: 1179-+ DOI: 10.1038/nmeth.3603 Published: DEC 2015","Abstract":"We describe an integrated workflow that robustly identifies cross-links from endogenous protein complexes in human cellular lysates. Our approach is based on the application of mass spectrometry (MS)-cleavable cross-linkers, sequential collision-induced dissociation (CID)-tandem MS (MS/MS) and electron-transfer dissociation (ETD)-MS/MS acquisitions, and a dedicated search engine, XlinkX, which allows rapid cross-link identification against a complete human proteome database. This approach allowed us to detect 2,179 unique cross-links (1,665 intraprotein cross-links at a 5% false discovery rate (FDR) and 514 interprotein cross-links at 1% FDR) in HeLa cell lysates. We validated the confidence of our cross-linking results by using a target-decoy strategy and mapping the observed cross-link distances onto existing high-resolution structures. Our data provided new structural information about many protein assemblies and captured dynamic interactions of the ribosome in contact with different elongation factors.","Authors":"Liu, F (Liu, Fan) ; Rijkers, DTS (Rijkers, Dirk T. S.) ; Post, H (Post, Harm) ; Heck, AJR (Heck, Albert J. R.) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Liu, Fan  http://orcid.org/0000-0002-2358-549X","Title":"Proteome-wide profiling of protein assemblies by cross-linking mass spectrometry"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000389578600012 ISSN: 1738-7906","Keywords":"Hadoop; Big Data; Map Reduce; Facebook; HDFS","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"INTERNATIONAL JOURNAL OF COMPUTER SCIENCE AND NETWORK SECURITY Volume: 15 Issue: 11 Pages: 58-62 Published: NOV 30 2015","Abstract":"Big Data is a collection of data that is large or complex to process using on-hand database management tools or data processing applications. Big Data has recently become one of the issues important in the networking world. Hadoop is a distributed paradigm used to manipulate the large amount of data. This manipulation contains not only storage as well as processing on the data. Hadoop is normally used for data intensive applications. It actually holds the huge amount of data and upon requirement perform the operations like data analysis, result analysis, data analytics etc. Now a day's almost every online users search for products, services, topics of interest etc. not only in Google and other search engines, but also more importantly on site itself (For example, in ecommerce site Amazon. com, search is the top product finding method used by site visitors).","Authors":"Lakavath, S (Lakavath, Suresh) ; Naik, LR (Naik, Ramlal L.)","Title":"A Big Data Hadoop Architecture for Online Analysis."}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000364026600001 ISSN: 0034-6861 eISSN: 1539-0756","Categories":"Physics Web of Science Categories:Physics, Multidisciplinary","Journal Information":"REVIEWS OF MODERN PHYSICS Volume: 87 Issue: 4 Pages: 1261-1310 DOI: 10.1103/RevModPhys.87.1261 Published: NOV 2 2015","Abstract":"In the past decade modern societies have developed enormous communication and social networks. Their classification and information retrieval processing has become a formidable task for the society. Because of the rapid growth of the World Wide Web, and social and communication networks, new mathematical methods have been invented to characterize the properties of these networks in a more detailed and precise way. Various search engines extensively use such methods. It is highly important to develop new tools to classify and rank a massive amount of network information in a way that is adapted to internal network structures and characteristics. This review describes the Google matrix analysis of directed complex networks demonstrating its efficiency using various examples including the World Wide Web, Wikipedia, software architectures, world trade, social and citation networks, brain neural networks, DNA sequences, and Ulam networks. The analytical and numerical matrix methods used in this analysis originate from the fields of Markov chains, quantum chaos, and random matrix theory.","Authors":"Ermann, L (Ermann, Leonardo) ; Frahm, KM (Frahm, Klaus M.) ; Shepelyansky, DL (Shepelyansky, Dima L.) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Shepelyansky, Dima  G-6050-2016 http://orcid.org/0000-0002-2752-0765 Frahm, Klaus  O-8555-2016 http://orcid.org/0000-0002-4077-3337","Title":"Google matrix analysis of directed networks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000369384200012 ISSN: 1738-9976","Keywords":"Web Crawler; Parallel; Search Engine","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"INTERNATIONAL JOURNAL OF SECURITY AND ITS APPLICATIONS Volume: 9 Issue: 10 Pages: 137-146 Published: OCT 2015","Abstract":"In the research of Web crawler, the most important things are structure design and solution of the key technologies. Based on the work of other people, we described the structure design of a distribute Web crawler, which including the organization of hardware and module partition of software. In this paper, one PC is utilized as the main node, and other PCs as the common nodes which are connected in LAN. The software architecture included main node design and common node design. Then, we analyzed solutions of the major techniques of the distributed Web crawler, such as how the nodes of the crawler cooperate with each other, how the task is distributed, how to keep the important Web fresh. We have proposed some practicable arithmetic to solve the problems mentioned above. Besides, we implemented a robust, distensible, customized, distributed Web crawler, and anatomized it. At last, we gave the results of two experiments, including common test and a site download test.","Authors":"Zhao, HY (Zhao, Hongyan)","Title":"Research on Detection Algorithm of WEB Crawler"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000361992800003 ISSN: 0138-9130 eISSN: 1588-2861","Keywords":"Bibliometric analysis; Software engineering; Turkey; Researchers; Scholars; Research community; Turkish universities and institutions KeyWords Plus:CITED ARTICLES; INSTITUTIONS; SCHOLARS; SYSTEMS; SCIENCE; TURKEY; WEB; JOURNALS; SCOPUS","Categories":"Computer Science; Information Science & Library Science Web of Science Categories:Computer Science, Interdisciplinary Applications; Information Science & Library Science","Journal Information":"SCIENTOMETRICS Volume: 105 Issue: 1 Pages: 23-49 DOI: 10.1007/s11192-015-1663-x Published: OCT 2015","Abstract":"This paper presents a bibliometric analysis of the Turkish software engineering (SE) community (researchers and institutions). The bibliometric analysis has been conducted based on the number of papers published in the software-engineering-related venues and indexed in the Scopus academic search engine until year 2014. According to the bibliometric analysis, the top-ranked institution is Middle East Technical University, and the top-ranked scholar is Ayse Basar Bener (formerly with Bogazici University and now with Ryerson University in Canada). The analysis reveals other important findings and presents a set of implications for the Turkish SE community and stakeholders (e.g., funding agencies and decision makers) such as the followings: (1) Turkey produces only about 0.49 % of the world-wide SE knowledge, as measured by the number of papers in Scopus, which is very negligible unfortunately. To take a more active role in the global SE community, the Turkish SE community has to increase their outputs. (2) We notice a lack of diversity in the general SE spectrum, e.g., we noticed very little focus on requirements engineering, software maintenance and evolution, and architecture. This denotes the need to further diversification in SE research topics in Turkey. (3) In total, 89 papers in the pool (30.8 % of the total) are internationally-authored SE papers. Having a good level of international collaborations is a good sign for the Turkish SE community. The highest international collaborations have been with researchers from United States, Canada and Netherlands. (4) In general, the involvement of industry in SE search is low. All stakeholders (e.g., government, industry and academia) should aim at increasing the level of industry-academia collaboration in the Turkish SE community, (5) Citation to Turkish SE papers, in general, are significantly lower than a set of three representative pools of SE papers. This is an area of concern which needs further investigation, and (6) In general, there is a need to increase both the quantity and quality of the Turkish SE papers, in the global stage. The approach we use in this study could be replicated in other countries to provide insights and trends about the SE research performance in other countries.","Authors":"Garousi, V (Garousi, Vahid) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Garousi, Vahid  http://orcid.org/0000-0001-6590-7576","Title":"A bibliometric analysis of the Turkish software engineering research community"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000365412900038 ISSN: 1546-1955 eISSN: 1546-1963","Keywords":"Cloud Resource; Resource Management; Semantic Search Engine (SSE); Multi-Agent System (MAS) KeyWords Plus:ALGORITHM; OPTIMIZATION; ARCHITECTURE; NETWORK; MODEL","Categories":"Chemistry; Science & Technology - Other Topics; Materials Science; Physics Web of Science Categories:Chemistry, Multidisciplinary; Nanoscience & Nanotechnology; Materials Science, Multidisciplinary; Physics, Applied; Physics, Condensed Matter","Journal Information":"JOURNAL OF COMPUTATIONAL AND THEORETICAL NANOSCIENCE Volume: 12 Issue: 9 Pages: 2268-2273 DOI: 10.1166/jctn.2015.4018 Published: SEP 2015","Abstract":"To address the problem of effective Cloud resource management, we propose a multi-agent system (MAS)-based semantic search engine (SSE). SSE is a new type of service search engine developed by semantic computing laboratory in University of California, Irvine. It provides Cloud users with a friendly problem-driven interface to automatically manage resource that would be used to build a solution according to users' requirements. Further we adopt MAS in SSE to speed the match process between the users' requirements and Cloud resources so as to enhance the Cloud resource management efficiency. The multi-agent system dispatches a set of individual agents (IA) to coordinate a user task over Cloud in a decentralized manner. The architecture of Cloud resource management based on SSE is presented, followed by the multi-agent system work process and IA implementation. The experiments show our proposed multi-agent system-based semantic search engine for Cloud resource management (SSECRM) can reduce about twenty percent average tasks execution time and obtain about seventeen percent higher communication performance in the point-to-point communication comparing to the traditional Cloud resource management (TCRM). Therefore our approach is feasible for Cloud resource management.","Authors":"Li, M (Li, Ming) ; Wu, Y (Wu, Yue) ; Chen, J (Chen, Jia)","Title":"Multi-Agent System-Based Semantic Search Engine for Cloud Resource Management"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000361162500008 ISSN: 0140-3664 eISSN: 1873-703X","Keywords":"Packet classification; FPGA; Pipeline; Trie KeyWords Plus:ARCHITECTURE","Categories":"Computer Science; Engineering; Telecommunications Web of Science Categories:Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications","Journal Information":"COMPUTER COMMUNICATIONS Volume: 67 Pages: 75-91 DOI: 10.1016/j.comcom.2015.05.017 Published: AUG 1 2015","Abstract":"A packet classification task incorporated in network firewalls to recognize and sift threats or unauthorized network accesses is accomplished by checking incoming packet headers against a pre-defined filter set. Plenty of solutions to reduce the memory requirement of filter set storage and accommodate packet classification to line rates have been proposed over the past decade. Among all the existing approaches, hierarchical data structures maintain great memory performance however their hardware realization suffers from two issues: (i) backtracking and (ii) memory inefficiency. In this paper, we propose two data structures denoted range tree-linked list hierarchical search structure (RLHS) and value-coded trie structure with epsilon-branch property (VC epsilon) for packet classification. RLHS resolves backtracking by exploiting range tree in Stage 1 and linked list data structure in Stage 2 overcomes the memory inefficiency. VC epsilon trie that naturally does not involve backtracking problem, solves memory inefficiency issue by comprising a fixed size bin at each node. Apart from conventional binary trie, a new rule is inserted into the first available bin on the path of this rule in a VC epsilon trie, and epsilon-branch property is utilized in case all the bins are full. We also propose a rule categorization algorithm that partitions an input ruleset by considering the field features of rules to minimize the memory requirement. To support the proposed data structures, we designed high throughput SRAM-based parallel and pipelined architectures on Field Programmable Gate Arrays (FPGAs). (C) 2015 Elsevier B.V. All rights reserved.","Authors":"Erdem, O (Erdem, Oguzhan) ; Carus, A (Carus, Aydin)","Title":"Multi-pipelined and memory-efficient packet classification engines on FPGAs"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000360007500005 ISSN: 2157-6904 eISSN: 2157-6912","Keywords":"Algorithms; Experimentation; Performance; Latent Dirichlet allocation; big topic models; big data; long-tail topic features; search engine; online advertising systems KeyWords Plus:LATENT DIRICHLET ALLOCATION","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems","Journal Information":"ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY Volume: 6 Issue: 4 Article Number: 47 DOI: 10.1145/2700497 Published: AUG 2015","Abstract":"Latent Dirichlet allocation (LDA) is a popular topic modeling technique in academia but less so in industry, especially in large-scale applications involving search engine and online advertising systems. A main underlying reason is that the topic models used have been too small in scale to be useful; for example, some of the largest LDA models reported in literature have up to 10(3) topics, which difficultly cover the long-tail semantic word sets. In this article, we show that the number of topics is a key factor that can significantly boost the utility of topic-modeling systems. In particular, we show that a \"big\" LDA model with at least 10(5) topics inferred from 10(9) search queries can achieve a significant improvement on industrial search engine and online advertising systems, both of which serve hundreds of millions of users. We develop a novel distributed system called Peacock to learn big LDA models from big data. The main features of Peacock include hierarchical distributed architecture, real-time prediction, and topic de-duplication. We empirically demonstrate that the Peacock system is capable of providing significant benefits via highly scalable LDA topic models for several industrial applications.","Authors":"Wang, Y (Wang, Yi) ; Zhao, XM (Zhao, Xuemin) ; Sun, ZL (Sun, Zhenlong) ; Yan, H (Yan, Hao) ; Wang, LF (Wang, Lifeng) ; Jin, ZH (Jin, Zhihui) ; Wang, LB (Wang, Liubin) ; Gao, Y (Gao, Yang) ; Law, C (Law, Ching) ; Zeng, J (Zeng, Jia)","Title":"Peacock: Learning Long-Tail Topic Features for Industrial Applications"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000358270200030 ISSN: 1546-1955 eISSN: 1546-1963","Keywords":"Cloud Computing; Resource Scheduling; Semantic Search Engine (SSE); Improved Parallel Genetic Algorithm (IPGA) KeyWords Plus:OPTIMIZATION","Categories":"Chemistry; Science & Technology - Other Topics; Materials Science; Physics Web of Science Categories:Chemistry, Multidisciplinary; Nanoscience & Nanotechnology; Materials Science, Multidisciplinary; Physics, Applied; Physics, Condensed Matter","Journal Information":"JOURNAL OF COMPUTATIONAL AND THEORETICAL NANOSCIENCE Volume: 12 Issue: 8 Pages: 1669-1676 DOI: 10.1166/jctn.2015.3945 Published: AUG 2015","Abstract":"Cloud resource has the features of dynamic, heterogeneous, distributed and complexity etc. Hence how to effectively schedule Cloud resources becomes a research focus in Cloud. Meanwhile the numbers of resources and tasks to be scheduled in Cloud are usually variable. This makes the Cloud resource scheduling a complex optimization problem. None of existing Cloud systems is both being an automated scheduling and considering the optimal usage of resources. To address these problems, we propose a Cloud resource scheduling strategy using improved parallel genetic algorithm (IPGA)-based Semantic Search Engine (SSE). SSE is a new type of service search engine developed by Semantic Computing laboratory in University of California, Irvine. It provides Cloud users with a friendly problem-driven interface to automatically schedule resources that would be used to build a solution according to users' requirements. Further we adopt IPGA in SSE to optimize the scheduling so as to obtain the optimal usage of resources. In our proposed IPGA there should be a code distant between the selected parents to retain the population diversity. Moreover, individuals can periodically migrate from one subpopulation to another. The architecture of our proposed scheduling strategy is presented as well as the process and implementation of IPGA. The experiment results show our proposed approach can reduce about twenty-one percent average tasks execution time due to the usage of SSE and IPGA can reduce about twenty-six percent average algorithm execution time and obtain better fitness with the generation increase comparing to the traditional genetic algorithm (TGA).","Authors":"Li, M (Li, Ming) ; Wu, Y (Wu, Yue) ; Chen, J (Chen, Jia)","Title":"Cloud Resource Scheduling Using Semantic Search Engine Based on Improved Parallel Genetic Algorithm"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000358176100007 PubMed ID: 25753033 ISSN: 0961-8368 eISSN: 1469-896X","Keywords":"cross-linking mass spectrometry; SAGA HAT subcomplex; proteomics; protein-; protein interaction KeyWords Plus:PROTEOME-SCALE MAP; SACCHAROMYCES-CEREVISIAE; LINKED PEPTIDES; TRANSCRIPTION FACTOR; STRUCTURAL BIOLOGY; COMPLEXES; PROTEINS; DYNAMICS; IDENTIFICATION; ARCHITECTURE","Categories":"Biochemistry & Molecular Biology Web of Science Categories:Biochemistry & Molecular Biology","Journal Information":"PROTEIN SCIENCE Volume: 24 Issue: 8 Pages: 1232-1246 Special Issue: SI DOI: 10.1002/pro.2676 Published: AUG 2015","Abstract":"Understanding the way how proteins interact with each other to form transient or stable protein complexes is a key aspect in structural biology. In this study, we combined chemical cross-linking with mass spectrometry to determine the binding stoichiometry and map the protein-protein interaction network of a human SAGA HAT subcomplex. MALDI-MS equipped with high mass detection was used to follow the cross-linking reaction using bis[sulfosuccinimidyl] suberate (BS3) and confirm the heterotetrameric stoichiometry of the specific stabilized subcomplex. Cross-linking with isotopically labeled BS3 d0-d4 followed by trypsin digestion allowed the identification of intra- and intercross-linked peptides using two dedicated search engines: pLink and xQuest. The identified interlinked peptides suggest a strong network of interaction between GCN5, ADA2B and ADA3 subunits; SGF29 is interacting with GCN5 and ADA3 but not with ADA2B. These restraint data were combined to molecular modeling and a low-resolution interacting model for the human SAGA HAT subcomplex could be proposed, illustrating the potential of an integrative strategy using cross-linking and mass spectrometry for addressing the structural architecture of multiprotein complexes.","Authors":"Nguyen-Huynh, NT (Nha-Thi Nguyen-Huynh) ; Sharov, G (Sharov, Grigory) ; Potel, C (Potel, Clement) ; Fichter, P (Fichter, Pelagie) ; Trowitzsch, S (Trowitzsch, Simon) ; Berger, I (Berger, Imre) ; Lamour, V (Lamour, Valerie) ; Schultz, P (Schultz, Patrick) ; Potier, N (Potier, Noelle) ; Leize-Wagner, E (Leize-Wagner, Emmanuelle) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Trowitzsch, Simon  O-4957-2016 http://orcid.org/0000-0001-9143-766X Nguyen-Huynh, Nha-Thi  http://orcid.org/0000-0002-0913-836X schultz, patrick  http://orcid.org/0000-0002-7310-6186","Title":"Chemical cross-linking and mass spectrometry to determine the subunit interaction network in a recombinant human SAGA HAT subcomplex"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000363895500002 ISSN: 2192-6611 eISSN: 2192-662X","Keywords":"Object category retrieval and recognition; Object instance retrieval; Face retrieval; On-the-fly; Convolutional neural networks","Categories":"Computer Science Web of Science Categories:Computer Science, Software Engineering","Journal Information":"INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL Volume: 4 Issue: 2 Pages: 75-93 Special Issue: SI DOI: 10.1007/s13735-015-0077-0 Published: JUN 2015","Abstract":"The objective of this work is to visually search large-scale video datasets for semantic entities specified by a text query. The paradigm we explore is constructing visual models for such semantic entities on-the-fly, i.e. at run time, by using an image search engine to source visual training data for the text query. The approach combines fast and accurate learning and retrieval, and enables videos to be returned within seconds of specifying a query. We describe three classes of queries, each with its associated visual search method: object instances (using a bag of visual words approach for matching); object categories (using a discriminative classifier for ranking key frames); and faces (using a discriminative classifier for ranking face tracks). We discuss the features suitable for each class of query, for example Fisher vectors or features derived from convolutional neural networks (CNNs), and how these choices impact on the trade-off between three important performance measures for a real-time system of this kind, namely: (1) accuracy, (2) memory footprint, and (3) speed. We also discuss and compare a number of important implementation issues, such as how to remove 'outliers' in the downloaded images efficiently, and how to best obtain a single descriptor for a face track. We also sketch the architecture of the real-time on-the-fly system. Quantitative results are given on a number of large-scale image and video benchmarks (e.g. TRECVID INS, MIRFLICKR-1M), and we further demonstrate the performance and real-world applicability of our methods over a dataset sourced from 10,000 h of unedited footage from BBC News, comprising 5M+ key frames.","Authors":"Chatfield, K (Chatfield, Ken) ; Arandjelovic, R (Arandjelovic, Relja) ; Parkhi, O (Parkhi, Omkar) ; Zisserman, A (Zisserman, Andrew)","Title":"On-the-fly learning for visual search of large-scale image and video datasets"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000359244200005 ISSN: 1083-3196 eISSN: 1743-288X","Keywords":"Erector spinae; Multifidus; Lumbar spine; Posture; Spinal loading; Biomechanics KeyWords Plus:LOW-BACK-PAIN; HEALTHY CONTROL SUBJECTS; CROSS-SECTIONAL AREA; PROLONGED BED REST; MUSCLE-FIBER SIZE; EXTENSOR MUSCLES; TRUNK MUSCLE; PARAVERTEBRAL MUSCLES; TORQUE PRODUCTION; SEX-DIFFERENCES","Categories":"Rehabilitation Web of Science Categories:Rehabilitation","Journal Information":"PHYSICAL THERAPY REVIEWS Volume: 20 Issue: 3 Pages: 178-186 DOI: 10.1179/1743288X15Y.0000000014 Published: JUN 2015","Abstract":"Background: Lumbar posture has a significant impact on the functional biomechanics of the erector spinae and multifidus muscles, which has implications for the loads placed on the tissues of the lumbar spine. Objectives: The objective of this review is to discuss the effects of lumbar posture on the functional biomechanics of the different divisions of the erector spinae and the multifidus muscle and its importance when developing clinical interventions. Methods: This review used the search engines PubMed, EBSCO, CINAHL and SCOPUS to identify studies investigating erector spinae and multifidus muscle architecture and the influence of lumbar posture on the biomechanical properties of these muscles and the resulting impact on spinal loading. Results: Changes in lumbar curvature alter muscle fascicle obliquity, lever arm distances, the length-tension relationships and muscle volume of the different divisions of erector spinae and multifidus, which impact on the spine's ability to resist moments and shear forces. Conclusion: Changes in lumbar posture influence the functional biomechanics of the different divisions of erector spinae and the multifidus muscles. Therapists should develop low back interventions that avoid end range of lumbar postures and optimise the functional biomechanics of the erector spinae and multifidus muscles and minimise loading on the lumbar spine.","Authors":"Mawston, GA (Mawston, Grant A.) ; Boocock, MG (Boocock, Mark G.)","Title":"Lumbar posture biomechanics and its influence on the functional anatomy of the erector spinae and multifidus"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000353463200001 ISSN: 1094-3420 eISSN: 1741-2846","Keywords":"Molecular docking; in silico virtual drug screening; many-core; GPU; OpenCL; performance portability KeyWords Plus:DOCKING; ACCELERATION; ALGORITHMS","Categories":"Computer Science Web of Science Categories:Computer Science, Hardware & Architecture; Computer Science, Interdisciplinary Applications; Computer Science, Theory & Methods","Journal Information":"INTERNATIONAL JOURNAL OF HIGH PERFORMANCE COMPUTING APPLICATIONS Volume: 29 Issue: 2 Pages: 119-134 DOI: 10.1177/1094342014528252 Published: SUM 2015","Abstract":"Drug screening is an important part of the drug development pipeline for the pharmaceutical industry. Traditional, lab-based methods are increasingly being augmented with computational methods, ranging from simple molecular similarity searches through more complex pharmacophore matching to more computationally intensive approaches, such as molecular docking. The latter simulates the binding of drug molecules to their targets, typically protein molecules. In this work, we describe BUDE, the Bristol University Docking Engine, which has been ported to the OpenCL industry standard parallel programming language in order to exploit the performance of modern many-core processors. Our highly optimized OpenCL implementation of BUDE sustains 1.43 TFLOP/s on a single Nvidia GTX 680 GPU, or 46% of peak performance. BUDE also exploits OpenCL to deliver effective performance portability across a broad spectrum of different computer architectures from different vendors, including GPUs from Nvidia and AMD, Intel's Xeon Phi and multi-core CPUs with SIMD instruction sets.","Authors":"McIntosh-Smith, S (McIntosh-Smith, Simon) ; Price, J (Price, James) ; Sessions, RB (Sessions, Richard B.) ; Ibarra, AA (Ibarra, Amaurys A.) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number McIntosh-Smith, Simon  http://orcid.org/0000-0002-5312-0378","Title":"High performance in silico virtual drug screening on many-core processors"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000355962400005 ISSN: 1016-2364","Keywords":"centralized parallel crawler; two-level URL duplication checking; coverage problem; high-speed parallel Web crawler; cost model","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"JOURNAL OF INFORMATION SCIENCE AND ENGINEERING Volume: 31 Issue: 3 Pages: 839-860 Published: MAY 2015","Abstract":"For efficient large-scale Web crawlers, URL duplication checking is an important technique since it is a significant bottleneck. In this paper, we propose a new URL duplication checking technique for a parallel Web crawler; we call it full-coverage two level URL duplication checking (full-coverage-2L-UDC). Full-coverage-2L-UDC provides efficient URL duplication checking while ensuring maximum coverage. First, we propose two-level URL duplication checking (2L-UDC). It provides efficiency in URL duplication checking by communicating at the Web site level rather than at the Web page level. Second, we present a solution for the so-called coverage problem, which is directly related to the recall of the search engine. It is the first solution for the coverage problem in the centralized parallel architecture. Third, we propose an architecture, FC2-LUDCbot, for a centralized parallel crawler using full-coverage-2L-UDC. We build a seven-agent FC2L-UDCbot for extensive experiments. We show that the crawling speed of FC2L-UDCbot is approximately proportional to the number of agents (Le., FC2L-UDCbot is faster than a single-machine crawler by 6.9 times). Full-coverage-2L-UDC allows FC2L-UDCbot to be scalable to the number of agents since it effectively deals with the overheads incurred in a parallel environment. Through an in-depth analysis, we construct a cost model for estimating the crawling speed of a scaled-up crawler. Using the model, we show that FC2L-UDCbot can crawl Google-scale Web pages within several days using dozens of agents.","Authors":"Younus, A (Younus, Arjumand) ; Whang, KY (Whang, Kyu-Young) ; Kwon, HY (Kwon, Hyuk-Yoon) ; Yeo, YM (Yeo, Yeon-Mi) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Whang, Kyu-Young  C-2009-2011  ","Title":"A Full-Coverage Two-Level URL Duplication Checking Method for a High-Speed Parallel Web Crawler"}]