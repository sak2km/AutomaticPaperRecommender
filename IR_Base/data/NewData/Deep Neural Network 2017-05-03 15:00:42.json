[{"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397379000003 ISSN: 0020-0255 eISSN: 1872-6291","Keywords":"Sound event recognition; Time-frequency image feature; Deep neural networks; Support vector machines KeyWords Plus:SPEECH RECOGNITION; AUDIO SURVEILLANCE; ALGORITHM; FEATURES; NOISE","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"INFORMATION SCIENCES Volume: 396 Pages: 24-32 DOI: 10.1016/j.ins.2017.02.013 Published: AUG 2017","Abstract":"Support vector machines (SVMs) have seen an increased usage in applications of acoustic event classification since its rise to popularity about two decades ago. However, in recent years, deep learning methods, such as deep neural networks (DNNs), have shown to outperform a number of classification methods in various pattern recognition problems. This work starts by comparing the classification performance of DNN5 against SVMs with a number of feature representations which fall into two categories: cepstral features and time-frequency image features. Unlike related work, the classification performance of the two classifiers is also compared with feature vector combination and the training and evaluation times of the classifiers and features are also compared. The performance is evaluated on an audio surveillance database containing 10 sound classes, each class having multiple subclasses, with the addition of noise at various signal-to-noise ratios (SNRs). The experimental results shows that DNNs have a better overall classification performance than SVMs with both individual and combined features and the classification accuracy with DNNs is particularly better at low SNRs. The evaluation time of the DNN classifier was also determined to be the fastest but with a slow training time. (C) 2017 Elsevier Inc. All rights reserved.","Authors":"Sharan, RV (Sharan, Roneel V.) ; Moir, TJ (Moir, Tom J.)","Title":"Robust acoustic event classification using deep neural networks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397687400004 ISSN: 0957-4174 eISSN: 1873-6793","Keywords":"Industrial application; Steel slab; Text localization; Deep learning; Deep convolutional neural network; Accumulated confidence","Categories":"Computer Science; Engineering; Operations Research & Management Science Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science","Journal Information":"EXPERT SYSTEMS WITH APPLICATIONS Volume: 77 Pages: 34-43 DOI: 10.1016/j.eswa.2017.01.026 Published: JUL 1 2017","Abstract":"This paper proposes a novel algorithm for localizing slab identification numbers (SINs) in factory scenes. Automatic identification of product information is important for the process management, and localization of SINs in complex scenes is a major challenge for the recognition. A previous rule-based localization algorithm for SINs requires lots of prior knowledge and heuristic tuning for parameters. In this paper, a deep convolutional neural network (DCNN) is employed to overcome these limitations, and accumulated confidence is proposed to utilize neighboring outputs of the DCNN in a scene. The localization error is remarkably reduced to 1.44% by the proposed algorithm compared to 4.59% in the previous work. The proposed data-driven method can be applied to construct other automatic identification systems with minimal manual handling. (C) 2017 Elsevier Ltd. All rights reserved.","Authors":"Lee, SJ (Lee, Sang Jun) ; Kim, SW (Kim, Sang Woo)","Title":"Localization of the slab information in factory.scenes using deep convolutional neural networks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394070100008 ISSN: 1566-2535 eISSN: 1872-6305","Keywords":"Deep leaming; Classification architecture for echo video images; Convolutional neural network; Echocardiography; KAZE; SIFT; SURF KeyWords Plus:KAZE FEATURES; RECOGNITION; REPRESENTATION; VIDEOS","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Theory & Methods","Journal Information":"INFORMATION FUSION Volume: 36 Pages: 103-113 DOI: 10.1016/j.inffus.2016.11.007 Published: JUL 2017","Abstract":"This study extends the state of the art of deep learning convolutional neural network (CNN) to the classification of video images of echocardiography, aiming at assisting clinicians in diagnosis of heart diseases. Specifically, the architecture of neural networks is established by embracing hand-crafted features within a data-driven learning framework, incorporating both spatial and temporal information sustained by the video images of the moving heart and giving rise to two strands of two-dimensional convolutional neural network (CNN). In particular, the acceleration measurement along the time direction at each point is calculated using dense optical flow technique to represent temporal motion information. Subsequently, the fusion of both networks is conducted via linear integrations of the vectors of class scores obtained from each of the two networks. As a result, this architecture maintains the best classification results for eight viewpoint categories of echo videos with 92.1% accuracy rate whereas 89.5% is achieved using only single spatial CNN network. When concerning only three primary locations, 98% of accuracy rate is realised. In addition, comparisons with a number of well-known hand-engineered approaches are also performed, including 2D KAZE, 2D KAZE with Optical Flow, 3D KAZA, Optical Flow, 2D SIFT and 3D SIFT, which delivers accuracy rate of 89.4%, 84.3%, 87.9%, 79.4%, 83.8% and 73.8% respectively. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Gao, XH (Gao, Xiaohong) ; Li, W (Li, Wei) ; Loomes, M (Loomes, Martin) ; Wang, LY (Wang, Lianyi)","Title":"A fused deep learning architecture for viewpoint classification of echocardiography"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394070100015 ISSN: 1566-2535 eISSN: 1872-6305","Keywords":"Image fusion; Multi-focus image fusion; Deep learning; Convolutional neural networks; Activity level measurement; Fusion rule KeyWords Plus:QUALITY ASSESSMENT; DECOMPOSITION; PERFORMANCE; TRANSFORM; INFORMATION; SIMILARITY; REPRESENTATION; ALGORITHM","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Theory & Methods","Journal Information":"INFORMATION FUSION Volume: 36 Pages: 191-207 DOI: 10.1016/j.inffus.2016.12.001 Published: JUL 2017","Abstract":"As is well known, activity level measurement and fusion rule are two crucial factors in image fusion. For most existing fusion methods, either in spatial domain or in a transform domain like wavelet, the activity level measurement is essentially implemented by designing local filters to extract high-frequency details, and the calculated clarity information of different source images are then compared using some elaborately designed rules to obtain a clarity/focus map. Consequently, the focus map contains the integrated clarity information, which is of great significance to various image fusion issues, such as multi-focus image fusion, multi-rhodal image fusion, etc. However, in order to achieve a satisfactory fusion performance, these two tasks are usually difficult to finish. In this study, we address this problem with a deep learning approach, aiming to learn a direct mapping between source images and focus map. To this end, a deep convolutional neural network (CNN) trained by high-quality image patches and their blurred versions is adopted to encode the mapping. The main novelty of this idea is that the activity level measurement and fusion rule can be jointly generated through learning a CNN model, which overcomes the difficulty faced by the existing fusion methods. Based on the above idea, a new multi-focus image fusion method is primarily proposed in this paper. Experimental results demonstrate that the proposed method can obtain state-of-the-art fusion performance in terms of both visual quality and objective assessment. The computational speed of the proposed method using parallel computing is fast enough for practical usage. The potential of the learned CNN model for some other-type image fusion issues is also briefly exhibited in the experiments. (C) 2016 Elsevier B.V.","Authors":"Liu, Y (Liu, Yu) ; Chen, X (Chen, Xun) ; Peng, H (Peng, Hu) ; Wang, ZF (Wang, Zengfu)","Title":"Multi-focus image fusion with a deep convolutional neural network"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000398620100013 PubMed ID: 28177526 ISSN: 0021-9967 eISSN: 1096-9861","Keywords":"approach behaviors; avoidance behaviors; cingulate area; motor cortex area 2; RRID:SCR_013672; RRID:SCR_013672; superior colliculus KeyWords Plus:CAT SUPERIOR COLLICULUS; BAT ROUSETTUS-AEGYPTIACUS; CINGULATE CORTEX; TECTORETICULOSPINAL PROJECTION; ELECTRICAL-STIMULATION; EFFERENT CONNECTIONS; VIRTUAL-NAVIGATION; OUTPUT NEURONS; VISUAL-CORTEX; RAT","Categories":"Neurosciences & Neurology; Zoology Web of Science Categories:Neurosciences; Zoology","Journal Information":"JOURNAL OF COMPARATIVE NEUROLOGY Volume: 525 Issue: 8 Pages: 1980-1999 DOI: 10.1002/cne.24186 Published: JUN 1 2017","Abstract":"The orchestration of orienting behaviors requires the interaction of many cortical and subcortical areas, for example the superior colliculus (SC), as well as prefrontal areas responsible for top-down control. Orienting involves different behaviors, such as approach and avoidance. In the rat, these behaviors are at least partially mapped onto different SC subdomains, the lateral (SCl) and medial (SCm), respectively. To delineate the circuitry involved in the two types of orienting behavior in mice, we injected retrograde tracer into the intermediate and deep layers of the SCm and SCl, and thereby determined the main input structures to these subdomains. Overall the SCm receives larger numbers of afferents compared to the SCl. The prefrontal cingulate area (Cg), visual, oculomotor, and auditory areas provide strong input to the SCm, while prefrontal motor area 2 (M2), and somatosensory areas provide strong input to the SCl. The prefrontal areas Cg and M2 in turn connect to different cortical and subcortical areas, as determined by anterograde tract tracing. Even though connectivity pattern often overlap, our labeling approaches identified segregated neural circuits involving SCm, Cg, secondary visual cortices, auditory areas, and the dysgranular retrospenial cortex likely to be involved in avoidance behaviors. Conversely, SCl, M2, somatosensory cortex, and the granular retrospenial cortex comprise a network likely involved in approach/appetitive behaviors.","Authors":"Savage, MA (Savage, Michael Anthony) ; McQuade, R (McQuade, Richard) ; Thiele, A (Thiele, Alexander)","Title":"Segregated fronto-cortical and midbrain connections in the mouse and their relation to approach and avoidance orienting behaviors"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000398419200056 ISSN: 0035-8711 eISSN: 1365-2966","Keywords":"gravitational lensing: weak; galaxies: distances and redshifts; cosmology:observations KeyWords Plus:ESTIMATING PHOTOMETRIC REDSHIFTS; ARTIFICIAL NEURAL-NETWORKS; BONN DEEP SURVEY; CROSS-CORRELATIONS; DATA REDUCTION; SURVEY VIPERS; DATA RELEASE; GALAXIES; DISTRIBUTIONS; TELESCOPE","Categories":"Astronomy & Astrophysics Web of Science Categories:Astronomy & Astrophysics","Journal Information":"MONTHLY NOTICES OF THE ROYAL ASTRONOMICAL SOCIETY Volume: 468 Issue: 1 Pages: 769-782 DOI: 10.1093/mnras/stx471 Published: JUN 2017","Abstract":"To measure the mass of foreground objects with weak gravitational lensing, one needs to estimate the redshift distribution of lensed background sources. This is commonly done in an empirical fashion, i. e. with a reference sample of galaxies of known spectroscopic redshift, matched to the source population. In this work, we develop a simple decision tree framework that, under the ideal conditions of a large, purely magnitude-limited reference sample, allows an unbiased recovery of the source redshift probability density function p(z), as a function of magnitude and colour. We use this framework to quantify biases in empirically estimated p( z) caused by selection effects present in realistic reference and weak lensing source catalogues, namely (1) complex selection of reference objects by the targeting strategy and success rate of existing spectroscopic surveys and (2) selection of background sources by the success of object detection and shape measurement at low signal to noise. For intermediate-to-high redshift clusters, and for depths and filter combinations appropriate for ongoing lensing surveys, we find that (1) spectroscopic selection can cause biases above the 10 per cent level, which can be reduced to approximate to 5 per cent by optimal lensing weighting, while (2) selection effects in the shape catalogue bias mass estimates at or below the 2 per cent level. This illustrates the importance of completeness of the reference catalogues for empirical redshift estimation.","Authors":"Gruen, D (Gruen, D.) ; Brimioulle, F (Brimioulle, F.)","Title":"Selection biases in empirical p(z) methods for weak lensing"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397371800010 ISSN: 0031-3203 eISSN: 1873-5142","Keywords":"Facial age estimation; Deep learning; Feature learning; Biometrics KeyWords Plus:DIMENSIONALITY REDUCTION; REGRESSION; RECOGNITION; FRAMEWORK; PATTERNS; MANIFOLD; IMAGES; NETS","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"PATTERN RECOGNITION Volume: 66 Pages: 82-94 Special Issue: SI DOI: 10.1016/j.patcog.2016.10.026 Published: JUN 2017","Abstract":"In this paper, we propose a group-aware deep feature learning (GA-DFL) approach for facial age estimation. Unlike most existing methods which utilize hand-crafted descriptors for face representation, our GA-DFL method learns a discriminative feature descriptor per image directly from raw pixels for face representation under the deep convolutional neural networks framework. Motivated by the fact that age labels are chronologically correlated and the facial aging datasets are usually lack of labeled data for each person in a long range of ages, we split ordinal ages into a set of discrete groups and learn deep feature transformations across age groups to project each face pair into the new feature space, where the intra-group variances of positive face pairs from the training set are minimized and the inter-group variances of negative face pairs are maximized, simultaneously. Moreover, we employ an overlapped coupled learning method to exploit the smoothness for adjacent age groups. To further enhance the discriminative capacity of face representation, we design a multi-path CNN approach to integrate the complementary information from multi-scale perspectives. Experimental results show that our approach achieves very competitive performance compared with most state-of-the-arts on three public face aging datasets that were captured under both controlled and uncontrolled environments.","Authors":"Liu, H (Liu, Hao) ; Lu, JW (Lu, Jiwen) ; Feng, JJ (Feng, Jianjiang) ; Zhou, J (Zhou, Jie)","Title":"Group-aware deep feature learning for facial age estimation"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397371800011 ISSN: 0031-3203 eISSN: 1873-5142","Keywords":"Age estimation; Deep learning; Convolutional neural network KeyWords Plus:FACE IMAGES; DIMENSIONALITY; REGRESSION","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"PATTERN RECOGNITION Volume: 66 Pages: 95-105 Special Issue: SI DOI: 10.1016/j.patcog.2017.01.007 Published: JUN 2017","Abstract":"Age estimation from face images is an important yet difficult task in computer vision. Its main difficulty lies in how to design aging features that remain discriminative in spite of large facial appearance variations. Meanwhile, due to the difficulty of collecting and labeling datasets that contain sufficient samples for all possible ages, the age distributions of most benchmark datasets are often imbalanced, which makes this problem more challenge. In this work, we try to solve these difficulties by means of the mainstream deep learning techniques. Specifically, we use a convolutional neural network which can learn discriminative aging features from raw face images without any handcrafting. To combat the sample imbalance problem, we propose a novel cumulative hidden layer which is supervised by a point-wise cumulative signal. With this cumulative hidden layer, our model is learnt indirectly using faces with neighbouring ages and thus alleviate the sample imbalance problem. In order to learn more effective aging features, we further propose a comparative ranking layer which is supervised by a pair-wise comparative signal. This comparative ranking layer facilitates aging feature learning and improves the performance of the main age estimation task. In addition, since one face can be included in many different training pairs, we can make full use of the limited training data. It is noted that both of these two novel layers are differentiable, so our model is end-to-end trainable. Extensive experiments on the two of the largest benchmark datasets show that our deep age estimation model gains notable advantage on accuracy when compared against existing methods.","Authors":"Li, K (Li, Kai) ; Xing, JL (Xing, Junliang) ; Hu, WM (Hu, Weiming) ; Maybank, SJ (Maybank, Stephen J.)","Title":"D2C: Deep cumulatively and comparatively learning for human age estimation"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397371800021 ISSN: 0031-3203 eISSN: 1873-5142","Keywords":"Change detection; Information unbalanced images; Deep neural networks; Feature representation; Feature mapping KeyWords Plus:UNSUPERVISED CHANGE DETECTION; MULTITEMPORAL SAR IMAGES; URBAN CHANGE DETECTION; REMOTE-SENSING IMAGES; HYPERSPECTRAL IMAGES; NEURAL-NETWORKS; ALGORITHMS; CRITERION; FRAMEWORK; FUSION","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"PATTERN RECOGNITION Volume: 66 Pages: 213-228 Special Issue: SI DOI: 10.1016/j.patcog.2017.01.002 Published: JUN 2017","Abstract":"This paper mainly introduces a novel deep learning and mapping (DLM) framework oriented to the ternary change detection task for information unbalanced images. Different from the traditional intensity-based methods available, the DLM framework is based on the operation of the features extracted from the two images. Due to the excellent performance of deep learning in information representation and feature learning, two networks are used here. First, the stacked denoising autoencoder is used on two images, serving as a feature extractor. Then after a sample selection process, the stacked mapping network is employed to obtain the mapping functions, establishing the relationship between the features for each class. Finally, a comparison between the features is made and the final ternary map is generated through the clustering of the comparison result. This work is highlighted by two aspects. Firstly, previous works focus on two images with similar properties, whereas the DLM framework is based on two images with quite different properties, which is a usually encountered case. Secondly, the DLM framework is based on the analysis of feature instead of superficial intensity, which avoids the corruptions of unbalanced information to a large extent. Parameter tests on three datasets provide us with the appropriate parameter settings and the corresponding experimental results demonstrate its robustness and effectiveness in terms of accuracy and time complexity.","Authors":"Su, LZ (Su, Linzhi) ; Gong, MG (Gong, Maoguo) ; Zhang, PZ (Zhang, Puzhao) ; Zhang, MY (Zhang, Mingyang) ; Liu, J (Liu, Jia) ; Yang, HL (Yang, Hailun)","Title":"Deep learning and mapping based ternary change detection for information unbalanced images"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397371800022 ISSN: 0031-3203 eISSN: 1873-5142","Keywords":"Driver parsing; Pictorial structure; Deep features; Region with Convolutional Neural Networks; (R-CNN); Structure based N-cuts KeyWords Plus:SEGMENTATION","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"PATTERN RECOGNITION Volume: 66 Pages: 229-238 Special Issue: SI DOI: 10.1016/j.patcog.2016.11.028 Published: JUN 2017","Abstract":"This paper presents a Grammar-aware Driver Parsing (GDP) algorithm, with deep features, to provide a novel driver behavior situational awareness system (DB-SAW). A deep model is first trained to extract highly discriminative features of the driver. Then, a grammatical structure on the deep features is defined to be used as prior knowledge for a semi-supervised proposal candidate generation. The Region with Convolutional Neural Networks (R-CNN) method is ultimately utilized to precisely segment parts of the driver. The proposed method not only aims to automatically find parts of the driver in challenging \"drivers in the wild\" databases, i.e. the standardized Strategic Highway Research Program (SHRP-2) and the challenging Vision for Intelligent Vehicles and Application (VIVA), but is also able to investigate seat belt usage and the position of the driver's hands (on a phone vs on a steering wheel). We conduct experiments on various applications and compare our GDP method against other state-of-the-art detection and segmentation approaches, i.e. SDS [1], CRF-RNN [2], DJTL [3], and R-CNN [4] on SHRP-2 and VIVA databases.","Authors":"Le, THN (Le, T. Hoang Ngan) ; Zhu, CC (Zhu, ChenChen) ; Zheng, YT (Zheng, Yutong) ; Luu, K (Luu, Khoa) ; Savvides, M (Savvides, Marios)","Title":"DeepSafeDrive: A grammar-aware driver parsing approach to Driver Behavioral Situational Awareness (DB-SAW)"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393932800023 ISSN: 0377-2217 eISSN: 1872-6860","Keywords":"Finance; Deep learning; Gradient-boosting; Random forests; Ensemble learning KeyWords Plus:PERFORMANCE; FORECASTS; MARKET; COMBINATION; OUTRANKING; ANOMALIES","Categories":"Business & Economics; Operations Research & Management Science Web of Science Categories:Management; Operations Research & Management Science","Journal Information":"EUROPEAN JOURNAL OF OPERATIONAL RESEARCH Volume: 259 Issue: 2 Pages: 689-702 DOI: 10.1016/j.ejor.2016.10.031 Published: JUN 1 2017","Abstract":"In recent years, machine learning research has gained momentum: new developments in the field of deep learning allow for multiple levels of abstraction and are starting to supersede well-known and powerful tree-based techniques mainly operating on the original feature space. All these methods can be applied to various fields, including finance. This paper implements and analyzes the effectiveness of deep neural networks (DNN), gradient-boosted-trees (GBT), random forests (RAF), and several ensembles of these methods in the context of statistical arbitrage. Each model is trained on lagged returns of all stocks in the S&P 500, after elimination of survivor bias. From 1992 to 2015, daily one-day-ahead trading signals are generated based on the probability forecast of a stock to outperform the general market. The highest k probabilities are converted into long and the lowest k probabilities into short positions, thus censoring the less certain middle part of the ranking. Empirical findings are promising. A simple, equal-weighted ensemble (ENS1) consisting of one deep neural network, one gradient-boosted tree, and one random forest produces out-of-sample returns exceeding 0.45 percent per day for k = 10, prior to transaction costs. Irrespective of the fact that profits are declining in recent years, our findings pose a severe challenge to the semi-strong form of market efficiency. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Krauss, C (Krauss, Christopher) ; Do, XA (Xuan Anh Do) ; Huck, N (Huck, Nicolas)","Title":"Deep neural networks, gradient-boosted trees, random forests: Statistical arbitrage on the S&P 500"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397689300018 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Deep learning; Image classification; Support vector machine; Extreme learning machine; Object recognition KeyWords Plus:CLASSIFICATION; DIMENSIONALITY; REGRESSION","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 239 Pages: 194-203 DOI: 10.1016/j.neucom.2017.02.016 Published: MAY 24 2017","Abstract":"Deep learning with a convolutional neural network (CNN) has been proved to be very effective in feature extraction and representation of images. For image classification problems, this work aims at exploring the capability of extreme learning machine on high-level deep features of images. Additionally, motivated by the biological learning mechanism of ELM, in this paper, an adaptive extreme learning machine (AELM) method is proposed for handling cross-task (domain) learning problems, without loss of its nature of randomization and high efficiency. The proposed AELM is an extension of ELM from single task to cross task learning, by introducing a new error term and Laplacian graph based manifold regularization term in objective function. We have discussed the nearest neighbor, support vector machines and extreme learning machines for image classification under deep convolutional activation feature representation. Specifically, we adopt 4 benchmark object recognition datasets from multiple sources with domain bias for evaluating different classifiers. The deep features of the object dataset are obtained by a well-trained CNN with five convolutional layers and three fully-connected layers on ImageNet. Experiments demonstrate that the proposed AELM is comparable and effective in single and multiple domains based recognition tasks. (C) 2017 Elsevier B.V. All rights reserved.","Authors":"Zhang, L (Zhang, Lei) ; He, ZW (He, Zhenwei) ; Liu, Y (Liu, Yan)","Title":"Deep object recognition across domains based on adaptive extreme learning machine"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397372100002 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Aircraft fuel system; Failure mode; Fault diagnosis; Deep belief network; Quantum inspired neural network KeyWords Plus:ARCHITECTURE SELECTION; LEARNING ALGORITHM; BOLTZMANN MACHINES; BELIEF NETWORKS; COMPUTER; MODEL; CLASSIFICATION","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 238 Pages: 13-23 DOI: 10.1016/j.neucom.2017.01.032 Published: MAY 17 2017","Abstract":"Fault diagnosis for aircraft fuel system can not only improve flight security, but also reduce the huge cost due to regular maintenance. It remains a problem because of the complicated system and the heterogeneous failure modes, especially the different failure modes that have similar impacts on the system. This paper uses the deep quantum inspired neural network (DQINN) which is an improved deep quantum network (DQN) to solve such problem. This method is the combination of classical deep belief network (DBN) and quantum inspired neural network (QINN). For the purpose of inhetiting the advantages of DBN and QINN, the structure of DQINN is built in a new fashion. From a system perspective, the DQINN is constructed by the linear superposition of multiple DBNs with quantum intervals in the last hidden layer. Experiments conducted on standard datasets show that DQINN outperforms other three classical algorithms. Finally, a normal model of aircraft fuel system is built and four kinds of common failure modes of the core components are injected into this model, respectively. And the DQINN is applied to the aircraft fuel system fault diagnosis. (C) 2017 Elsevier B.V. All rights reserved.","Authors":"Gao, ZH (Gao, Zehai) ; Ma, CB (Ma, Cunbao) ; Song, D (Song, Dong) ; Liu, Y (Liu, Yang)","Title":"Deep quantum inspired neural network with application to aircraft fuel system fault diagnosis"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397372100035 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Particular object retrieval; Bag-of-words; SIFT matching; Convolutional neural networks KeyWords Plus:NEAREST-NEIGHBOR SEARCH; IMAGE RETRIEVAL; SCALE; SIMILARITY","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 238 Pages: 399-409 DOI: 10.1016/j.neucom.2017.01.081 Published: MAY 17 2017","Abstract":"Many object instance retrieval systems are typically based on matching of local features, such as SIFT. However, these local descriptors serve as low-level clues, which are not sufficiently distinctive to prevent false matches. Recently, deep convolutional neural networks (CNN) have shown their promise as a semantic-aware representation for many computer vision tasks. In this paper, we propose a novel approach to employ CNN evidences to improve the SIFT matching accuracy, which plays a critical role in improving the object retrieval performance. To weaken the interference of noise, we extract compact CNN representations from a number of generic object regions. Then a query-adaptive method is proposed to choose appropriate CNN evidence to verify each pre-matched SIFT pair. Two different visual matching verification functions are introduced and evaluated. Moreover, we investigate the suitability of fine-tuning the CNN for our proposed approach. Extensive experiments on benchmark dataSets demonstrate the effectiveness of our method for particular object retrieval. Our results compare favorably to the state-of-the-art methods with acceptable memory usage and query time. (C) 2017 Elsevier B.V. All rights reserved.","Authors":"Zhang, GX (Zhang, Guixuan) ; Zeng, Z (Zeng, Zhi) ; Zhang, SW (Zhang, Shuwu) ; Zhang, Y (Zhang, Yuan) ; Wu, WC (Wu, Wanchun)","Title":"SIFT Matching with CNN Evidences for Particular Object Retrieval"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397356700018 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Deep learning; Memristor; Neural networks; Hardware design; Backpropagation algorithm KeyWords Plus:RECOGNITION","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 237 Pages: 193-199 DOI: 10.1016/j.neucom.2016.10.061 Published: MAY 10 2017","Abstract":"We describe an approximation to backpropagation algorithm for training deep neural networks, which is designed to work with synapses implemented with memristors. The key idea is to represent the values of both the input signal and the backpropagated delta value with a series of pulses that trigger multiple positive or negative updates of the synaptic weight, and to use the min operation instead of the product of the two signals. In computational simulations, we show that the proposed approximation to backpropagation is well converged and may be suitable for memristor implementations of multilayer neural networks.","Authors":"Negrov, D (Negrov, D.) ; Karandashev, I (Karandashev, I.) ; Shakirov, V (Shakirov, V.) ; Matveyev, Y (Matveyev, Yu.) ; Dunin-Barkowski, W (Dunin-Barkowski, W.) ; Zenkevich, A (Zenkevich, A.)","Title":"An approximate backpropagation learning rule for memristor based neural networks using synaptic plasticity"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397356700021 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Image retrieval; CNN; Multi-level features","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 237 Pages: 235-241 DOI: 10.1016/j.neucom.2016.12.002 Published: MAY 10 2017","Abstract":"Deep convolutional neural networks have demonstrated breakthrough accuracies for image classification. A series of feature extractors learned from CNN have been used in other computer vision tasks. However, CNN features of different layers aim to encode different-level information. High-layer features care More about semantic information but less detail information, while low-layer features contain more detail information but suffer from the problem of background clutter and semantic ambiguity. We propose to exploit complementary strengths of different layers in a simple but effective way. A mapping function is designed to highlight the effectiveness of low-layer similarity, when measuring fine-grained similarity between query linage and its nearest neighbors with similar semantic. Extensive experiments show that our method can achieve Competitive performance on popular retrieval benchmarks. Extensive experiments show that the proposed method outperforms the features extracted from single layers and their direct concatenations. Meanwhile, our method achieves competitive performance on popular retrieval benchmarks.","Authors":"Yu, W (Yu, Wei) ; Yang, KY (Yang, Kuiyuan) ; Yao, HX (Yao, Hongxun) ; Sun, XS (Sun, Xiaoshuai) ; Xu, PF (Xu, Pengfei)","Title":"Exploiting the complementary strengths of multi-layer CNN features for image retrieval"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397356700030 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Image segmentation; Learning; CNN; Choroid; OCT KeyWords Plus:AUTOMATIC SEGMENTATION; MACULAR DEGENERATION; IMAGE SEGMENTATION; THICKNESS; HEALTHY; OCT","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 237 Pages: 332-341 DOI: 10.1016/j.neucom.2017.01.023 Published: MAY 10 2017","Abstract":"Examining choroid in Optical Coherence Tomography (OCT) plays a vital role in pathophysiologic factors of many ocular conditions. Among the existing approaches to detecting choroidal boundaries, graph-searching based techniques belong to the state-of-the-art. However, most of these techniques rely on hand-crafted models on the graph-edge weight and their performances are limited mainly due to the weak choroidal boundaries, textural structure of the choroid, inhomogeneity of the textural structure of the choroid and great variation of the choroidal thickness. In order to circumvent this limitation, we present a multi-scale and end-to-end convolutional network architecture where an optimal graph-edge weight can be learned directly from raw pixels. Our method operates on multiple scales and combines local and global information from the 2D OCT image. Experimental results obtained based on 912 OCT B-scans show that our learned graph-edge weights outperform conventional hand-crafted ones and behave robustly and accurately no matter the OCT image is from normal subjects or patients for whom significant retinal structure variations can be observed.","Authors":"Sui, XD (Sui, Xiaodan) ; Zheng, YJ (Zheng, Yuanjie) ; Wei, BZ (Wei, Benzheng) ; Bi, HS (Bi, Hongsheng) ; Wu, JF (Wu, Jianfeng) ; Pan, XM (Pan, Xuemei) ; Yin, YL (Yin, Yilong) ; Zhang, ST (Zhang, Shaoting)","Title":"Choroid segmentation from Optical Coherence Tomography with graph edge weights learned from deep convolutional neural networks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396952300018 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Chinese anthroposcopy; Chinese face reading; Facial attribute estimation; Deep convolutional neural networks; Facial region pooling layer","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 236 Pages: 153-163 Special Issue: SI DOI: 10.1016/j.neucom.2016.09.110 Published: MAY 2 2017","Abstract":"Chinese face reading has demonstrated the often satisfying capabilities to tell the characteristics (mostly exaggerated as fortune) of a person by reading his/her face, i.e. understanding the fine-grained facial attributes (e.g., length of nose, single/double-fold eyelid, density of eyebrows, etc.). Thus, a smart face reading system should estimate the fine-grained facial attributes well. Therefore, In this paper, we first study the fine-grained facial attribute estimation problem and propose a novel deep convolutional network equipped with a new facial region pooling layer (called FRP-net), to accurately estimate the fine-grained facial attributes. To capture the characteristics of fine-grained facial attributes, the embedded FRP layer implements the pooling operation on the searched facial region windows (locates the region of each facial attribute) instead of the commonly-used sliding windows. Further, we push the proposed fine-grained facial attribute estimation method into the face reading problem and present a computational face reader system to automatically infer the characteristics of a person based on his/her face. For example, it can estimate the attractive and easy-going characteristics of a Chinese person from his/her big eyes according to the Chinese anthroposcopy literature. The experimental results on facial attribute estimation demonstrate the superiority of the proposed FRP-net compared to the baselines, and the qualitative and quantitative evaluations on face reading validate the excellence of the presented face reader system.","Authors":"Shu, XB (Shu, Xiangbo) ; Cai, YF (Cai, Yunfei) ; Yang, L (Yang, Liu); Zhang, LY (Zhang, Liyan) ; Tang, JH (Tang, Jinhui)","Title":"Computational face reader based on facial attribute estimation"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395901200016 ISSN: 1568-4946 eISSN: 1872-9681","Keywords":"Empirical Mode Decomposition; Deep learning; Ensemble method; Time series forecasting; Load demand forecasting; Neural networks; Support vector regression; Random forests KeyWords Plus:SUPPORT VECTOR REGRESSION; ARTIFICIAL NEURAL-NETWORKS; RANDOM FORESTS; ELECTRICITY; ALGORITHM; RECOGNITION; CLASSIFIERS; FRAMEWORK; MACHINES; SYSTEM","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications","Journal Information":"APPLIED SOFT COMPUTING Volume: 54 Pages: 246-255 DOI: 10.1016/j.asoc.2017.01.015 Published: MAY 2017","Abstract":"Load demand forecasting is a critical process in the planning of electric utilities. An ensemble method composed of Empirical Mode Decomposition (EMD) algorithm and deep learning approach is presented in this work. For this purpose, the load demand series were first decomposed into several intrinsic mode functions (IMFs). Then a Deep Belief Network (DBN) including two restricted Boltzmann machines (RBMs) was used to model each of the extracted IMFs, so that the tendencies of these IMFs can be accurately predicted. Finally, the prediction results of all IMFs can be combined by either unbiased or weighted summation to obtain an aggregated output for load demand. The electricity load demand data sets from Australian Energy Market Operator (AEMO) are used to test the effectiveness of the proposed EMD-based DBN approach. Simulation results demonstrated attractiveness of the proposed method compared with nine forecasting methods. (C) 2017 Elsevier B.V. All rights reserved.","Authors":"Qiu, XH (Qiu, Xueheng) ; Ren, Y (Ren, Ye) ; Suganthan, PN (Suganthan, Ponnuthurai Nagaratnam) ; Amaratunga, GAJ (Amaratunga, Gehan A. J.)","Title":"Empirical Mode Decomposition based ensemble deep learning for load demand time series forecasting"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000398581900001 ISSN: 1093-9687 eISSN: 1467-8667","Categories":"Computer Science; Construction & Building Technology; Engineering; Transportation Web of Science Categories:Computer Science, Interdisciplinary Applications; Construction & Building Technology; Engineering, Civil; Transportation Science & Technology","Journal Information":"COMPUTER-AIDED CIVIL AND INFRASTRUCTURE ENGINEERING Volume: 32 Issue: 5 Pages: 361-378 DOI: 10.1111/mice.12263 Published: MAY 2017","Abstract":"A number of image processing techniques (IPTs) have been implemented for detecting civil infrastructure defects to partially replace human-conducted onsite inspections. These IPTs are primarily used to manipulate images to extract defect features, such as cracks in concrete and steel surfaces. However, the extensively varying real-world situations (e.g., lighting and shadow changes) can lead to challenges to the wide adoption of IPTs. To overcome these challenges, this article proposes a vision-based method using a deep architecture of convolutional neural networks (CNNs) for detecting concrete cracks without calculating the defect features. As CNNs are capable of learning image features automatically, the proposed method works without the conjugation of IPTs for extracting features. The designed CNN is trained on 40 K images of 256 x 256 pixel resolutions and, consequently, records with about 98% accuracy. The trained CNN is combined with a sliding window technique to scan any image size larger than 256 x 256 pixel resolutions. The robustness and adaptability of the proposed approach are tested on 55 images of 5,888 x 3,584 pixel resolutions taken from a different structure which is not used for training and validation processes under various conditions (e.g., strong light spot, shadows, and very thin cracks). Comparative studies are conducted to examine the performance of the proposed CNN using traditional Canny and Sobel edge detection methods. The results show that the proposed method shows quite better performances and can indeed find concrete cracks in realistic situations.","Authors":"Cha, YJ (Cha, Young-Jin) ; Choi, W (Choi, Wooram) ; Buyukozturk, O (Buyukozturk, Oral)","Title":"Deep Learning-Based Crack Damage Detection Using Convolutional Neural Networks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000398535900014 ISSN: 0887-3801 eISSN: 1943-5487","Keywords":"Surveillance; Aerial camera arrays; Data collection; Vehicle tracking; Vehicle detection; Computer vision; Speeded up robust features (SURF); Deep learning; Convolutional neural network","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Interdisciplinary Applications; Engineering, Civil","Journal Information":"JOURNAL OF COMPUTING IN CIVIL ENGINEERING Volume: 31 Issue: 3 Article Number: 04016072 DOI: 10.1061/(ASCE)CP.1943-5487.0000646 Published: MAY 2017","Abstract":"The paper presents a novel computer vision-based traffic surveillance system capable of processing aerial imagery to track vehicles and their movements. The system uses a preprocessed 1-Hz image sequence with a coverage of 64.80 km(2) (25 sq mi) from an aerial camera array mounted on an airplane. The unique characteristics of the input data make this work challenging. Heuristic and machine-learning approaches are combined and evaluated to detect and track vehicles for the purpose of collecting speed, density, and volume data for uninterrupted flow corridors, which are useful for big-data monitoring of traffic parameters over an entire 64.80 km2 (25 sq mi) area with a single sensor. The deep learning combined with speeded up robust features (SURF)-based approach is able to achieve over 94, 93, and 92% accuracies in speed, density, and volume estimates, respectively, on 50 s of data when compared with manually collected ground truth. It has 100% accuracy in measuring level of service (LOS) for the uninterrupted flow facilities tested. These evaluations were conducted for facilities of different levels of congestion as indicated by the different levels of service. With further research, improved preprocessing, and a higher frame rate, the accuracy of tracking vehicles can be improved, which will allow for other potential applications such as identification of erratic drivers and origin-destination studies. (C) 2016 American Society of Civil Engineers.","Authors":"Zhao, X (Zhao, Xi) ; Dawson, D (Dawson, Douglas) ; Sarasua, WA (Sarasua, Wayne A.) ; Birchfield, ST (Birchfield, Stanley T.)","Title":"Automated Traffic Surveillance System with Aerial Camera Arrays Imagery: Macroscopic Data Collection with Vehicle Tracking"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397689000013 ISSN: 0164-1212 eISSN: 1873-1228","Keywords":"Data stream mining; Big data; Classification; GPUs KeyWords Plus:NETWORKS","Categories":"Computer Science Web of Science Categories:Computer Science, Software Engineering; Computer Science, Theory & Methods","Journal Information":"JOURNAL OF SYSTEMS AND SOFTWARE Volume: 127 Pages: 195-204 DOI: 10.1016/jjss.2016.06.009 Published: MAY 2017","Abstract":"Big Data streams are being generated in a faster, bigger, and more commonplace. In this scenario, Hoeffding Trees are an established method for classification. Several extensions exist, including high performing ensemble setups such as online and leveraging bagging. Also, k-nearest neighbors is a popular choice, with most extensions dealing with the inherent performance limitations over a potentially-infinite stream. At the same time, gradient descent methods are becoming increasingly popular, owing in part to the successes of deep learning. Although deep neural networks can learn incrementally, they have so far proved too sensitive to hyper-parameter options and initial conditions to be considered an effective 'off -the-shelf' data-streams solution. In this work, we look at combinations of Hoeffding-trees, nearest neighbor, and gradient descent methods with a streaming preprocessing approach in the form of a random feature functions filter for additional predictive power. We further extend the investigation to implementing methods on GPUs, which we test on some large real-world datasets, and show the benefits of using GPUs for data-stream learning due to their high scalability. Our empirical evaluation yields positive results for the novel approaches that we experiment with, highlighting important issues, and shed light on promising future directions in approaches to data-stream classification. (C) 2016 Elsevier Inc. All rights reserved.","Authors":"Marron, D (Marron, Diego) ; Read, J (Read, Jesse) ; Bifet, A (Bifet, Albert) ; Navarro, N (Navarro, Nacho)","Title":"Data stream classification using random feature functions and novel method combinations"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396949800010 ISSN: 0921-8890 eISSN: 1872-793X","Keywords":"Tool-body assimilation; Motor babbling; Deep neural network; Recurrent neural network; Transfer learning KeyWords Plus:NEURAL-NETWORKS; INVERSE KINEMATICS; AFFORDANCES; PERCEPTION","Categories":"Automation & Control Systems; Computer Science; Robotics Web of Science Categories:Automation & Control Systems; Computer Science, Artificial Intelligence; Robotics","Journal Information":"ROBOTICS AND AUTONOMOUS SYSTEMS Volume: 91 Pages: 115-127 DOI: 10.1016/j.robot.2017.01.002 Published: MAY 2017","Abstract":"We propose a tool-body assimilation model that considers grasping during motor babbling for using tools. A robot with tool-use skills can be useful in human robot symbiosis because this allows the robot to expand its task performing abilities. Past studies that included tool-body assimilation approaches were mainly focused on obtaining the functions of the tools, and demonstrated the robot starting its motions with a tool pre-attached to the robot. This implies that the robot would not be able to decide whether and where to grasp the tool. In real life environments, robots would need to consider the possibilities of tool grasping positions, and then grasp the tool. To address these issues, the robot performs motor babbling by grasping and nongrasping the tools to learn the robot's body model and tool functions. In addition, the robot grasps various parts of the tools to learn different tool functions from different grasping positions. The motion experiences are learned using deep learning. In model evaluation, the robot manipulates an object task without tools, and with several tools of different shapes. The robot generates motions after being shown the initial state and a target image, by deciding whether and where to grasp the tool. Therefore, the robot is capable of generating the correct motion and grasping decision when the initial state and a target image are provided to the robot. (C) 2017 The Authors. Published by Elsevier B.V.","Authors":"Takahashi, K (Takahashi, Kuniyuki) ; Kim, K (Kim, Kitae) ; Ogata, T (Ogata, Tetsuya) ; Sugano, S (Sugano, Shigeki)","Title":"Tool-body assimilation model considering grasping motion through deep learning"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394197700018 ISSN: 0031-3203 eISSN: 1873-5142","Keywords":"Deep neural networks; Heaimapping; Taylor decomposition; Relevance propagation; Image recognition KeyWords Plus:TRAINED NEURAL-NETWORKS; MODELS","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"PATTERN RECOGNITION Volume: 65 Pages: 211-222 DOI: 10.1016/j.patcog.2016.11.008 Published: MAY 2017","Abstract":"Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems such as image recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method called deep Taylor decomposition efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.","Authors":"Montavon, G (Montavon, Gregoire) ; Lapuschkin, S (Lapuschkin, Sebastian) ; Binder, A (Binder, Alexander) ; Samek, W (Samek, Wojciech) ; Muller, KR (Mueller, Klaus-Robert)","Title":"Explaining nonlinear classification decisions with deep Taylor decomposition"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394197700020 ISSN: 0031-3203 eISSN: 1873-5142","Keywords":"Linear discriminant analysis; Deep Fisher networks; Person re-identification KeyWords Plus:IMAGE CLASSIFICATION; LEARNING ALGORITHM; RECONSTRUCTION","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"PATTERN RECOGNITION Volume: 65 Pages: 238-250 DOI: 10.1016/j.patcog.2016.12.022 Published: MAY 2017","Abstract":"Person re-identification is to seek a correct match for a person of interest across different camera views among a large number of impostors. It typically involves two procedures of non-linear feature extractions against dramatic appearance changes, and subsequent discriminative analysis in order to reduce intra-personal variations while enlarging inter-personal differences. In this paper, we introduce a hybrid deep architecture which combines Fisher vectors and deep neural networks to learn non-linear transformations of pedestrian images to a deep space where data can be linearly separable. The proposed method starts from Fisher vector encoding which computes a sequence of local feature extraction, aggregation, and encoding. The resulting Fisher vector output are fed into stacked supervised layer to seek non-linear transformation into a deep space. On top of the deep neural network, Linear Discriminant Analysis (LDA) is reinforced such that linearly separable latent representations can be learned in an end-to-end fashion. By optimizing an objective function modified from LDA, the network is enforced to produce feature distributions which have a low variance within the same class and high variance between classes. The objective is essentially derived from the general LDA eigenvalue problem and allows to train the network with Stochastic Gradient Descent and back-propagate LDA gradients to compute Gaussian Mixture Model (GMM) gradients in Fisher vector encoding. For empirical evaluations, we test our approach on four benchmark data sets in person re-identification (VIPeR [1], CUHK03 [2], CUHK01 [3], and Market 1501 [4]). Extensive experiments on these benchmarks show that our method can achieve state-of-the-art results.","Authors":"Wu, L (Wu, Lin) ; Shen, CH (Shen, Chunhua) ; van den Hengel, A (van den Hengel, Anton)","Title":"Deep linear discriminant analysis on fisher networks: A hybrid architecture for person re-identification"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395869700001 ISSN: 1556-6013 eISSN: 1556-6021","Keywords":"Image sharing; privacy setting recommendation; object-privacy alignment; image privacy protection; privacysensitive object classes; deep multi-task learning; tree classifier for hierarchical object detection","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY Volume: 12 Issue: 5 Pages: 1005-1016 DOI: 10.1109/TIFS.2016.2636090 Published: MAY 2017","Abstract":"To achieve automatic recommendation of privacy settings for image sharing, a new tool called iPrivacy (image privacy) is developed for releasing the burden from users on setting the privacy preferences when they share their images for special moments. Specifically, this paper consists of the following contributions: 1) massive social images and their privacy settings are leveraged to learn the object-privacy relatedness effectively and identify a set of privacy-sensitive object classes automatically; 2) a deep multi-task learning algorithm is developed to jointly learn more representative deep convolutional neural networks and more discriminative tree classifier, so that we can achieve fast and accurate detection of large numbers of privacy-sensitive object classes; 3) automatic recommendation of privacy settings for image sharing can be achieved by detecting the underlying privacy-sensitive objects from the images being shared, recognizing their classes, and identifying their privacy settings according to the object-privacy relatedness; and 4) one simple solution for image privacy protection is provided by blurring the privacy-sensitive objects automatically. We have conducted extensive experimental studies on real-world images and the results have demonstrated both the efficiency and effectiveness of our proposed approach.","Authors":"Yu, J (Yu, Jun) ; Zhang, BP (Zhang, Baopeng) ; Kuang, ZZ (Kuang, Zhengzhong)[ 1,4,9,10 ] ; Lin, D (Lin, Dan)[ 5,11,12 ] ; Fan, JP (Fan, Jianping)","Title":"iPrivacy: Image Privacy Protection by Identifying Sensitive Objects via Deep Multi-Task Learning"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395869700002 ISSN: 1556-6013 eISSN: 1556-6021","Keywords":"Periocular recognition; deep learning; convolution neural network; training data augmentation","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY Volume: 12 Issue: 5 Pages: 1017-1030 DOI: 10.1109/TIFS.2016.2636093 Published: MAY 2017","Abstract":"Accurate biometric identification under real environments is one of the most critical and challenging tasks to meet growing demand for higher security. This paper proposes a new framework to efficiently and accurately match periocular images that are automatically acquired under less-constrained environments. Our framework, referred to as semantics-assisted convolutional neural networks (SCNNs) in this paper, incorporates explicit semantic information to automatically recover comprehensive periocular features. This strategy enables superior matching accuracy with the usage of relatively smaller number of training samples, which is often an issue with several biometrics. Our reproducible experimental results on four different publicly available databases suggest that the SCNN-based periocular recognition approach can achieve outperforming results, both in achievable accuracy and matching time, for less-constrained periocular matching. Additional experimental results presented in this paper also indicate that the effectiveness of proposed SCNN architecture is not only limited to periocular recognition but it can also be useful for generalized image classification. Without increasing the volume of training data, the SCNN is able to automatically extract more discriminative features from the input data than a single CNN, therefore can consistently improve the recognition performance. The experimental results presented in this paper validate such an approach to enable faster and more accurate periocular recognition under less constrained environments.","Authors":"Zhao, ZJ (Zhao, Zijing) ; Kumar, A (Kumar, Ajay)","Title":"Accurate Periocular Recognition Under Less Constrained Environment Using Semantics-Assisted Convolutional Neural Network"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393266500002 ISSN: 0885-2308 eISSN: 1095-8363","Keywords":"Language learning; L2 English speech; Suprasegmental; Intonation; Pitch accent; Deep neural networks KeyWords Plus:PITCH ACCENT; PRONUNCIATION; STRESS; FRAMEWORK; PATTERNS; FEATURES; PROSODY","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"COMPUTER SPEECH AND LANGUAGE Volume: 43 Pages: 18-33 DOI: 10.1016/j.csl.2016.11.006 Published: MAY 2017","Abstract":"This paper investigates the use of multi-distribution deep neural networks (MD-DNNs) for automatic intonation classification in second-language (L2) English speech. If a classified intonation is different from the target one, we consider that mispronunciation is detected and appropriate diagnostic feedback can be provided thereafter. To transcribe speech data for intonation classification, we propose the RULF labels which are used to transcribe an intonation as rising, upper, lower or falling. These four types of labels can be further merged into two groups - rising and falling. Based on the annotated data from 100 Mandarin and 100 Cantonese learners, we develop an intonation classifier, which considers only 8 frames (i.e., 80 ms) of pitch value prior to the end of the pitch contour over an intonational phrase (IP). This classifier determines the intonation of L2 English speech as either rising or falling with an accuracy of 93.0%. (C) 2016 Elsevier Ltd. All rights reserved..","Authors":"Li, K (Li, Kun) ; Wu, XX (Wu, Xixin) ; Meng, HL (Meng, Helen)","Title":"Intonation classification for L2 English speech using multi-distribution deep neural networks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395219700005 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Deep learning; Convolutional neural network; Multi-label classification","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 235 Pages: 38-45 DOI: 10.1016/j.neucom.2016.12.051 Published: APR 26 2017","Abstract":"Multi-label image classification is a challenging problem in computer vision. Motivated by the recent development in image classification performance using Deep Neural Networks, in this work, we propose a flexible deep Convolutional Neural Network (CNN) framework, called Local-Global-CNN (LGC), to improve multi-label image classification performance. LGC consists of firstly a local level multi-label classifier which takes object segment hypotheses as inputs to a local CNN. The output results of these local hypotheses are aggregated together with max-pooling and then re-weighted to consider the label co-occurrence or inter-dependencies information by using a graphical model in the label space. LGC also utilizes a global CNN that is trained by multi-label images to directly predict the multiple labels from the input. The predictions of local and global level classifiers are finally fused together to obtain MAP estimation of the final multi-label prediction. The above LGC framework could benefit from a pre-train process with a large-scale single-label image dataset, e.g., ImageNet. Experimental results have shown that the proposed framework could achieve promising performance on Pascal VOC2007 and VOC2012 multi-label image dataset.","Authors":"Yu, QH (Yu, Qinghua) ; Wang, JJ (Wang, Jinjun) ; Zhang, SZ (Zhang, Shizhou) ; Gong, YH (Gong, Yihong) ; Zhao, JZ (Zhao, Jizhong)","Title":"Combining local and global hypotheses in deep neural network for multi-label image classification"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395219700024 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Convolutional neural networks; Deep learning; Plant identification; Transfer learning; Inverse rank score","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 235 Pages: 228-235 DOI: 10.1016/j.neucom.2017.01.018 Published: APR 26 2017","Abstract":"We use deep convolutional neural networks to identify the plant species captured in a photograph and evaluate different factors affecting the performance of these networks. Three powerful and popular deep learning architectures, namely GoogLeNet, AlexNet, and VGGNet, are used for this purpose. Transfer learning is used to fine-tune the pre-trained models, using the plant task datasets of LifeCLEF 2015. To decrease the chance of overfitting, data augmentation techniques are applied based on image transforms such as rotation, translation, reflection, and scaling. Furthermore, the networks' parameters are adjusted and different classifiers are fused to improve overall performance. Our best combined system has achieved an overall accuracy of 80% on the validation set and an overall inverse rank score of 0.752 on the official test set. A comparison of our results against the results of the LifeCLEF 2015 plant identification campaign shows that we have improved the overall validation accuracy of the top system by 15% points and its overall inverse rank score on the test set by 0.1 while outperforming the top three competition participants in all categories. The system recently obtained a very close second place in the P1antCLEF 2016.","Authors":"Ghazi, MM (Ghazi, Mostafa Mehdipour) ; Yanikoglu, B (Yanikoglu, Berrin) ; Aptoula, E (Aptoula, Erchan)","Title":"Plant identification using deep neural networks via optimization of transfer learning parameters"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395221800002 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Autoencoder; Convolutional neural network; Deep learning; Deep belief network; Restricted Boltzmann machine KeyWords Plus:NONLINEAR STOCHASTIC-SYSTEMS; SPEECH RECOGNITION; CONTRASTIVE DIVERGENCE; DENOISING AUTOENCODERS; MISSING MEASUREMENTS; BOLTZMANN MACHINES; STATE ESTIMATION; ACOUSTIC MODELS; CLASSIFICATION; ALGORITHM","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 234 Pages: 11-26 DOI: 10.1016/j.neucom.2016.12.038 Published: APR 19 2017","Abstract":"Since the proposal of a fast learning algorithm for deep belief networks in 2006, the deep learning techniques have drawn ever-increasing research interests because of their inherent capability of overcoming the drawback of traditional algorithms dependent on hand-designed features. Deep learning approaches have also been found to be suitable for big data analysis with successful applications to computer vision, pattern recognition, speech recognition, natural language processing, and recommendation systems. In this paper, we discuss some widely used deep learning architectures and their practical applications. An up-to-date overview is provided on four deep learning architectures, namely, autoencoder, convolutional neural network, deep belief network, and restricted Boltzmann machine. Different types of deep neural networks are surveyed and recent progresses are summarized. Applications of deep learning techniques on some selected areas (speech recognition, pattern recognition and computer vision) are highlighted. A list of future research topics are finally given with clear justifications.","Authors":"Liu, WB (Liu, Weibo) ; Wang, ZD (Wang, Zidong) ; Liu, XH (Liu, Xiaohui) ; Zengb, NY (Zeng, Nianyin) ; Liu, YR (Liu, Yurong) ; Alsaadi, FE (Alsaadi, Fuad E.)","Title":"A survey of deep neural network architectures and their applications"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395221800016 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Biologically inspired model; SAR target configuration recognition; Episodic features; Semantic features; Aspect angle estimation KeyWords Plus:PROPERTY; CORTEX","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 234 Pages: 185-191 DOI: 10.1016/j.neucom.2016.12.054 Published: APR 19 2017","Abstract":"How to extract proper features is very important for synthetic aperture radar (SAR) target configuration recognition. However, most of feature extraction methods are hand-designed and usually can not achieve a satisfactory performance. In this paper, we propose a novel method based on the biologically inspired model to extract features automatically from limited data. Specifically, we learn episodic features (containing the key components and their spatial relations) and semantic features (i.e., semantic descriptions of the key components) which are two important types of features for the human cognition process. Episode features are learned through a deep neural network (DNN) and then semantic geometric features of the key components are defined. Moreover, SAR images are very sensitive to aspect angles. Therefore, we use episode features to estimate aspect angles of testing samples for the final recognition. This paper is a preliminary study and the preliminary experimental results on the moving and stationary target automatic recognition (MSTAR) database demonstrate the effectiveness of the proposed method.","Authors":"Huang, XY (Huang, Xiayuan) ; Nie, XL (Nie, Xiangli) ; Wu, W (Wu, Wei) ; Qiao, H (Qiao, Hong) ; Zhang, B (Zhang, Bo)","Title":"SAR target configuration recognition based on the biologically inspired model"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392770900019 ISSN: 0957-4174 eISSN: 1873-6793","Keywords":"Natural language processing; Sentiment analysis; Deep neural network KeyWords Plus:LEVEL; LANGUAGE; NETWORK","Categories":"Computer Science; Engineering; Operations Research & Management Science Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science","Journal Information":"EXPERT SYSTEMS WITH APPLICATIONS Volume: 72 Pages: 221-230 DOI: 10.1016/j.eswa.2016.10.065 Published: APR 15 2017","Abstract":"Different types of sentences express sentiment in very different ways. Traditional sentence-level sentiment classification research focuses on one-technique-fits-all solution or only centers on one special type of sentences. In this paper, we propose a divide-and-conquer approach which first classifies sentences into different types, then performs sentiment analysis separately on sentences from each type. Specifically, we find that sentences tend to be more complex if they contain more sentiment targets. Thus, we propose to first apply a neural network based sequence model to classify opinionated sentences into three types according to the number of targets appeared in a sentence. Each group of sentences is then fed into a one-dimensional convolutional neural network separately for sentiment classification. Our approach has been evaluated on four sentiment classification datasets and compared with a wide range of baselines. Experimental results show that: (1) sentence type classification can improve the performance of sentence-level sentiment analysis; (2) the proposed approach achieves state-of-the-art results on several benchmarking datasets. (C) 2016 The Authors. Published by Elsevier Ltd.","Authors":"Chen, T (Chen, Tao) ; Xu, RF (Xu, Ruifeng) ; He, YL (He, Yulan) ; Wang, X (Wang, Xuan) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number He, Yulan http://orcid.org/0000-0003-3948-5845","Title":"Improving sentiment analysis via sentence type classification using BiLSTM-CRF and CNN"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392770900028 ISSN: 0957-4174 eISSN: 1873-6793","Keywords":"Machine learning; Classification; Deep learning; Convolutional neural networks; Parking space dataset KeyWords Plus:VEHICLE DETECTION; CLASSIFICATION; IMAGES","Categories":"Computer Science; Engineering; Operations Research & Management Science Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science","Journal Information":"EXPERT SYSTEMS WITH APPLICATIONS Volume: 72 Pages: 327-334 DOI: 10.1016/j.eswa.2016.10.055 Published: APR 15 2017","Abstract":"A smart camera is a vision system capable of extracting application-specific information from the captured images. The paper proposes a decentralized and efficient solution for visual parking lot occupancy detection based on a deep Convolutional Neural Network (CNN) specifically designed for smart cameras. This solution is compared with state-of-the-art approaches using two visual datasets: PKLot, already existing in literature, and CNRParlc-EXT. The former is an existing dataset, that allowed us to exhaustively compare with previous works. The latter dataset has been created in the context of this research, accumulating data across various seasons of the year, to test our approach in particularly challenging situations, exhibiting occlusions, and diverse and difficult viewpoints. This dataset is public available to the scientific community and is another contribution of our research. Our experiments show that our solution outperforms and generalizes the best performing approaches on both datasets. The performance of our proposed CNN architecture on the parking lot occupancy detection task, is comparable to the well-known AlexNet, which is three orders of magnitude larger. (C) 2016 Elsevier Ltd. All rights reserved.","Authors":"Amato, G (Amato, Giuseppe) ; Carrara, F (Carrara, Fabio) ; Falchi, F (Falchi, Fabrizio) ; Gennaro, C (Gennaro, Claudio) ; Meghini, C (Meghini, Carlo) ; Vairo, C (Vairo, Claudio) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Falchi, Fabrizio J-2920-2012 http://orcid.org/0000-0001-6258-5313 Amato, Giuseppe F-2227-2013 http://orcid.org/0000-0003-0171-4315 Falchi, Fabrizio B-5160-2015 http://orcid.org/0000-0001-6258-5313","Title":"Deep learning for decentralized parking lot occupancy detection"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000398671200001 PubMed ID: 28395661 ISSN: 1474-760X","Keywords":"Deep learning; Artificial neural network; Machine learning; Single-cell genomics; DNA methylation; Epigenetics KeyWords Plus:EMBRYONIC STEM-CELLS; EPIGENETIC HETEROGENEITY; SEQUENCE; GENOME; SITES; DIFFERENTIATION; HEXANUCLEOTIDE; VERTEBRATE; LANDSCAPES; EXPRESSION","Categories":"Biotechnology & Applied Microbiology; Genetics & Heredity Web of Science Categories:Biotechnology & Applied Microbiology; Genetics & Heredity","Journal Information":"GENOME BIOLOGY Volume: 18 Article Number: 67 DOI: 10.1186/s13059-017-1189-z Published: APR 11 2017","Abstract":"Recent technological advances have enabled DNA methylation to be assayed at single-cell resolution. However, current protocols are limited by incomplete CpG coverage and hence methods to predict missing methylation states are critical to enable genome-wide analyses. We report DeepCpG, a computational approach based on deep neural networks to predict methylation states in single cells. We evaluate DeepCpG on single-cell methylation data from five cell types generated using alternative sequencing protocols. DeepCpG yields substantially more accurate predictions than previous methods. Additionally, we show that the model parameters can be interpreted, thereby providing insights into how sequence composition affects methylation variability.","Authors":"Angermueller, C (Angermueller, Christof) ; Lee, HJ (Lee, Heather J.) ; Reik, W (Reik, Wolf) ; Stegle, O (Stegle, Oliver)","Title":"DeepCpG: accurate prediction of single-cell DNA methylation states using deep learning"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000398456900001 ISSN: 1687-5281","Keywords":"Fine-grained image recognition; Convolutional Neural Networks (CNN); Bag-of-visual-words; Feature weighting; Dimension reduction KeyWords Plus:CLASSIFICATION","Categories":"Engineering; Imaging Science & Photographic Technology Web of Science Categories:Engineering, Electrical & Electronic; Imaging Science & Photographic Technology","Journal Information":"EURASIP JOURNAL ON IMAGE AND VIDEO PROCESSING Article Number: 27 DOI: 10.1186/s13640-017-0176-3 Published: APR 8 2017","Abstract":"Fine-grained image recognition, a computer vision task filled with challenges due to its imperceptible inter-class variance and large intra-class variance, has been drawing increasing attention. While manual annotation can be utilized to effectively enhance performance in this task, it is extremely time-consuming and expensive. Recently, Convolutional Neural Networks (CNN) achieved state-of-the-art performance in image classification. We propose a fine-grained image recognition framework by exploiting CNN as the raw feature extractor along with several effective methods including a feature encoding method, a feature weighting method, and a strategy to better incorporate information from multi-scale images to further improve recognition ability. Besides, we investigate two dimension reduction methods and successfully merge them to our framework to compact the final image representation. Based on the discriminative and compact framework, we achieved the state-of-the-art performance in terms of classification accuracy on several fine-grained image recognition benchmarks based on weekly supervision.","Authors":"Zhang, WX (Zhang, Weixia) ; Yan, J (Yan, Jia) ; Shi, WX (Shi, Wenxuan) ; Feng, TP (Feng, Tianpeng) ; Deng, DX (Deng, Dexiang)","Title":"Refining deep convolutional features for improving fine-grained image recognition"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000398340900001 PubMed ID: 28378829 ISSN: 2045-2322","Categories":"Science & Technology - Other Topics Web of Science Categories:Multidisciplinary Sciences","Journal Information":"SCIENTIFIC REPORTS Volume: 7 Article Number: 45938 DOI: 10.1038/srep45938 Published: APR 5 2017","Abstract":"Tissue biomarker scoring by pathologists is central to defining the appropriate therapy for patients with cancer. Yet, inter-pathologist variability in the interpretation of ambiguous cases can affect diagnostic accuracy. Modern artificial intelligence methods such as deep learning have the potential to supplement pathologist expertise to ensure constant diagnostic accuracy. We developed a computational approach based on deep learning that automatically scores HER2, a biomarker that defines patient eligibility for anti-HER2 targeted therapies in breast cancer. In a cohort of 71 breast tumour resection samples, automated scoring showed a concordance of 83% with a pathologist. The twelve discordant cases were then independently reviewed, leading to a modification of diagnosis from initial pathologist assessment for eight cases. Diagnostic discordance was found to be largely caused by perceptual differences in assessing HER2 expression due to high HER2 staining heterogeneity. This study provides evidence that deep learning aided diagnosis can facilitate clinical decision making in breast cancer by identifying cases at high risk of misdiagnosis.","Authors":"Vandenberghe, ME (Vandenberghe, Michel E.) ; Scott, MLJ (Scott, Marietta L. J.) ; Scorer, PW (Scorer, Paul W.) ; Soderberg, M (Soderberg, Magnus) ; Balcerzak, D (Balcerzak, Denis) ; Barker, C (Barker, Craig)","Title":"Relevance of deep learning to facilitate the diagnosis of HER2 status in breast cancer"}, {"Document Information":"Document Type:Review Language:English Accession Number: WOS:000396971500009 ISSN: 0952-1976 eISSN: 1873-6769","Keywords":"Feedforward neural network; Metaheuristics; Nature-inspired algorithms; Multiobjective; Ensemble KeyWords Plus:PARTICLE SWARM OPTIMIZATION; MULTIOBJECTIVE EVOLUTIONARY ALGORITHMS; GRAVITATIONAL SEARCH ALGORITHM; IMPROVED GENETIC ALGORITHM; CENTRAL FORCE OPTIMIZATION; ANT COLONY OPTIMIZATION; OF-THE-ART; DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; LEARNING ALGORITHM","Categories":"Automation & Control Systems; Computer Science; Engineering Web of Science Categories:Automation & Control Systems; Computer Science, Artificial Intelligence; Engineering, Multidisciplinary; Engineering, Electrical & Electronic","Journal Information":"ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE Volume: 60 Pages: 97-116 DOI: 10.1016/j.engappai.2017.01.013 Published: APR 2017","Abstract":"Over the past two decades, the feedforward neural network (FNN) optimization has been a key interest among the researchers and practitioners of multiple disciplines. The FNN optimization is often viewed from the various perspectives: the optimization of weights, network architecture, activation nodes, learning parameters, learning environment, etc. Researchers adopted such different viewpoints mainly to improve the FNN's generalization ability. The gradient-descent algorithm such as backpropagation has been widely applied to optimite the FNNs. Its success is evident from the FNN's application to numerous real-world problems. However, due to the limitations of the gradient-based optimization methods, the metaheuristic algorithms including the evolutionary algorithms, swarm intelligence, etc., are still being widely explored by the researchers aiming to obtain generalized FNN for a given problem. This article attempts to summarize a broad spectrum of FNN optimization methodologies including conventional and metaheuristic approaches. This article also tries to connect various research directions emerged out of the FNN optimization practices, such as evolving neural network (NN), cooperative coevolution NN, complex-valued NN, deep learning, extreme learning machine, quantum NN, etc. Additionally, it provides interesting research challenges for future research to cope-up with the present information processing era.","Authors":"Ojha, VK (Ojha, Varun Kumar) ; Abraham, A (Abraham, Ajith) ; Snasel, V (Snasel, Vaclav)","Title":"Metaheuristic design of feedforward neural networks: A review of two decades of research"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000398178600005 ISSN: 2329-9290","Keywords":"Acoustic feature selection; mutual information; microphone array; sound source enhancement; Weiner filter KeyWords Plus:MEAN-SQUARE ERROR; SPEECH ENHANCEMENT; DIMENSIONALITY REDUCTION; MICROPHONE ARRAYS; SOURCE SEPARATION; REGRESSION; AUDIO","Categories":"Acoustics; Engineering Web of Science Categories:Acoustics; Engineering, Electrical & Electronic","Journal Information":"IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING Volume: 25 Issue: 4 Pages: 768-779 DOI: 10.1109/TASLP.2017.2662232 Published: APR 2017","Abstract":"An informative acoustic-feature-selection method for collecting target sources in noisy environments is proposed. Wiener filtering is a powerful framework for sound-source enhancement. For Wiener-filter estimation, statistical-mapping functions, such as deep neural network based or Gaussian mixture model based mappings, have been used. In this framework, it is essential to find informative acoustic features that provide effective cues for Wiener-filter estimation. In this study, we measured the informativeness of acoustic features using mutual information between acoustic features and supervised Wiener-filter parameters, e.g., prior signal-to-noise ratios, and developed a method for automatically selecting informative acoustic features from a large number of feature candidates. To automatically select optimum features, we derived a differentiable objective function in proportion to mutual information based on the kernel method. Since the higher order correlations between acoustic features and Wiener-filter parameters are calculated using the kernel method, the statistical dependence of these variables is accurately calculated; thus, only meaningful acoustic features are selected. Through several experiments conducted on a mock sports field, we confirmed that the signal-to-distortion ratio score improved when various types of target sources were surrounded by loud cheering noise.","Authors":"Koizumi, Y (Koizumi, Yuma) ; Niwa, K (Niwa, Kenta) ; Hioka, Y (Hioka, Yusuke) ; Kobayashi, K (Kobayashi, Kazunori) ; Ohmuro, H (Ohmuro, Hitoshi)","Title":"Informative Acoustic Feature Selection to Maximize Mutual Information for Collecting Target Sources"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000398178600008 ISSN: 2329-9290","Keywords":"Deep learning; deep neural network; deep belief network; i-vector; speaker recognition KeyWords Plus:NEURAL-NETWORKS; SPEECH RECOGNITION; VERIFICATION","Categories":"Acoustics; Engineering Web of Science Categories:Acoustics; Engineering, Electrical & Electronic","Journal Information":"IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING Volume: 25 Issue: 4 Pages: 807-817 DOI: 10.1109/TASLP.2017.2661705 Published: APR 2017","Abstract":"The lack of labeled background data makes a big performance gap between cosine and Probabilistic Linear Discriminant Analysis (PLDA) scoring baseline techniques for i-vectors in speaker recognition. Although there are some unsupervised clustering techniques to estimate the labels, they cannot accurately predict the true labels and they also assume that there are several samples from the same speaker in the background data that could not be true in reality. In this paper, the authors make use of Deep Learning (DL) to fill this performance gap given unlabeled background data. To this goal, the authors have proposed an impostor selection algorithm and a universal model adaptation process in a hybrid system based on deep belief networks and deep neural networks to discriminatively model each target speaker. In order to have more insight into the behavior of DL techniques in both single-and multisession speaker enrollment tasks, some experiments have been carried out in this paper in both scenarios. Experiments on National Institute of Standards and Technology 2014 i-vector challenge show that 46% of this performance gap, in terms of minimum of the decision cost function, is filled by the proposed DL-based system. Furthermore, the score combination of the proposed DL-based system and PLDA with estimated labels covers 79% of this gap.","Authors":"Ghahabi, O (Ghahabi, Omid) ; Hernando, J (Hernando, Javier)","Title":"Deep Learning Backend for Single and Multisession i-Vector Speaker Recognition"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000398178600009 ISSN: 2329-9290","Keywords":"Speaker adaptation; deep neural networks; i-vectors; multi-basis adaptive neural networks KeyWords Plus:HIDDEN MARKOV-MODELS; SPEECH RECOGNITION; SPEAKER ADAPTATION; TRANSFORMATIONS","Categories":"Acoustics; Engineering Web of Science Categories:Acoustics; Engineering, Electrical & Electronic","Journal Information":"IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING Volume: 25 Issue: 4 Pages: 818-828 DOI: 10.1109/TASLP.2017.2670141 Published: APR 2017","Abstract":"A lot of interest has been risen in the last years on the adaptation of deep neural network (DNN) acoustic models, as the latter become the state-of-art in automatic speech recognition. This work focuses on approaches that allow for rapid and robust adaptation of such models. First, i-vectors are added to the DNN input as speaker-informed features. An informative prior is introduced to i-vector estimation to improve the robustness to limited adaptation data. I-vectors are then combined with a structured adaptive DNN, the multibasis adaptive neural network (MBANN), and the complementarity of these adaptation techniques is investigated. Moreover, i-vectors are used to predict the MBANN transforms, avoiding the initial decoding pass and alignment. These approaches are evaluated on a U.S. English Broadcast News (BN) transcription task with two distinct sets of test data. The first, from the BN task and BN-style Youtube videos, yields test data acoustically matched to the training data, while the second set is from acoustically mismatched Youtube videos of diverse context. The performance gains from these schemes are found to be sensitive to the level of mismatch between training and test sets. The MBANN system combined with i-vector input achieves best performance for BN test sets. The i-vector-based predictive MBANN scheme is proven to be more robust to acoustically mismatched conditions and outperforms the other adaptation schemes in such scenarios.","Authors":"Karanasou, P (Karanasou, Penny) ; Wu, CY (Wu, Chunyang) ; Gales, M (Gales, Mark) ; Woodland, PC (Woodland, Philip C.)","Title":"I-Vectors and Structured Neural Networks for Rapid Adaptation of Acoustic Models"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000398178600014 ISSN: 2329-9290","Keywords":"CFSMN; Deep neural networks; feedforward sequential memory networks; language modeling; speech recognition KeyWords Plus:SPEECH RECOGNITION; NETWORKS; TIME; ERROR; MODEL","Categories":"Acoustics; Engineering Web of Science Categories:Acoustics; Engineering, Electrical & Electronic","Journal Information":"IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING Volume: 25 Issue: 4 Pages: 871-884 DOI: 10.1109/TASLP.2017.2672398 Published: APR 2017","Abstract":"In this paper, we propose a novel neural network structure, namely feedforward sequential memory networks (FSMN), to model long-term dependence in time series without using recurrent feedback. The proposed FSMN is a standard fully connected feedforward neural network equipped with some learnable memory blocks in its hidden layers. The memory blocks use a tapped-delay line structure to encode the long context information into a fixed-size representation as short-term memory mechanism which are somehow similar to the time-delay neural networks layers. We have evaluated the FSMNs in several standard benchmark tasks, including speech recognition and language modeling. Experimental results have shown that FSMNs outperform the conventional recurrent neural networks (RNN) while can be learned much more reliably and faster in modeling sequential signals like speech or language. Moreover, we also propose a compact feedforward sequential memory networks (cFSMN) by combining FSMN with low-rank matrix factorization and make a slight modification to the encoding method used in FSMNs in order to further simplify the network architecture. On the speech recognition Switchboard task, the proposed cFSMN structures can reduce the model size by 60% and speed up the learning by more than seven times while the model can still significantly outperform the popular bidirectional LSTMs for both frame-level cross-entropy criterion-based training and MMI-based sequence training.","Authors":"Zhang, SL (Zhang, Shiliang) ; Liu, C (Liu, Cong) ; Jiang, H (Jiang, Hui) ; Wei, S (Wei, Si) ; Dai, LR (Dai, Lirong) ; Hu, Y (Hu, Yu)","Title":"Nonrecurrent Neural Structure for Long-Term Dependence"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395932300005 ISSN: 1070-9908 eISSN: 1558-2361","Keywords":"Automatic speech recognition (ASR); Aurora-4; channelmismatch; deep neural network (DNN); locally normalized filter bank (LNFB)","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"IEEE SIGNAL PROCESSING LETTERS Volume: 24 Issue: 4 Pages: 377-381 DOI: 10.1109/LSP.2017.2661699 Published: APR 2017","Abstract":"This letter describes modifications to locally normalized filter banks (LNFB), which substantially improve their performance on the Aurora-4 robust speech recognition task using a Deep Neural Network-Hidden Markov Model (DNN-HMM)based speech recognition system. The modified coefficients, referred to as LNFB features, are a filter-bank version of locally normalized cepstral coefficients (LNCC), which have been described previously. The ability of the LNFB features is enhanced through the use of newly proposed dynamic versions of them, which are developed using an approach that differs somewhat from the traditional development of delta and delta-delta features. Further enhancements are obtained through the use of mean normalization and mean-variance normalization, which is evaluated both on a per-speaker and a per-utterance basis. The best performing feature combination (typically LNFB combined with LNFB delta and delta-delta features and mean-variance normalization) provides an average relative reduction in word error rate of 11.4% and 9.4%, respectively, compared to comparable features derived from Mel filter banks when clean and multinoise training are used for the Aurora-4 evaluation. The results presented here suggest that the proposed technique is more robust to channel mismatches between training and testing data than MFCC-derived features and is more effective in dealing with channel diversity.","Authors":"Fredes, J (Fredes, Josue) ; Novoa, J (Novoa, Jose) ; King, S (King, Simon) ; Stern, RM (Stern, Richard M.) ; Yoma, NB (Becerra Yoma, Nestor)","Title":"Locally Normalized Filter Banks Applied to Deep Neural-Network-Based Robust Speech Recognition"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000398296400014 PubMed ID: 28123005 ISSN: 0022-3077 eISSN: 1522-1598","Keywords":"prefrontal cortex; delta oscillations; calcium imaging; synchrony KeyWords Plus:LESS-THAN-1 HZ OSCILLATION; WORKING-MEMORY; PERSISTENT ACTIVITY; PYRAMIDAL CELLS; RAT-BRAIN; IN-VITRO; NEURONS; NEOCORTEX; ASSEMBLIES; DELTA","Categories":"Neurosciences & Neurology; Physiology Web of Science Categories:Neurosciences; Physiology","Journal Information":"JOURNAL OF NEUROPHYSIOLOGY Volume: 117 Issue: 4 Pages: 1581-1594 DOI: 10.1152/jn.00295.2016 Published: APR 2017","Abstract":"Cortical systems maintain and process information through the sustained activation of recurrent local networks of neurons. Layer 5 is known to have a major role in generating the recurrent activation associated with these functions, but relatively little is known about its intrinsic dynamics at the mesoscopic level of large numbers of neighboring neurons. Using calcium imaging, we measured the spontaneous activity of networks of deep-layer medial prefrontal cortical neurons in an acute slice model. Inferring the simultaneous activity of tens of neighboring neurons, we found that while the majority showed only sporadic activity, a subset of neurons engaged in sustained delta frequency rhythmic activity. Spontaneous activity under baseline conditions was weakly correlated between pairs of neurons, and rhythmic neurons showed little coherence in their oscillations. However, we consistently observed brief bouts of highly synchronous activity that must be attributed to network activity. NMDA-mediated stimulation enhanced rhythmicity, synchrony, and correlation within these local networks. These results characterize spontaneous prefrontal activity at a previously unexplored spatio-temporal scale and suggest that medial prefrontal cortex can act as an intrinsic generator of delta oscillations. NEW & NOTEWORTHY Using calcium imaging and a novel analytic framework, we characterized the spontaneous and NMDA-evoked activity of layer 5 prefrontal cortex at a largely unexplored spatiotemporal scale. Our results suggest that the mPFC microcircuitry is capable of intrinsically generating delta oscillations and sustaining synchronized network activity that is potentially relevant for understanding its contribution to cognitive processes.","Authors":"Blaeser, AS (Blaeser, Andrew S.) ; Connors, BW (Connors, Barry W.) ; Nurmikko, AV (Nurmikko, Arto V.)","Title":"Spontaneous dynamics of neural networks in deep layers of prefrontal cortex"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397551500008 PubMed ID: 28167394 ISSN: 1361-8415 eISSN: 1361-8423","Keywords":"Alzheimer's disease; Convolutional neural network; Deep ensemble learning; Sparse regression model KeyWords Plus:MILD COGNITIVE IMPAIRMENT; ALZHEIMERS-DISEASE; FEATURE-SELECTION; NEURAL-NETWORKS; PATTERN-CLASSIFICATION; SEGMENTATION; REGISTRATION; MRI; PREDICTION; IMAGES","Categories":"Computer Science; Engineering; Radiology, Nuclear Medicine & Medical Imaging Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging","Journal Information":"MEDICAL IMAGE ANALYSIS Volume: 37 Pages: 101-113 DOI: 10.1016/j.media.2017.01.008 Published: APR 2017","Abstract":"Recent studies on brain imaging analysis witnessed the core roles of machine learning techniques in computer-assisted intervention for brain disease diagnosis. Of various machine-learning techniques, sparse regression models have proved their effectiveness in handling high-dimensional data but with a small number of training samples, especially in medical problems. In the meantime, deep learning methods have been making great successes by outperforming the state-of-the-art performances in various applications. In this paper, we propose a novel framework that combines the two conceptually different methods of sparse regression and deep learning for Alzheimer's disease/mild cognitive impairment diagnosis and prognosis. Specifically, we first train multiple sparse regression models, each of which is trained with different values of a regularization control parameter. Thus, our multiple sparse regression models potentially select different feature subsets from the original feature set; thereby they have different powers to predict the response values, i.e., clinical label and clinical scores in our work. By regarding the response values from our sparse regression models as target-level representations, we then build a deep convolutional neural network for clinical decision making, which thus we call 'Deep Ensemble Sparse Regression Network.' To our best knowledge, this is the first work that combines sparse regression models with deep neural network. In our experiments with the ADNI cohort, we validated the effectiveness of the proposed method by achieving the highest diagnostic accuracies in three classification tasks. We also rigorously analyzed our results and compared with the previous studies on the ADNI cohort in the literature. (C) 2017 Elsevier B.V. All rights reserved.","Authors":"Suk, HI (Suk, Heung-Il) ; Lee, SW (Lee, Seong-Whan) ; Shen, DG (Shen, Dinggang) Group Author(s): Alzheimer's Dis Neuroimaging Initi Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Suk, Heung-Il http://orcid.org/0000-0001-7019-8962","Title":"Deep ensemble learning of sparse regression models for brain disease diagnosis"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397551500009 PubMed ID: 28171807 ISSN: 1361-8415 eISSN: 1361-8423","Keywords":"Mammograms; Masses; Detection; Segmentation; Classification; Deep learning; Bayesian optimisation; Transfer learning; Structured output learning KeyWords Plus:COMPUTER-AIDED DETECTION; LEVEL SET SEGMENTATION; NEURAL-NETWORKS; BREAST-CANCER; DIGITAL MAMMOGRAMS; ACTIVE CONTOURS; DIAGNOSIS CAD; INFORMATION; ALGORITHM; FEATURES","Categories":"Computer Science; Engineering; Radiology, Nuclear Medicine & Medical Imaging Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging","Journal Information":"MEDICAL IMAGE ANALYSIS Volume: 37 Pages: 114-128 DOI: 10.1016/j.media.2017.01.009 Published: APR 2017","Abstract":"We present an integrated methodology for detecting, segmenting and classifying breast masses from mammograms with minimal user intervention. This is a long standing problem due to low signal-tonoise ratio in the visualisation of breast masses, combined with their large variability in terms of shape, size, appearance and location. We break the problem down into three stages: mass detection, mass segmentation, and mass classification. For the detection, we propose a cascade of deep learning methods to select hypotheses that are refined based on Bayesian optimisation. For the segmentation, we propose the use of deep structured output learning that is subsequently refined by a level set method. Finally, for the classification, we propose the use of a deep learning classifier, which is pre-trained with a regression to hand-crafted feature values and fine-tuned based on the annotations of the breast mass classification dataset. We test our proposed system on the publicly available INbreast dataset and compare the results with the current state-of-the-art methodologies. This evaluation shows that our system detects 90% of masses at 1 false positive per image, has a segmentation accuracy of around 0.85 (Dice index) on the correctly detected masses, and overall classifies masses as malignant or benign with sensitivity (Se) of 0.98 and specificity (Sp) of 0.7. (C) 2017 Elsevier B.V. All rights reserved.","Authors":"Dhungel, N (Dhungel, Neeraj) ; Carneiro, G (Carneiro, Gustavo) ; Bradley, AP (Bradley, Andrew P.)","Title":"A deep learning approach for the analysis of masses in mammograms with minimal user intervention"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397959900003 PubMed ID: 28157556 ISSN: 0893-6080 eISSN: 1879-2782","Keywords":"Semantic clustering; Neural networks; Short text; Unsupervised learning","Categories":"Computer Science; Neurosciences & Neurology Web of Science Categories:Computer Science, Artificial Intelligence; Neurosciences","Journal Information":"NEURAL NETWORKS Volume: 88 Pages: 22-31 DOI: 10.1016/j.neunet.2016.12.008 Published: APR 2017","Abstract":"Short text clustering is a challenging problem due to its sparseness of text representation. Herewepropose a flexible Self-Taught Convolutional neural network framework for Short Text Clustering (dubbed STC2), which can flexibly and successfully incorporate more useful semantic features and learn non-biased deep text representation in an unsupervised manner. In our framework, the original raw text features are firstly embedded into compact binary codes by using one existing unsupervised dimensionality reduction method. Then, word embeddings are explored and fed into convolutional neural networks to learn deep feature representations, meanwhile the output units are used to fit the pre-trained binary codes in the training process. Finally, we get the optimal clusters by employing K-means to cluster the learned representations. Extensive experimental results demonstrate that the proposed framework is effective, flexible and outperform several popular clustering methods when tested on three public short text datasets. (C) 2017 Elsevier Ltd. All rights reserved.","Authors":"Xu, JM (Xu, Jiaming) ; Xu, B (Xu, Bo) ; Wang, P (Wang, Peng) ; Zheng, SC (Zheng, Suncong) ; Tian, GH (Tian, Guanhua) ; Zhao, J (Zhao, Jun) ; Xu, B (Xu, Bo)","Title":"Self-Taught convolutional neural networks for short text clustering"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397959900007 ISSN: 0893-6080 eISSN: 1879-2782","Keywords":"Parkinsonian state; Beta oscillation; Basal ganglia; Cortical input; Neural mass model KeyWords Plus:DEEP BRAIN-STIMULATION; GLOBUS PALLIDUS NETWORK; FIELD POTENTIAL ACTIVITY; BASAL GANGLIA; SUBTHALAMIC NUCLEUS; SUBTHALAMOPALLIDAL NETWORK; COMPUTATIONAL MODEL; MATHEMATICAL-MODEL; DISEASE; GENERATION","Categories":"Computer Science; Neurosciences & Neurology Web of Science Categories:Computer Science, Artificial Intelligence; Neurosciences","Journal Information":"NEURAL NETWORKS Volume: 88 Pages: 65-73 DOI: 10.1016/j.neunet.2017.01.011 Published: APR 2017","Abstract":"In Parkinson's disease, the enhanced beta rhythm is closely associated with akinesia/bradykinesia and rigidity. An increase in beta oscillations (12-35 Hz) within the basal ganglia (BG) nuclei does not proliferate throughout the cortico-basal ganglia loop in uniform fashion; rather it can be subdivided into two distinct frequency bands, i.e. the lower beta (12-20 Hz) and upper beta (21-35 Hz). A computational model of the excitatory and inhibitory neural network that focuses on the population properties is proposed to explore the mechanism underlying the pathological beta oscillations. Simulation results show several findings. The upper beta frequency in the BG originates from a high frequency cortical beta, while the emergence of exaggerated lower beta frequency in the BG depends greatly on the enhanced excitation of a reciprocal network consisting of the globus pallidus externus (GPe) and the subthalamic nucleus (STN). There is also a transition mechanism between the upper and lower beta oscillatory activities, and we explore the impact of self-inhibition within the GPe on the relationship between the upper beta and lower beta oscillations. It is shown that increased self-inhibition within the GPe contributes to increased upper beta oscillations driven by the cortical rhythm, while decrease in the self-inhibition within the GPe facilitates an enhancement of the lower beta oscillations induced by the increased excitability of the BG. This work provides an analysis for understanding the mechanism underlying pathological synchronization in neurological diseases. (C) 2017 Elsevier Ltd. All rights reserved.","Authors":"Liu, C (Liu, Chen) ; Zhu, YL (Zhu, Yulin) ; Liu, F (Liu, Fei) ; Wang, J (Wang, Jiang) ; Li, HY (Li, Huiyan) ; Deng, B (Deng, Bin) ; Fietkiewicz, C (Fietkiewicz, Chris) ; Loparo, KA (Loparo, Kenneth A.)","Title":"Neural mass models describing possible origin of the excessive beta oscillations correlated with Parkinsonian state"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397959900010 PubMed ID: 28232260 ISSN: 0893-6080 eISSN: 1879-2782","Keywords":"Recurrent neural networks; Computer vision; Object class-segmentation","Categories":"Computer Science; Neurosciences & Neurology Web of Science Categories:Computer Science, Artificial Intelligence; Neurosciences","Journal Information":"NEURAL NETWORKS Volume: 88 Pages: 105-113 DOI: 10.1016/j.neunet.2017.01.003 Published: APR 2017","Abstract":"Object class segmentation is a computer vision task which requires labeling each pixel of an image with the class of the object it belongs to. Deep convolutional neural networks (DNN) are able to learn and take advantage of local spatial correlations required for this task. They are, however, restricted by their small, fixed-sized filters, which limits their ability to learn long-range dependencies. Recurrent Neural Networks (RNN), on the other hand, do not suffer from this restriction. Their iterative interpretation allows them to model long-range dependencies by propagating activity. This property is especially useful when labeling video sequences, where both spatial and temporal long-range dependencies occur. In this work, a novel RNN architecture for object class segmentation is presented. We investigate several ways to train such a network. We evaluate our models on the challenging NYU Depth v2 dataset for object class segmentation and obtain competitive results. (C) 2017 Elsevier Ltd. All rights reserved.","Authors":"Pavel, MS (Pavel, Mircea Serban) ; Schulz, H (Schulz, Hannes) ; Behnke, S (Behnke, Sven)","Title":"Object class segmentation of RGB-D video using recurrent convolutional neural networks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397906400009 ISSN: 0167-8655 eISSN: 1872-7344","Keywords":"Online handwritten Chinese character; recognition; Deep convolutional neural network; Spatial stochastic max-pooling; Character distortion; Path signature KeyWords Plus:RECOGNITION","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"PATTERN RECOGNITION LETTERS Volume: 89 Pages: 60-66 DOI: 10.1016/j.patrec.2017.02.011 Published: APR 1 2017","Abstract":"This paper presents an investigation of several techniques that increase the accuracy of online handwritten Chinese character recognition (HCCR). We propose a new training strategy named DropDistortion to train a deep convolutional neural network (DCNN) with distorted samples. DropDistortion gradually lowers the degree of character distortion during training, which allows the DCNN to better generalize. Path signature is used to extract effective features for online characters. Further improvement is achieved by employing spatial stochastic max-pooling as a method of feature map distortion and model averaging. Experiments were carried out on three publicly available datasets, namely CASIA-OLHWDB 1.0, CASIA-OLHWDB 1.1, and the ICDAR2013 online HCCR competition dataset. The proposed techniques yield state-of-the-art recognition accuracies of 97.67%, 97.30%, and 97.99%, respectively. (C) 2017 Elsevier B.V. All rights reserved.","Authors":"Lai, SX (Lai, Songxuan) ; Jin, LW (Jin, Lianwen) ; Yang, WX (Yang, Weixin)","Title":"Toward high-performance online HCCR: A CNN approach with DropDistortion, path signature and spatial stochastic max-pooling"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000398645000012 ISSN: 0034-4257 eISSN: 1879-0704","Keywords":"Passive microwave remote sensing; Snow water equivalent; Retrieval algorithm KeyWords Plus:ARTIFICIAL NEURAL-NETWORK; RADIATIVE-TRANSFER THEORY; EMISSION MODEL; DRY-SNOW; LAYERED SNOWPACKS; DEPTH RETRIEVAL; RADIOMETER DATA; SCATTERING; SOIL; ASSIMILATION","Categories":"Environmental Sciences & Ecology; Remote Sensing; Imaging Science & Photographic Technology Web of Science Categories:Environmental Sciences; Remote Sensing; Imaging Science & Photographic Technology","Journal Information":"REMOTE SENSING OF ENVIRONMENT Volume: 192 Pages: 150-165 DOI: 10.1016/kse.2017.02.006 Published: APR 2017","Abstract":"Recent applications of passive microwave remote sensing techniques to estimate snow water equivalent (SWE) increasingly rely on the comprehension of microwave emission theories, instead of traditional empirical fitting approaches. In this study, an advanced SWE retrieval algorithm based on the Markov Chain Monte Carlo method was developed. This method samples the posterior multiple-layer snow properties according to the likelihood of the brightness temperature (TB) simulation with the actual TB observations. The Microwave Emission Model of Layered Snowpacks with improved Born approximation (MEMLS-IBA) was used as the observation model. Using a globally applicable method to produce prior estimates of snow properties, the retrieval approach is called the Bayesian Algorithm for SWE Estimation with Passive Microwave measurements (BASE-PM), and was applied on 48 snowpits at Sodanlcyla, Finland; Churchill, Canada and Colorado, US. The result shows that the root mean squared (RMS) error of the retrieved SWE is 42.7 mm excluding two outliers, and is 30.8 mm if the outliers as well as six deep snowpits from Colorado are excluded. This accuracy approximately meets the 30-mm requirement of Integrated Global Observing Strategy for shallow snow. The poor performance for the outlier and deep snowpits is explained. Additional experiments using more accurate priors show that SWE retrieval accuracy can be improved with local snowcover knowledge, e.g. if historical snowpit measurements or snow process model simulations are available. (C) 2017 Elsevier Inc. All rights reserved.","Authors":"Pan, JM (Pan, Jinmei) ; Durand, MT (Durand, Michael T.) ; Vander Jagt, BJ (Vander Jagt, Benjamin J.) ; Liu, DS (Liu, Desheng)","Title":"Application of a Markov Chain Monte Carlo algorithm for snow water equivalent retrieval from passive microwave measurements"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000398009600006 ISSN: 0167-6393 eISSN: 1872-7182","Keywords":"Text-dependent speaker verification; DNN Posteriors; Dynamic time warping KeyWords Plus:GAUSSIAN MIXTURE-MODELS; RECOGNITION","Categories":"Acoustics; Computer Science Web of Science Categories:Acoustics; Computer Science, Interdisciplinary Applications","Journal Information":"SPEECH COMMUNICATION Volume: 88 Pages: 96-105 DOI: 10.1016/j.specom.2017.01.009 Published: APR 2017","Abstract":"In the last decade, i-vector and Joint Factor Analysis (JFA) approaches to speaker modeling have become ubiquitous in the area of automatic speaker recognition. Both of these techniques involve the computation of posterior probabilities, using either Gaussian Mixture Models (GMM) or Deep Neural Networks (DNN), as a prior step to estimating i-vectors or speaker factors. GMMs focus on implicitly modeling phonetic information of acoustic features while DNNs focus on explicitly modeling phonetic/linguistic units. For text-dependent speaker verification, DNN-based systems have considerably outperformed GMM for fixed-phrase tasks. However, both approaches ignore phone sequence information. In this paper, we aim at exploiting this information by using Dynamic Time Warping (DTW) with speaker-informative features. These features are obtained from i-vector models extracted over short speech segments, also called online i-vectors. Probabilistic Linear Discriminant Analysis (PLDA) is further used to project online i-vectors onto a speaker-discriminative subspace. The proposed DTW approach obtained at least 74% relative improvement in equal error rate on the RSR corpus over other state-of-the-art approaches, including i-vector and JFA. (C) 2017 Elsevier B.V. All rights reserved.","Authors":"Dey, S (Dey, Subhadeep) ; Motlicek, P (Motlicek, Petr) ; Madikeri, S (Madikeri, Srikanth) ; Ferras, M (Ferras, Marc)","Title":"Template-matching for text-dependent speaker verification"}, {"Document Information":"Document Type:Review Language:English Accession Number: WOS:000395154600002 ISSN: 1134-3060 eISSN: 1886-1784","Categories":"Computer Science; Engineering; Mathematics Web of Science Categories:Computer Science, Interdisciplinary Applications; Engineering, Multidisciplinary; Mathematics, Interdisciplinary Applications","Journal Information":"ARCHIVES OF COMPUTATIONAL METHODS IN ENGINEERING Volume: 24 Issue: 2 Pages: 251-280 DOI: 10.1007/s11831-016-9166-3 Published: APR 2017","Abstract":"Ranque-Hilsch vortex tube is a simple devise with no moving parts which could generate cold and hot air/gas streams simultaneously with compressed air/gas as a working fluid. The energy and flow separation in a vortex tube is highly depends on factors like nozzle shape, nozzle number, diameter and length of the vortex tube, inlet pressure, control valve, diaphragm hole size and cold mass fraction. As the energy separation and flow patterns in a vortex tube are highly complex and were not explained successfully by any researcher, a computational study of vortex tube flow and energy separation will give a better understanding about the physics and mechanism involved. Many researchers conducted computational fluid dynamic analysis of the vortex to have a deep insight about the process of flow separation. In this paper computational analysis of vortex by many researchers were presented along with the results obtained and suggestions to improve the performance of the vortex tube. Researchers considered Turbulence models which predict the performance precisely were discussed in the present paper. Researchers considered turbulence models like LES, k-epsilon, k-omega and RMS to predict the energy separation in vortex tube. Some researchers considered artificial neural networks (ANN) and Taguchi methods for their analysis. Comparison of the predictions with simulation results were also presented to give a clear idea for the reader about the CFD models prediction capabilities.","Authors":"Sharma, TK (Sharma, T. Karthikeya) ; Rao, GAP (Rao, G. Amba Prasad) ; Murthy, KM (Murthy, K. Madhu)","Title":"Numerical Analysis of a Vortex Tube: A Review"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395495200011 ISSN: 1051-2004 eISSN: 1095-4333","Keywords":"Acoustic event detection; Information bottleneck principle; Audio signal processing KeyWords Plus:DEEP NEURAL-NETWORKS; CLASSIFICATION; DIARIZATION; RECOGNITION; DISTANCE","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"DIGITAL SIGNAL PROCESSING Volume: 63 Pages: 123-134 DOI: 10.1016/j.dsp.2016.12.012 Published: APR 2017","Abstract":"An unsupervised approach based on Information Bottleneck (IB) principle is proposed for detecting acoustic events from audio streams. In this paper, the IB principle is first concisely presented, and then the practical issues related to the application of IB principle to acoustic event detection are described in detail, including definitions of various variables, criterion for determining the number of acoustic events, tradeoff between amount of information preserved and compression of the initial representation, and detection steps. Further, we compare the proposed approach with both unsupervised and supervised approaches on four different types of audio files. Experimental results show that the proposed approach obtains lower detection errors and higher running speed compared to two state-of-the-art unsupervised approaches, and is little inferior to the state-of-the-art supervised approach in terms of both detection errors and runtime. The advantage of the proposed unsupervised approach over the supervised approach is that it does not need to pre-train classifiers and pre-know any prior information about audio streams. (C) 2017 Elsevier Inc. All rights reserved.","Authors":"Li, YX (Li, Yanxiong) ; Wang, Q (Wang, Qin) ; Li, XK (Li, Xianku) ; Zhang, X (Zhang, Xue) ; Zhang, YH (Zhang, Yuhan) ; Chen, AW (Chen, Aiwu) ; He, QH (He, Qianhua) ; Huang, Q (Huang, Qian)","Title":"Unsupervised detection of acoustic events using information bottleneck principle"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397717600004 ISSN: 0162-8828 eISSN: 1939-3539","Keywords":"Image captioning; recurrent neural network; sequence-to-sequence; language model","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE Volume: 39 Issue: 4 Pages: 652-663 Published: APR 2017","Abstract":"Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. Finally, given the recent surge of interest in this task, a competition was organized in 2015 using the newly released COCO dataset. We describe and analyze the various improvements we applied to our own baseline and show the resulting performance in the competition, which we won ex-aequo with a team from Microsoft Research.","Authors":"Vinyals, O (Vinyals, Oriol) ; Toshev, A (Toshev, Alexander) ; Bengio, S (Bengio, Samy) ; Erhan, D (Erhan, Dumitru)","Title":"Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397717600005 PubMed ID: 27514036 ISSN: 0162-8828 eISSN: 1939-3539","Keywords":"Image captioning; deep neural networks; visual-semantic embeddings; recurrent neural network; language model","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE Volume: 39 Issue: 4 Pages: 664-676 DOI: 10.1109/TPAMI.2016.2598339 Published: APR 2017","Abstract":"We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks (RNN) over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions outperform retrieval baselines on both full images and on a new dataset of region-level annotations. Finally, we conduct large-scale analysis of our RNN language model on the Visual Genome dataset of 4.1 million captions and highlight the differences between image and region-level caption statistics.","Authors":"Karpathy, A (Karpathy, Andrej) ; Li, FF (Li Fei-Fei)","Title":"Deep Visual-Semantic Alignments for Generating Image Descriptions"}, {"Categories":"Urology & Nephrology Web of Science Categories:Urology & Nephrology","Journal Information":"JOURNAL OF UROLOGY Volume: 197 Issue: 4 Pages: E209-E209 Supplement: S Meeting Abstract: PD11-08 Published: APR 2017","Abstract":"Document Information Document Type:Meeting Language:English Accession Number: WOS:000398276600470 ISSN: 0022-5347 eISSN: 1527-3792","Authors":"Ishioka, J (Ishioka, Junichiro); Matsuoka, Y (Matsuoka, Yoh); Itoh, M (Itoh, Masaya); Inoue, M (Inoue, Masaharu); Kijima, T (Kijima, Toshiki); Yoshida, S (Yoshida, Soichiro); Yokoyama, M (Yokoyama, Minato); Saito, K (Saito, Kazutaka); Kihara, K (Kihara, Kazunori); Fujii, Y (Fujii, Yasuhisa); Tanaka, H (Tanaka, Hiroshi); Kimura, T (Kimura, Tomo)...More...Less","Title":"COMPUTER-AIDED DIAGNOSIS OF PROSTATE CANCER USING A DEEP NEURAL NETWORKS ALGORITHM IN PRE-BIOPSY MULTIPARAMETRIC MAGNETIC RESONANCE IMAGING"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395952200015 ISSN: 0950-7051 eISSN: 1872-7409","Keywords":"Small bowel motility; Cine-MRI; Deep neural networks; Fully convolutional networks; LSTM KeyWords Plus:CINE-MRI","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"KNOWLEDGE-BASED SYSTEMS Volume: 121 Pages: 163-172 DOI: 10.1016/j.knosys.2017.01.023 Published: APR 1 2017","Abstract":"Assessment of small bowel's motility plays an important role in the diagnosis of small bowel disease. Conventional assessment methods rely on hand-designed features or manual guide-line drawing, which results in difficult modeling and high time consumption, thus they are still inefficient and impractical for clinical uses. With the help of deep neural networks, we introduced a semi-automated approach, replacing hand-designed features with automatic feature extraction, and an automated approach, eliminating manual guide-line drawing, to assess small bowel motility by automatically marking cross-sectional diameters on small bowel images, measuring temporal fluctuation of diameter lengths, and evaluating contraction frequency. Experiment results show that proposed methods could estimate small bowel contraction frequency correctly. The difference between predicted diameter lengths and one manually labeled is within reasonable range, and estimated frequency is close to the groundtruth. Therefore, proposed methods can be utilized for diagnosis of small bowel disease, which will assist radiologist in decision-making. (C) 2017 Elsevier B.V. All rights reserved.","Authors":"Pei, MQ (Pei, Mengqi) ; Wu, X (Wu, Xing) ; Guo, YK (Guo, Yike) ; Fujita, H (Fujita, Hamido)","Title":"Small bowel motility assessment based on fully convolutional networks and long short-term memory"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000398284600022 ISSN: 0035-8711 eISSN: 1365-2966","Keywords":"methods: statistical; surveys; galaxies: distances and redshifts KeyWords Plus:DARK ENERGY SURVEY; DIGITAL SKY SURVEY; BROAD-BAND PHOTOMETRY; DEEP FIELD; NEURAL-NETWORKS; GALAXY-EVOLUTION; NEXT-GENERATION; SURVEY DESIGN; DATA RELEASE; STRIPE 82","Categories":"Astronomy & Astrophysics Web of Science Categories:Astronomy & Astrophysics","Journal Information":"MONTHLY NOTICES OF THE ROYAL ASTRONOMICAL SOCIETY Volume: 466 Issue: 2 Pages: 1582-1596 DOI: 10.1093/mnras/stw3151 Published: APR 2017","Abstract":"We present a new training set for estimating empirical photometric redshifts of galaxies, which was created as part of the 2-degree Field Lensing Survey project. This training set is located in a similar to 700 deg(2) area of the Kilo-Degree-Survey South field and is randomly selected and nearly complete at r < 19.5. We investigate the photometric redshift performance obtained with ugriz photometry from VST-ATLAS and W1/W2 fromWISE, based on several empirical and template methods. The best redshift errors are obtained with kernel-density estimation (KDE), as are the lowest biases, which are consistent with zero within statistical noise. The 68th percentiles of the redshift scatter for magnitude-limited samples at r < (15.5, 17.5, 19.5) are (0.014, 0.017, 0.028). In this magnitude range, there are no known ambiguities in the colour-redshift map, consistent with a small rate of redshift outliers. In the fainter regime, the KDE method produces p(z) estimates per galaxy that represent unbiased and accurate redshift frequency expectations. The p(z) sum over any subsample is consistent with the true redshift frequency plus Poisson noise. Further improvements in redshift precision at r < 20 would mostly be expected from filter sets with narrower passbands to increase the sensitivity of colours to small changes in redshift.","Authors":"Wolf, C (Wolf, C.) ; Johnson, AS (Johnson, A. S.) ; Bilicki, M (Bilicki, M.) ; Blake, C (Blake, C.) ; Amon, A (Amon, A.) ; Erben, T (Erben, T.) ; Glazebrook, K (Glazebrook, K.) ; Heymans, C (Heymans, C.) ; Hildebrandt, H (Hildebrandt, H.) ; Joudaki, S (Joudaki, S.) ; Klaes, D (Klaes, D.) ; Kuijken, K (Kuijken, K.) ; Lidman, C (Lidman, C.) ; Marin, F (Marin, F.) ; Parkinson, D (Parkinson, D.) ; Poole, G (Poole, G.)[ 10 ] ...More...Less","Title":"The 2-degree Field Lensing Survey: photometric redshifts from a large new training sample to r < 19.5"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396672700002 ISSN: 1557-3958 eISSN: 1557-3966","Keywords":"Artificial Neural Network; Deep Neural Network; Feature Extraction; Form Analysis; Identity Mapping; State Trajectory; Table Tennis","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE Volume: 11 Issue: 2 DOI: 10.4018/IJCINI.2017040102 Published: APR-MAR 2017","Abstract":"In deep neural networks, which have been gaining attention in recent years, the features of input images are expressed in a middle layer. Using the information on this feature layer, high performance can be demonstrated in the image recognition field. In the present study, we achieve image recognition, without using convolutional neural networks or sparse coding, through an image feature extraction function obtained when identity mapping learning is applied to sandglass-style feed-forward neural networks. In sports form analysis, for example, a state trajectory is mapped in a low-dimensional feature space based on a consecutive series of actions. Here, we discuss ideas related to image analysis by applying the above method.","Authors":"Hayakawa, Y (Hayakawa, Yoshihiro) ; Oonuma, T (Oonuma, Takanori) ; Kobayashi, H (Kobayashi, Hideyuki) ; Takahashi, A (Takahashi, Akiko) ; Chiba, S (Chiba, Shinji)","Title":"Feature Extraction of Video Using Artificial Neural Network"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397900500021 PubMed ID: 28218899 ISSN: 1548-7091 eISSN: 1548-7105","Categories":"Biochemistry & Molecular Biology Web of Science Categories:Biochemical Research Methods","Journal Information":"NATURE METHODS Volume: 14 Issue: 4 Pages: 403-+ DOI: 10.1038/nmeth.4182 Published: APR 2017","Abstract":"Differentiation alters molecular properties of stem and progenitor cells, leading to changes in their shape and movement characteristics. We present a deep neural network that prospectively predicts lineage choice in differentiating primary hematopoietic progenitors using image patches from brightfield microscopy and cellular movement. Surprisingly, lineage choice can be detected up to three generations before conventional molecular markers are observable. Our approach allows identification of cells with differentially expressed lineage-specifying genes without molecular labeling.","Authors":"Buggenthin, F (Buggenthin, Felix) ; Buettner, F (Buettner, Florian) ; Hoppe, PS (Hoppe, Philipp S.) ; Endele, M (Endele, Max) ; Kroiss, M (Kroiss, Manuel) ; Strasser, M (Strasser, Michael) ; Schwarzfischer, M (Schwarzfischer, Michael) ; Loeffler, D (Loeffler, Dirk) ; Kokkaliaris, KD (Kokkaliaris, Konstantinos D.) ; Hilsenbeck, O (Hilsenbeck, Oliver) ; Schroeder, T (Schroeder, Timm) ; Theis, FJ (Theis, Fabian J.) ; Marr, C (Marr, Carsten) ...More...Less Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Theis, Fabian http://orcid.org/0000-0002-2419-1943","Title":"Prospective identification of hematopoietic lineage choice by deep learning"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397900500028 PubMed ID: 28250467 ISSN: 1548-7091 eISSN: 1548-7105","Categories":"Biochemistry & Molecular Biology Web of Science Categories:Biochemical Research Methods","Journal Information":"NATURE METHODS Volume: 14 Issue: 4 Pages: 435-+ DOI: 10.1038/nmeth.4206 Published: APR 2017","Abstract":"Teravoxel volume electron microscopy data sets from neural tissue can now be acquired in weeks, but data analysis requires years of manual labor. We developed the SyConn framework, which uses deep convolutional neural networks and random forest classifiers to infer a richly annotated synaptic connectivity matrix from manual neurite skeleton reconstructions by automatically identifying mitochondria, synapses and their types, axons, dendrites, spines, myelin, somata and cell types. We tested our approach on serial block-face electron microscopy data sets from zebrafish, mouse and zebra finch, and computed the synaptic wiring of songbird basal ganglia. We found that, for example, basal-ganglia cell types with high firing rates in vivo had higher densities of mitochondria and vesicles and that synapse sizes and quantities scaled systematically, depending on the innervated postsynaptic cell types.","Authors":"Dorkenwald, S (Dorkenwald, Sven) ; Schubert, PJ (Schubert, Philipp J.) ; Killinger, MF (Killinger, Marius F.) ; Urban, G (Urban, Gregor) ; Mikula, S (Mikula, Shawn) ; Svara, F (Svara, Fabian) ; Kornfeld, J (Kornfeld, Joergen)","Title":"Automated synaptic connectivity inference for volume electron microscopy"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396396700017 PubMed ID: 26992191 ISSN: 2168-2267 eISSN: 2168-2275","Keywords":"Convolution; deep learning; denoising autoencoders; unsupervised learning KeyWords Plus:SCENE RECOGNITION; NEURAL-NETWORKS; LEARNING DEEP; AUTOENCODERS; MODEL","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Cybernetics","Journal Information":"IEEE TRANSACTIONS ON CYBERNETICS Volume: 47 Issue: 4 Pages: 1017-1027 DOI: 10.1109/TCYB.2016.2536638 Published: APR 2017","Abstract":"Deep networks have achieved excellent performance in learning representation from visual data. However, the supervised deep models like convolutional neural network require large quantities of labeled data, which are very expensive to obtain. To solve this problem, this paper proposes an unsupervised deep network, called the stacked convolutional denoising auto-encoders, which can map images to hierarchical representations without any label information. The network, optimized by layer-wise training, is constructed by stacking layers of denoising autoencoders in a convolutional way. In each layer, high dimensional feature maps are generated by convolving features of the lower layer with kernels learned by a denoising auto-encoder. The autoencoder is trained on patches extracted from feature maps in the lower layer to learn robust feature detectors. To better train the large network, a layer-wise whitening technique is introduced into the model. Before each convolutional layer, a whitening layer is embedded to sphere the input data. By layers of mapping, raw images are transformed into high-level feature representations which would boost the performance of the subsequent support vector machine classifier. The proposed algorithm is evaluated by extensive experimentations and demonstrates superior classification performance to state-of-the-art unsupervised networks.","Authors":"Du, B (Du, Bo) ; Xiong, W (Xiong, Wei) ; Wu, J (Wu, Jia) ; Zhang, LF (Zhang, Lefei) ; Zhang, LP (Zhang, Liangpei) ; Tao, DC (Tao, Dacheng)","Title":"Stacked Convolutional Denoising Auto-Encoders for Feature Representation"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396394900046 ISSN: 0196-2892 eISSN: 1558-0644","Keywords":"Contractive autoencoder (AE); deep neural network (DNN); supervised classification; synthetic aperture radar (SAR) image KeyWords Plus:SYNTHETIC-APERTURE RADAR; LAND CHANGE; REPRESENTATION; SEGMENTATION; FEATURES; RECOGNITION; INFORMATION; INTEGRATION; ALGORITHM; HISTOGRAM","Categories":"Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science & Photographic Technology Web of Science Categories:Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote Sensing; Imaging Science & Photographic Technology","Journal Information":"IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING Volume: 55 Issue: 4 Pages: 2442-2459 DOI: 10.1109/TGRS.2016.2645226 Published: APR 2017","Abstract":"The classification of a synthetic aperture radar (SAR) image is a significant yet challenging task, due to the presence of speckle noises and the absence of effective feature representation. Inspired by deep learning technology, a novel deep supervised and contractive neural network (DSCNN) for SAR image classification is proposed to overcome these problems. In order to extract spatial features, a multiscale patch-based feature extraction model that consists of gray level-gradient co-occurrence matrix, Gabor, and histogram of oriented gradient descriptors is developed to obtain primitive features from the SAR image. Then, to get discriminative representation of initial features, the DSCNN network that comprises four layers of supervised and contractive autoencoders is proposed to optimize features for classification. The supervised penalty of the DSCNN can capture the relevant information between features and labels, and the contractive restriction aims to enhance the locally invariant and robustness of the encoding representation. Consequently, the DSCNN is able to produce effective representation of sample features and provide superb predictions of the class labels. Moreover, to restrain the influence of speckle noises, a graph-cut-based spatial regularization is adopted after classification to suppress misclassified pixels and smooth the results. Experiments on three SAR data sets demonstrate that the proposed method is able to yield superior classification performance compared with some related approaches.","Authors":"Geng, J (Geng, Jie) ; Wang, HY (Wang, Hongyu) ; Fan, JC (Fan, Jianchao) ; Ma, XR (Ma, Xiaorui)","Title":"Deep Supervised and Contractive Neural Network for SAR Image Classification"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396395500006 ISSN: 1520-9210 eISSN: 1941-0077","Keywords":"Color space; machine learning; skin detection; stacked autoencoders KeyWords Plus:NEURAL-NETWORK; CLASSIFICATION; RECOGNITION; MODELS","Categories":"Computer Science; Telecommunications Web of Science Categories:Computer Science, Information Systems; Computer Science, Software Engineering; Telecommunications","Journal Information":"IEEE TRANSACTIONS ON MULTIMEDIA Volume: 16 Issue: 4 Pages: 740-749 DOI: 10.1109/TMM.2016.2638204 Published: APR 2017","Abstract":"A good skin detector that is capable of capturing skin tones under different conditions is important for human-machine interaction applications. In a general situation, skin detectors, such as skin probability maps or Gaussian mixture models, achieve acceptable skin segmentation results. However, the false positive rate increases significantly when the skin tones are in shadow or when skin-like background objects are under similar illumination. In this paper, we propose a novel skin feature learning algorithm based on stacked autoencoders, which are deep neural networks. To overcome the problems encountered in skin segmentation that are caused by different ethnicities and varying illumination conditions, the stacked autoencoders are utilized to learn more discriminative representations of the skin area in both the RGB color space and the HSV color space. Unlike traditional machine learning methods, instead of predicting each pixel individually, our algorithm utilizes blocks to learn the representations and detect the skin areas. The algorithm exploits the learning ability of deep neural networks to learn high-level representations of skin tones. Experiments on test images show that the proposed algorithm achieves acceptable results on several publicly available data sets. To reduce the difficulty of detecting skin pixels in these data sets, the ground truths of these data sets are commonly focused on foreground skin area detection. Our skin detector is also able to detect background areas, as shown in our experiments.","Authors":"Lei, Y (Lei, You) ; Yuan, W (Yuan, Wang) ; Wang, HP (Wang, Hongpeng) ; You, WH (You Wenhu) ; Bo, W (Bo, Wu)","Title":"A Skin Segmentation Algorithm Based on Stacked Autoencoders"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396395500017 ISSN: 1520-9210 eISSN: 1941-0077","Keywords":"Convolutional neural network; indoor localization; magnetic field; particle filter; visual image KeyWords Plus:NEURAL-NETWORKS; LOCATION; RECOGNITION; PERFORMANCE; NAVIGATION; TRACKING; VISION; DEVICE; SYSTEM","Categories":"Computer Science; Telecommunications Web of Science Categories:Computer Science, Information Systems; Computer Science, Software Engineering; Telecommunications","Journal Information":"IEEE TRANSACTIONS ON MULTIMEDIA Volume: 16 Issue: 4 Pages: 874-888 DOI: 10.1109/TMM.2016.2636750 Published: APR 2017","Abstract":"Accurate and infrastructure-free indoor positioning can be very useful in a variety of applications. However, most existing approaches (e.g., WiFi and infrared-based methods) for indoor localization heavily rely on infrastructure, which is neither scalable nor pervasively available. In this paper, we propose a novel indoor localization and tracking approach, termed VMag, that does not require any infrastructure assistance. The user can be localized while simply holding a smartphone. To the best of our knowledge, the proposed method is the first exploration of fusing geomagnetic and visual sensing for indoor localization. More specifically, we conduct an in-depth study on both the advantageous properties and the challenges in leveraging the geomagnetic field and visual images for indoor localization. Based on these studies, we design a context-aware particle filtering framework to track the user with the goal of maximizing the positioning accuracy. We also introduce a neural-network-based method to extract deep features for the purpose of indoor positioning. We have conducted extensive experiments on four different indoor settings including a laboratory, a garage, a canteen, and an office building. Experimental results demonstrate the superior performance of VMag over the state of the art with these four indoor settings.","Authors":"Liu, ZG (Liu, Zhenguang) ; Zhang, LM (Zhang, Luming) ; Liu, Q (Liu, Qi) ; Yin, YF (Yin, Yifang) ; Cheng, L (Cheng, Li) ; Zimmermann, R (Zimmermann, Roger)","Title":"Fusion of Magnetic and Visual Sensors for Indoor Localization: Infrastructure-Free and More Effective"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396381300010 PubMed ID: 27723607 ISSN: 2162-237X eISSN: 2162-2388","Keywords":"Brain data classification; brain data modeling; brain data visualization; evolving spatiotemporal data machines (eSTDMs); functional magnetic resonance imaging (fMRI); NeuCube; neuro information processing; spatiotemporal brain data (STBD); spiking neural networks (SNNs) KeyWords Plus:BRAIN ACTIVITY; EEG DATA; COMMUNICATION; METHODOLOGY; COMPUTATION; INTERFACE; NEURONS","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Volume: 28 Issue: 4 Pages: 887-899 DOI: 10.1109/TNNLS.2016.2612890 Published: APR 2017","Abstract":"This paper introduces a new methodology for dynamic learning, visualization, and classification of functional magnetic resonance imaging (fMRI) as spatiotemporal brain data. The method is based on an evolving spatiotemporal data machine of evolving spiking neural networks (SNNs) exemplified by the NeuCube architecture [1]. The method consists of several steps: mapping spatial coordinates of fMRI data into a 3-D SNN cube (SNNc) that represents a brain template; input data transformation into trains of spikes; deep, unsupervised learning in the 3-D SNNc of spatiotemporal patterns from data; supervised learning in an evolving SNN classifier; parameter optimization; and 3-D visualization and model interpretation. Two benchmark case study problems and data are used to illustrate the proposed methodology-fMRI data collected from subjects when reading affirmative or negative sentences and another one-on reading a sentence or seeing a picture. The learned connections in the SNNc represent dynamic spatiotemporal relationships derived from the fMRI data. They can reveal new information about the brain functions under different conditions. The proposed methodology allows for the first time to analyze dynamic functional and structural connectivity of a learned SNN model from fMRI data. This can be used for a better understanding of brain activities and also for online generation of appropriate neurofeedback to subjects for improved brain functions. For example, in this paper, tracing the 3-D SNN model connectivity enabled us for the first time to capture prominent brain functional pathways evoked in language comprehension. We found stronger spatiotemporal interaction between left dorsolateral prefrontal cortex and left temporal while reading a negated sentence. This observation is obviously distinguishable from the patterns generated by either reading affirmative sentences or seeing pictures. The proposed NeuCube-based methodology offers also a superior classification accuracy when compared with traditional AI and statistical methods. The created NeuCube-based models of fMRI data are directly and efficiently implementable on high performance and low energy consumption neuromorphic platforms for real-time applications.","Authors":"Kasabov, NK (Kasabov, Nikola K.) ; Doborjeh, MG (Doborjeh, Maryam Gholami) ; Doborjeh, ZG (Doborjeh, Zohreh Gholami) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Kasabov, Nikola http://orcid.org/0000-0003-4433-7521","Title":"Mapping, Learning, Visualization, Classification, and Understanding of fMRI Data in the NeuCube Evolving Spatiotemporal Data Machine of Spiking Neural Networks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393245000015 ISSN: 0020-0255 eISSN: 1872-6291","Keywords":"Deep neural networks; RGB-D data; Feature learning; Performance evaluation KeyWords Plus:OBJECT RECOGNITION; NEURAL-NETWORKS","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"INFORMATION SCIENCES Volume: 385 Pages: 266-283 DOI: 10.1016/j.ins.2017.01.013 Published: APR 2017","Abstract":"Deep Neural Networks for image/video classification have obtained much success in various computer vision applications. Existing deep learning algorithms are widely used on RGB images or video data. Meanwhile, with the development of low-cost RGB-D sensors (such as Microsoft Kinect and Xtion Pro-Live), high-quality RGB-D data can be easily acquired and used to enhance computer vision algorithms [14]. It would be interesting to investigate how deep learning can be employed for extracting and fusing features from RGB-D data. In this paper, after briefly reviewing the basic concepts of RGB-D information and four prevalent deep learning models (i.e., Deep Belief Networks (DBNs), Stacked Denoising Auto-Encoders (SDAE), Convolutional Neural Networks (CNNs) and Long Short Term Memory (LSTM) Neural Networks), we conduct extensive experiments on five popular RGB-D datasets including three image datasets and two video datasets. We then present a detailed analysis about the comparison between the learned feature representations from the four deep learning models. In addition, a few suggestions on how to adjust hyper parameters for learning deep neural networks are made in this paper. According to the extensive experimental results, we believe that this evaluation will provide insights and a deeper understanding of different deep learning algorithms for RGB-D feature extraction and fusion. (C) 2017 Elsevier Inc. All rights reserved.","Authors":"Shao, L (Shao, Ling) ; Cai, ZY (Cai, Ziyun) ; Liu, L (Liu, Li) ; Lu, K (Lu, Ke)","Title":"Performance evaluation of deep feature learning for RGB-D image/video classification"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393245000019 ISSN: 0020-0255 eISSN: 1872-6291","Keywords":"Canonical correlation analysis; Deep learning; Convolutional neural networks; Image classification KeyWords Plus:DEEP NETWORKS; SUPERRESOLUTION; CLASSIFICATION; SEARCH","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"INFORMATION SCIENCES Volume: 385 Pages: 338-352 DOI: 10.1016/j.ins.2017.01.011 Published: APR 2017","Abstract":"In recent years, deep learning has attracted an increasing amount of attention in machine learning and artificial intelligence areas. Currently, many deep learning network-related architectures such as deep neural networks (DNNs), convolutional neural network (CNN), wavelet scattering network (ScatNet) and principal component analysis network (PCANet) have been proposed. The most effective network is PCANet, which has achieved promising performance in image classification, such as for face, object and handwritten digit recognition. PCANet can only handle data that are represented by single-view features. In this paper, we present a canonical correlation analysis network (CCANet) to address image classification, in which images are represented by two-view features. The CCANet learns two-view multistage filter banks by a canonical correlation analysis (CCA) method and constructs a cascaded convolutional deep network. Then, we incorporate filters with binaryzation and block-wise histogram processes to form the final depth structure. In addition, we introduce a variation of CCANet dubbed RandNet-2-in which the filter banks are randomly generated. Extensive experiments are conducted using the ETH-80, Yale-B, and USPS databases for object classification, face classification and handwritten digits classification, respectively. The experimental results demonstrate that the CCANet algorithm is more effective than PCANet, RandNet-1 and RandNet-2. (C) 2017 Elsevier Inc. All rights reserved.","Authors":"Yang, XH (Yang, Xinghao) ; Liu, WF (Liu, Weifeng) ; Tao, DP (Tao, Dapeng) ; Cheng, J (Cheng, Jun)","Title":"Canonical correlation analysis networks for two-view image recognition"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392768700020 ISSN: 0957-4174 eISSN: 1873-6793","Keywords":"Scene categorization; Random forest; Convolutional neural networks; Feature selection KeyWords Plus:CLASSIFICATION; WORDS","Categories":"Computer Science; Engineering; Operations Research & Management Science Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science","Journal Information":"EXPERT SYSTEMS WITH APPLICATIONS Volume: 71 Pages: 279-287 DOI: 10.1016/j.eswa.2016.10.038 Published: APR 1 2017","Abstract":"Breakthrough performances have been achieved in computer vision by utilizing deep neural networks. In this paper we propose to use random forest to classify image representations obtained by concatenating multiple layers of learned features of deep convolutional neural networks for scene classification. Specifically, we first use deep convolutional neural networks pre-trained on the large-scale image database Places to extract features from scene images. Then, we concatenate multiple layers of features of the deep neural networks as image representations. After that, we use random forest as the classifier for scene classification. Moreover, to reduce feature redundancy in image representations we derived a novel feature selection method for selecting features that are suitable for random forest classification. Extensive experiments are conducted on two benchmark datasets, i.e. MIT-Indoor and UIUC-Sports. Obtained results demonstrated the effectiveness of the proposed method. The contributions of the paper are as follows. First, by extracting multiple layers of deep neural networks, we can explore more information of image contents for determining their categories. Second, we proposed a novel feature selection method that can be used to reduce redundancy in features obtained by deep neural networks for classification based on random forest. In particular, since deep learning methods can be used to augment expert systems by having the systems essentially training themselves, and the proposed framework is general, which can be easily extended to other intelligent systems that utilize deep learning methods, the proposed method provide a potential way for improving performances of other expert and intelligent systems. (C) 2016 Elsevier Ltd. All rights reserved.","Authors":"Bai, S (Bai, Shuang)","Title":"Growing random forest on deep convolutional neural networks for scene categorization"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392682400011 ISSN: 0031-3203 eISSN: 1873-5142","Keywords":"Sparse Coding; Deep Model; Multi-scale; Multi-locality; Image classification KeyWords Plus:ALGORITHM","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"PATTERN RECOGNITION Volume: 64 Pages: 130-140 DOI: 10.1016/j.patcog.2016.10.032 Published: APR 2017","Abstract":"This paper introduces a deep model called Deep Sparse-Coding Network (DeepSCNet) to combine the advantages of Convolutional Neural Network (CNN) and sparse-coding techniques for image feature representation. DeepSCNet consists of four type of basic layers:\" The sparse-coding layer performs generalized linear coding for local patch within the receptive field by replacing the convolution operation in CNN into sparse-coding. The Pooling layer and the Normalization layer perform identical operations as that in CNN. And finally the Map reduction layer reduces CPU/memory consumption by reducing the number of feature maps before stacking with the following layers. These four type of layers can be easily stacked to construct a deep model for image feature learning. The paper further discusses the multi-scale, multi-locality extension to the basic DeepSCNet, and the overall approach is fully unsupervised. Compared to CNN, training DeepSCNet is relatively easier even with training set of moderate size. Experiments show that DeepSCNet can automatically discover highly discriminative feature directly from raw image pixels.","Authors":"Zhang, SZ (Zhang, Shizhou) ; Wang, JJ (Wang, Jinjun) ; Tao, XY (Tao, Xiaoyu) ; Gong, YH (Gong, Yihong) ; Zheng, NN (Zheng, Nanning)","Title":"Constructing Deep Sparse Coding Network for image classification"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392682400016 ISSN: 0031-3203 eISSN: 1873-5142","Keywords":"Object segmentation; Unsupervised discriminant clustering; Graph cut KeyWords Plus:IMAGE SEGMENTATION; GRAPH CUTS","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"PATTERN RECOGNITION Volume: 64 Pages: 202-214 DOI: 10.1016/j.patcog.2016.11.009 Published: APR 2017","Abstract":"Category-specific object segmentation has been a long-standing research topic in pattern recognition. This paper presents an unsupervised discriminant shape (UDS) to address category-specific object segmentation by incorporating the proposed shape prior into an intuitive energy minimization framework. Recently, based on the region proposal methods, deep Convolutional Neural Networks (CNNs) provide access to candidate segments in categories of interest from images. However, the segments obtained from bottom-up proposals tend to undershoot or overshoot objects and are easily classified into one specific class. To address this problem, we propose an unsupervised discriminant projection based clustering algorithm (UDC) to obtain more precise shape prior to guide the segmentation, and the class-specific proposals are clustered based on their projections onto the discriminant projection direction. Based on the set of proposals, we then obtain the prior information of foreground UDS with an easy voting scheme. The derived UDS prior is finally utilized in the subsequent energy minimizing formulation based figure-ground segmentation. We conduct extensive and comprehensive evaluations on the MSRC, Object Discovery, Fashionista and PASCAL-S datasets, demonstrating the effectiveness and robustness of the UDS based segmentation.","Authors":"Dai, LZ (Dai, Lingzheng) ; Yang, J (Yang, Jian) ; Chen, L (Chen, Liang) ; Li, JX (Li, Junxia)","Title":"Category-specific object segmentation via unsupervised discriminant shape"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392682400035 ISSN: 0031-3203 eISSN: 1873-5142","Keywords":"Spatio-temporal object analysis; Vision-based behavior analysis; Intelligent and automated vehicles; Human-centric artificial intelligence; Contextual robotics; Driver perception modeling; Object detection KeyWords Plus:FEATURES; SYSTEMS","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"PATTERN RECOGNITION Volume: 64 Pages: 425-436 DOI: 10.1016/j.patcog.2016.08.029 Published: APR 2017","Abstract":"Understanding intent and relevance of surrounding agents from video is an essential task for many applications in robotics and computer vision. The modeling and evaluation of contextual, spatio-temporal situation awareness is particularly important in the domain of intelligent vehicles, where a robot is required to smoothly navigate in a complex environment while also interacting with humans. In this paper, we address these issues by studying the task of on-road object importance ranking from video. First, human-centric object importance annotations are employed in order to analyze the relevance of a variety of multi-modal cues for the importance prediction task. A deep convolutional neural network model is used for capturing video-based contextual spatial and temporal cues of scene type, driving task, and object properties related to intent. Second, the proposed importance annotations are used for producing novel analysis of error types in image-based object detectors. Specifically, we demonstrate how cost-sensitive training, informed by the object importance annotations, results in improved detection performance on objects of higher importance. This insight is essential for an application where navigation mistakes are safety-critical, and the quality of automation and human robot interaction is key.","Authors":"Ohn-Bar, E (Ohn-Bar, Eshed) ; Trivedi, MM (Trivedi, Mohan Manubhai)","Title":"Are all objects equal? Deep spatio-temporal importance prediction in driving videos"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392682400036 ISSN: 0031-3203 eISSN: 1873-5142","Keywords":"Semantic video segmentation; Deconvolutional neural network; Coarse-to-fine training; Spatio-temporal consistence KeyWords Plus:DATABASE","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"PATTERN RECOGNITION Volume: 64 Pages: 437-445 DOI: 10.1016/j.patcog.2016.09.046 Published: APR 2017","Abstract":"Semantic video segmentation is a challenging task of fine-grained semantic understanding of video data. In this paper, we present a jointly trained deep learning framework to make the best use of spatial and temporal information for semantic video segmentation. Along the spatial dimension, a hierarchically supervised deconvolutional neural network (HDCNN) is proposed to conduct pixel-wise semantic interpretation for single video frames. HDCNN is constructed with convolutional layers in VGG-net and their mirrored deconvolutional structure, where all fully connected layers are removed. And hierarchical classification layers are added to multi scale deconvolutional features to introduce more contextual information for pixel-wise semantic interpretation. Besides, a coarse-to-fine training strategy is adopted to enhance the performance of foreground object segmentation in videos. Along the temporal dimension, we introduce Transition Layers upon the structure of HDCNN to make the pixel-wise label prediction consist with adjacent, pixels across space and time domains. The learning process of the Transition Layers can be implemented as a set of extra convolutional calculations connected with HDCNN. These two parts are jointly trained as a unified deep network in our approach. Thorough evaluations are performed on two challenging video datasets, i.e., CamVid and GATECH. Our approach achieves state-of-the-art performance on both of the two datasets.","Authors":"Wang, YH (Wang, Yuhang) ; Liu, J (Liu, Jing) ; Li, Y (Li, Yong) ; Fu, J (Fu, Jun) ; Xu, M (Xu, Min) ; Lu, HQ (Lu, Hanqing)","Title":"Hierarchically Supervised Deconvolutional Network for Semantic Video Segmentation"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397567300001 PubMed ID: 28424576 ISSN: 1662-453X","Keywords":"defensive behavior; macaque; PTSD; primate; GABA-A; bicuculline KeyWords Plus:SPINOTHALAMIC TRACT NEURONS; AMINO-ACID MICROINJECTION; SUPERIOR COLLICULUS; GREY REGION; HORSERADISH-PEROXIDASE; ELECTRICAL-STIMULATION; CHEMICAL-STIMULATION; RESTRICTED PORTION; TONIC IMMOBILITY; CENTRAL AMYGDALA","Categories":"Neurosciences & Neurology Web of Science Categories:Neurosciences","Journal Information":"FRONTIERS IN NEUROSCIENCE Volume: 11 Article Number: 163 DOI: 10.3389/hina2D17.0010 Published: MAR 29 2017","Abstract":"Rapid and reflexive responses to threats are present across phylogeny. The neural circuitry mediating reflexive defense reactions has been well-characterized in a variety of species, for example, in rodents and cats, the detection of and species-typical response to threats is mediated by a network of structures including the midbrain tectum (deep and intermediate layers of the superior colliculus [DLSC]), periaqueductal gray (RAG), and forebrain structures such as the amygdala and hypothalamus. However, relatively little is known about the functional architecture of defense circuitry in primates. We have previously reported that pharmacological activation of the DLSC evokes locomotor asymmetry, defense-associated vocalizations, cowering behavior, escape responses, and attack of inanimate objects (Holmes et al., 2012: DesJardin et al., 2013: Forcelli et al., 2016). Here, we sought to determine if pharmacological activation of the PAG would induce a similar profile of responses. We activated the PAG in three awake, behaving macaques by microinfusion of GABA-A receptor antagonist, bicuculline methiodide. Activation of PAG evoked defense-associated vocalizations and postural/locomotor asymmetry, but not motor defense responses (e.g., cowering, escape behavior). These data suggest a partial dissociation between the role of the PAG and the DLSC in the defense network of macaques, but a general conservation of the role of PAG in defense responses across species.","Authors":"Forcelli, PA (Forcelli, Patrick A.) ; Waguespack, HF (Waguespack, Hannah F.) ; Malkova, L (Malkova, Ludise)","Title":"Defensive Vocalizations and Motor Asymmetry Triggered by Disinhibition of the Periaqueductal Gray in Non-human Primates"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393250300005 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Reservoir Computing; Echo State Networks; Intrinsic Plasticity; Deep Neural Encoders; Feature dimensionality reduction; Affective computing KeyWords Plus:INTRINSIC PLASTICITY; CLASSIFICATION","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 231 Pages: 28-40 Special Issue: SI DOI: 10.1016/j.neucom.2016.03.108 Published: MAR 29 2017","Abstract":"In this paper we propose a new approach for feature dimensionality reduction based on Reservoir Computing (Echo State Networks). The method is validated with EEG data to identify the common neural signatures based on which the positive and negative valence of human emotions across multiple subjects can be reliably discriminated. The key step in the proposed approach is the Intrinsic Plasticity (IP) adaptation of the reservoir states. Learning Echo State Networks (ESN) with IP maximizes the entropy of the distribution of reservoir vectors given static data as a fixed input, which is supposed to follow Gaussian distribution. The equilibrium reservoir vector is ektracted for each static input vector by iterating updates of the reservoir vector until it converges. Standard classification and clustering models provided with selected combinations of reservoir neurons are ranked based on their discriminate performance. The IP tuned ESNs is more powerful technique to map the high dimenSional input feature vector into a low dimensional representation and improve the emotion valence discrimination compared to classical ESNs and Deep Neural Encoders.","Authors":"Bozhkov, L (Bozhkov, Lachezar) ; Koprinkova-Hristova, P (Koprinkova-Hristova, Petia) ; Georgieva, P (Georgieva, Petia) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Koprinkova-Hristova, Petia http://orcid.org/0000-0002-0447-9667 Georgieva, Petia http://orcid.org/0000-0002-6424-6590","Title":"Reservoir computing for emotion valence discrimination from EEG signals"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397607300081 PubMed ID: 28292907 ISSN: 0027-8424","Keywords":"synaptic consolidation; artificial intelligence; stability plasticity; continual learning; deep learning KeyWords Plus:COMPLEMENTARY LEARNING-SYSTEMS; PREFRONTAL CORTEX; CONNECTIONIST NETWORKS; MEMORY; MODELS; PLASTICITY; AGENTS","Categories":"Science & Technology - Other Topics Web of Science Categories:Multidisciplinary Sciences","Journal Information":"PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA Volume: 114 Issue: 13 Pages: 3521-3526 DOI: 10.1073/pnas.1611835114 Published: MAR 28 2017","Abstract":"The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.","Authors":"Kirkpatricka, J (Kirkpatricka, James) ; Pascanu, R (Pascanu, Razvan) ; Rabinowitz, N (Rabinowitz, Neil) ; Veness, J (Veness, Joel) ; Desjardins, G (Desjardins, Guillaume) ; Rusu, AA (Rusu, Andrei A.) ; Milan, K (Milan, Kieran) ; Quan, J (Quan, John) ; Ramalho, T (Ramalho, Tiago) ; Grabska-Barwinska, A (Grabska-Barwinska, Agnieszka) ; Hassabis, D (Hassabis, Demis) ; Clopath, C (Clopath, Claudia) ; Kumaran, D (Kumaran, Dharshan) ; Hadsell, R (Hadsell, Raia) ...More...Less","Title":"Overcoming catastrophic forgetting in neural networks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396988500001 PubMed ID: 28381998 ISSN: 1662-5218","Keywords":"action recognition; key pose selection; deep learning; neuromorphic architecture; IBM neurosynaptic system KeyWords Plus:MOTION PERCEPTION; LEARNING REPRESENTATIONS; CONVOLUTIONAL NETWORKS; IMPLIED MOTION; HUMAN MOVEMENT; NEURAL MODEL; FEATURES; INTEGRATION; SEQUENCES; MACAQUE","Categories":"Computer Science; Robotics; Neurosciences & Neurology Web of Science Categories:Computer Science, Artificial Intelligence; Robotics; Neurosciences","Journal Information":"FRONTIERS IN NEUROROBOTICS Volume: 11 Article Number: UNSP 13 DOI: 10.3389/fnbot.2017.00013 Published: MAR 22 2017","Abstract":"Intelligent agents, such as robots, have to serve a multitude of autonomous functions. Examples are, e.g., collision avoidance, navigation and route planning, active sensing of its environment, or the interaction and non-verbal communication with people in the extended reach space. Here, we focus on the recognition of the action of a human agent based on a biologically inspired visual architecture of analyzing articulated movements. The proposed processing architecture builds upon coarsely segregated streams of sensory processing along different pathways which separately process form and motion information (Layher et al., 2014). Action recognition is performed in an event-based scheme by identifying representations of characteristic pose configurations (key poses) in an image sequence. In line with perceptual studies, key poses are selected unsupervised utilizing a feature-driven criterion which combines extrema in the motion energy with the horizontal and the vertical extendedness of a body shape. Per class representations of key pose frames are learned using a deep convolutional neural network consisting of 15 convolutional layers. The network is trained using the energy-efficient deep neuromorphic networks (Eedn) framework (Esser et al., 2016), which realizes the mapping of the trained synaptic weights onto the IBM Neurosynaptic System platform (Merolla et al., 2014). After the mapping, the trained network achieves real-time capabilities for processing input streams and classify input images at about 1,000 frames per second while the computational stages only consume about 70 mW of energy (without spike transduction). Particularly regarding mobile robotic systems, a low energy profile might be crucial in a variety of application scenarios. Cross-validation results are reported for two different datasets and compared to state-of-the-art action recognition approaches. The results demonstrate, that (I) the presented approach is on par with other key pose based methods described in the literature, which select key pose frames by optimizing classification accuracy, (II) compared to the training on the full set of frames, representations trained on key pose frames result in a higher confidence in class assignments, and (III) key pose representations show promising generalization capabilities in a cross-dataset evaluation.","Authors":"Layher, G (Layher, Georg) ; Brosch, T (Brosch, Tobias) ; Neumann, H (Neumann, Heiko)","Title":"Real-Time Biologically Inspired Action Recognition from Key Poses Using a Neuromorphic Architecture"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396994200001 PubMed ID: 28327593 ISSN: 2045-2322","Categories":"Science & Technology - Other Topics Web of Science Categories:Multidisciplinary Sciences","Journal Information":"SCIENTIFIC REPORTS Volume: 7 Article Number: 45141 DOI: 10.1038/srep45141 Published: MAR 22 2017","Abstract":"The number of people affected by mental illness is on the increase and with it the burden on health and social care use, as well as the loss of both productivity and quality-adjusted life-years. Natural language processing of electronic health records is increasingly used to study mental health conditions and risk behaviours on a large scale. However, narrative notes written by clinicians do not capture first-hand the patients' own experiences, and only record cross-sectional, professional impressions at the point of care. Social media platforms have become a source of 'in the moment' daily exchange, with topics including well- being and mental health. In this study, we analysed posts from the social media platform Reddit and developed classifiers to recognise and classify posts related to mental illness according to 11 disorder themes. Using a neural network and deep learning approach, we could automatically recognise mental illness-related posts in our balenced dataset with an accuracy of 91.08% and select the correct theme with a weighted average accuracy of 71.37%. We believe that these results are a first step in developing methods to characterise large amounts of user-generated content that could support content curation and targeted interventions.","Authors":"Gkotsis, G (Gkotsis, George) ; Oellrich, A (Oellrich, Anika) ; Velupillai, S (Velupillai, Sumithra) ; Liakata, M (Liakata, Maria) ; Hubbard, TJP (Hubbard, Tim J. P.) ; Dobson, RJB (Dobson, Richard J. B.) ; Dutta, R (Dutta, Rina)","Title":"Characterisation of mental health conditions in social media using Informed Deep Learning"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394061800005 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Convolutional neural networks; GPU; Target detection","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 230 Pages: 48-59 DOI: 10.1016/j.neucom.2016.11.046 Published: MAR 22 2017","Abstract":"Target detection is a hard real-time task for video and image processing. This task has recently been accomplished through the feedforward process of convolutional neural networks (CNN), which is usually accelerated by general-purpose graphic units (GPUs). However, there are two challenges for this task. One is that the running speed remains to be improved. The other is that we probably use a deeper and larger CNN model, but a more sophisticated model may not be trained well due to the shortage of GPU memory. In this paper, we present two scheduling algorithms to solve the aforementioned challenges for improving the system performance holistically. The first one is an efficient image combination algorithm used to accelerate the feedforward process of CNN. The other is a light-memory-cost algorithm used to train an arbitrarily large CNN model for a GPU device with a limited memory. We run our experiments on a GTX980 card and use a CNN model with 8 GB of model parameters, which is larger than the size of the global memory of a GPU. Compared with that of cuDNNv3, a high speedup of 6.97x is obtained in the detection task.","Authors":"Li, SJ (Li, Shijie) ; Dou, Y (Dou, Yong) ; Niu, X (Niu, Xin) ; Lv, Q (Lv, Qi) ; Wang, Q (Wang, Qiang)","Title":"A fast and memory saved GPU acceleration algorithm of convolutional neural networks for target detection"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394061800017 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Face recognition; Data augmentation; Landmark perturbation; Image synthesis; 3D reconstruction KeyWords Plus:IMAGE; PATTERNS; NETWORK; POSE","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 230 Pages: 184-196 DOI: 10.1016/j.neucom.2016.12.025 Published: MAR 22 2017","Abstract":"Recently, Deep Convolution Neural Networks (DCNNs) have shown outstanding performance in face recognition. However, the supervised training process of DCNN requires a large number of labeled samples which are expensive and time consuming to collect. In this paper, we propose five data augmentation methods dedicated to face images, including landmark perturbation and four synthesis methods (hairstyles, glasses, poses, illuminations). The proposed methods effectively enlarge the training dataset, which alleviates the impacts of misalignment, pose variance, illumination changes and partial occlusions, as well as the overfitting during training. The performance of each data augmentation method is tested on the Multi-PIE database. Furthermore, comparison of these methods are conducted on LFW, YTF and IJB-A databases. Experimental results show that our proposed methods can greatly improve the face recognition performance.","Authors":"Lv, JJ (Lv, Jiang-Jing) ; Shao, XH (Shao, Xiao-Hu) ; Huang, JS (Huang, Jia-Shui) ; Zhou, XD (Zhou, Xiang-Dong) ; Zhou, X (Zhou, Xi)","Title":"Data augmentation for face recognition"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394061800025 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Pornography classification; Deep learning and motion information; Optical flow; MPEG motion vectors; Sensitive video classification KeyWords Plus:SPACE-TIME; CLASSIFICATION; REPRESENTATION; RECOGNITION; SYSTEM","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 230 Pages: 279-293 DOI: 10.1016/j.neucom.2016.12.017 Published: MAR 22 2017","Abstract":"Recent literature has explored automated pornographic detection a bold move to replace humans in the tedious task of moderating online content. Unfortunately, on scenes with high skin exposure, such as people sunbathing and wrestling, the state of the art can have many false alarms. This paper is based on the premise that incorporating motion information in the models can alleviate the problem of mapping skin exposure to pornographic content, and advances the bar on automated pornography detection with the use of motion information and deep learning architectures. Deep Learning, especially in the form of Convolutional Neural Networks, have striking results on computer vision, but their potential for pornography detection is yet to be fully explored through the use of motion information. We propose novel ways for combining static (picture) and dynamic (motion) information using optical flow and MPEG motion vectors. We show that both methods provide equivalent accuracies, but that MPEG motion vectors allow a more efficient implementation. The best proposed method yields a classification accuracy of 97.9% an error reduction of 64.4% when compared to the state of the art on a dataset of 800 challenging test cases. Finally, we present and discuss results on a larger, and more challenging, dataset.","Authors":"Perez, M (Perez, Mauricio) ; Avila, S (Avila, Sandra) ; Moreira, D (Moreira, Daniel) ; Moraes, D (Moraes, Daniel) ; Testoni, V (Testoni, Vanessa) ; Valle, E (Valle, Eduardo) ; Goldenstein, S (Goldenstein, Siome) ; Rocha, A (Rocha, Anderson)","Title":"Video pornography detection through deep learning techniques and motion information"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394061800034 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Extreme learning machine; Generalized extreme learning machine autoencoder; Manifold regularization; Deep neural network; Multilayer generalized extreme learning machine autoencoder KeyWords Plus:FACE RECOGNITION; DIMENSIONALITY; CLASSIFICATION","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 230 Pages: 374-381 DOI: 10.1016/j.neucom.2016.12.027 Published: MAR 22 2017","Abstract":"Extreme learning machine (ELM) is an efficient learning algorithm of training single layer feed-forward neural networks (SLFNs). With the development of unsupervised learning in recent years, integrating ELM with autoencoder has become a new perspective for extracting feature using unlabeled data. In this paper, we propose a new variant of extreme learning machine autoencoder (ELM-AE) called generalized extreme learning machine autoencoder (GELM-AE) which adds the manifold regularization to the objective of ELM-AE. Some experiments carried out on real-world data sets show that GELM-AE outperforms some state-of-the-art unsupervised learning algorithms, including k-means, laplacian embedding (LE), spectral clustering (SC) and ELM-AE. Furthermore, we also propose a new deep neural network called multilayer generalized extreme learning machine autoencoder (ML-GELM) by stacking several GELM-AE to detect more abstract representations. The experiments results show that ML-GELM outperforms ELM and many other deep models, such as multilayer ELM autoencoder (ML-ELM), deep belief network (DBN) and stacked autoencoder (SAE). Due to the utilization of ELM, ML-GELM is also faster than DBN and SAE.","Authors":"Sun, K (Sun, Kai) ; Zhang, JS (Zhang, Jiangshe) ; Zhang, CX (Zhang, Chunxia) ; Hu, JY (Hu, Junying)","Title":"Generalized extreme learning machine autoencoder and a new deep neural network"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396927500001 PubMed ID: 28377709 ISSN: 1662-5188","Keywords":"connectionist modeling; unsupervised deep learning; restricted Boltzmann machines; autoencoders; sparseness; space coding; gain modulation; sensorimotor transformations KeyWords Plus:POSTERIOR PARIETAL NEURONS; BAYESIAN-INFERENCE; GENERATIVE MODELS; EYE-MOVEMENTS; CORTEX; REPRESENTATIONS; ATTENTION; COGNITION; BRAIN; TRANSFORMATIONS","Categories":"Mathematical & Computational Biology; Neurosciences & Neurology Web of Science Categories:Mathematical & Computational Biology; Neurosciences","Journal Information":"FRONTIERS IN COMPUTATIONAL NEUROSCIENCE Volume: 11 Article Number: 13 DOI: 10.3389/fncom.2017.00013 Published: MAR 21 2017","Abstract":"The recent \"deep learning revolution\" in artificial neural networks had strong impact and widespread deployment for engineering applications, but the use of deep learning for neurocomputational modeling has been so far limited. In this article we argue that unsupervised deep learning represents an important step forward for improving neurocomputational models of perception and cognition, because it emphasizes the role of generative learning as opposed to discriminative (supervised) learning. As a case study, we present a series of simulations investigating the emergence of neural coding of visual space for sensorimotor transformations. We compare different network architectures commonly used as building blocks for unsupervised deep learning by systematically testing the type of receptive fields and gain modulation developed by the hidden neurons. In particular, we compare Restricted Boltzmann Machines (RBMs), which are stochastic, generative networks with bidirectional connections trained using contrastive divergence, with autoencoders, which are deterministic networks trained using error backpropagation. For both learning architectures we also explore the role of sparse coding, which has been identified as a fundamental principle of neural computation. The unsupervised models are then compared with supervised, feed-forward networks that learn an explicit mapping between different spatial reference frames. Our simulations show that both architectural and learning constraints strongly influenced the emergent coding of visual space in terms of distribution of tuning functions at the level of single neurons. Unsupervised models, and particularly RBMs, were found to more closely adhere to neurophysiological data from single-cell recordings in the primate parietal cortex. These results provide new insights into how basic properties of artificial neural networks might be relevant for modeling neural information processing in biological systems.","Authors":"Testolin, A (Testolin, Alberto) ; De Grazia, MD (De Grazia, Michele De Filippo) ; Zorzi, M (Zorzi, Marco)","Title":"The Role of Architectural and Learning Constraints in Neural Network Models: A Case Study on Visual Space Coding"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396765200001 ISSN: 2296-9144","Keywords":"drone; virtual reality; stereoscopic cameras; real time; robots; first person view KeyWords Plus:CAMERA SEPARATION; MOTION SICKNESS; VIRTUAL-REALITY; PERFORMANCE; CONSCIOUSNESS; ENVIRONMENT; AWARENESS; SIMULATOR","Categories":"Robotics Web of Science Categories:Robotics","Journal Information":"FRONTIERS IN ROBOTICS AND AI Volume: 4 Article Number: 11 DOI: 10.3389/frobt.2017.00011 Published: MAR 20 2017","Abstract":"Ground control of unmanned aerial vehicles (UAV) is a key to the advancement of this technology for commercial purposes. The need for reliable ground control arises in scenarios where human intervention is necessary, e.g. handover situations when autonomous systems fail. Manual flights are also needed for collecting diverse datasets to train deep neural network-based control systems. This axiom is even more prominent for the case of unmanned flying robots where there is no simple solution to capture optimal navigation footage. In such scenarios, improving the ground control and developing better autonomous systems are two sides of the same coin. To improve the ground control experience, and thus the quality of the footage, we propose to upgrade onboard teleoperation systems to a fully immersive setup that provides operators with a stereoscopic first person view (FPV) through a virtual reality (VR) head-mounted display. We tested users ( n = 7) by asking them to fly our drone on the field. Test flights showed that operators flying our system can take off, fly, and land successfully while wearing VR headsets. In addition, we ran two experiments with prerecorded videos of the flights and walks to a wider set of participants ( n = 69 and n = 20) to compare the proposed technology to the experience provided by current drone FPV solutions that only include monoscopic vision. Our immersive stereoscopic setup enables higher accuracy depth perception, which has clear implications for achieving better teleoperation and unmanned navigation. Our studies show comprehensive data on the impact of motion and simulator sickness in case of stereoscopic setup. We present the device specifications as well as the measures that improve teleoperation experience and reduce induced simulator sickness. Our approach provides higher perception fidelity during flights, which leads to a more precise better teleoperation and ultimately translates into better flight data for training deep UAV control policies.","Authors":"Smolyanskiy, N (Smolyanskiy, Nikolai) ; Gonzalez-Franco, M (Gonzalez-Franco, Mar)","Title":"Stereoscopic First Person View System for Drone Navigation"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396710300001 PubMed ID: 28306716 ISSN: 1932-6203","Categories":"Science & Technology - Other Topics Web of Science Categories:Multidisciplinary Sciences","Journal Information":"PLOS ONE Volume: 12 Issue: 3 Article Number: e0168606 DOI: 10.1371/journal.pone.0168606 Published: MAR 17 2017","Abstract":"Slit-lamp images play an essential role for diagnosis of pediatric cataracts. We present a computer vision-based framework for the automatic localization and diagnosis of slit-lamp images by identifying the lens region of interest (ROI) and employing a deep learning convolutional neural network (CNN). First, three grading degrees for slit-lamp images are proposed in conjunction with three leading ophthalmologists. The lens ROI is located in an automated manner in the original image using two successive applications of Candy detection and the Hough transform, which are cropped, resized to a fixed size and used to form pediatric cataract datasets. These datasets are fed into the CNN to extract high-level features and implement automatic classification and grading. To demonstrate the performance and effectiveness of the deep features extracted in the CNN, we investigate the features combined with support vector machine (SVM) and softmax classifier and compare these with the traditional representative methods. The qualitative and quantitative experimental results demonstrate that our proposed method offers exceptional mean accuracy, sensitivity and specificity: classification (97.07%, 97.28%, and 96.83%) and a three-degree grading area (89.02%, 86.63%, and 90.75%), density (92.68%, 91.05%, and 93.94%) and location (89.28%, 82.70%, and 93.08%). Finally, we developed and deployed a potential automatic diagnostic software for ophthalmologists and patients in clinical applications to implement the validated model.","Authors":"Liu, XY (Liu, Xiyang) ; Jiang, JW (Jiang, Jiewei) ; Zhang, K (Zhang, Kai) ; Long, EP (Long, Erping) ; Cui, JT (Cui, Jiangtao) ; Zhu, MM (Zhu, Mingmin) ; An, YY (An, Yingying) ; Zhang, J (Zhang, Jia) ; Liu, ZZ (Liu, Zhenzhen) ; Lin, ZL (Lin, Zhuoling) ; Li, XY (Li, Xiaoyan) ; Chen, JJ (Chen, Jingjing) ; Cao, QZ (Cao, Qianzhong) ; Li, J (Li, Jing) ; Wu, XH (Wu, Xiaohang) ; Wang, DN (Wang, Dongni) ; Lin, HT (Lin, Haotian) ...More...Less","Title":"Localization and diagnosis framework for pediatric cataracts based on slit-lamp images using deep features of a convolutional neural network"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000398587700023 ISSN: 0013-5194 eISSN: 1350-911X","Keywords":"radiotelephony; air traffic control; recurrent neural nets; feature extraction; telephone traffic recording; text analysis; Chinese aviation radiotelephony readback; LSTM-RNN; radiotelephony communication; air traffic controller; civil aviation safety; semantic consistency verification method; recurrent neural network; long short-term memory structure; Chinese civil aviation radiotelephony recording; textual format; semantic similarity; pilot readback; word-based feature extraction; deep network architecture; high-level sentence semantic abstraction; aviation radiotelephony readback intelligent checking","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"ELECTRONICS LETTERS Volume: 53 Issue: 6 DOI: 10.1049/el.2016.2877 Published: MAR 16 2017","Abstract":"Reading back of the instructions acquired by pilots through radiotelephony communication from air traffic controllers plays a very important role for civil aviation safety. Whereas the mistakes of readbacks are difficult to find out when the controller or the pilot is under great pressure, fatigue, tension etc. To solve this problem, the authors propose a novel semantic consistency verification method based on recurrent neural network with long short-term memory structure (LSTM-RNN) for Chinese radiotelephony readbacks. The actual Chinese civil aviation radiotelephony recordings are converted to textual format, and the semantic similarity is studied to verify whether the semantics is the same between the controller instructions and the pilot readbacks. The word-based feature is extracted by one-hot vector, and LSTM-RNN is employed to build up a deep network architecture for producing high-level sentence semantic abstraction of the initial input instructions and readbacks pairs. Cosine similarity is used to quantify the semantic similarity, and different classification methods are adopted to verify consistency in semantics. The experimental results show that the method is effective and provides a new scheme for the intelligent checking of aviation radiotelephony readbacks.","Authors":"Jia, GM (Jia, Guimin) ; Lu, YJ (Lu, Yujun) ; Lu, WB (Lu, Weibing) ; Shi, YH (Shi, Yihua) ; Yang, JF (Yang, Jinfeng)","Title":"Verification method for Chinese aviation radiotelephony readbacks based on LSTM-RNN"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396514100001 PubMed ID: 28360831 ISSN: 1662-453X","Keywords":"artificial neuron; Morris-Lecar neuron; CMOS; subthreshold; analog VLSI; spiking neural network KeyWords Plus:SPIKING NEURONS; SILICON NEURON; MEMBRANE CAPACITANCE; NETWORKS; OSCILLATIONS; DEPENDENCE; SYNAPSES; SEIZURES; BEHAVIOR; MODELS","Categories":"Neurosciences & Neurology Web of Science Categories:Neurosciences","Journal Information":"FRONTIERS IN NEUROSCIENCE Volume: 11 Article Number: 123 DOI: 10.3389/fnins.2017.00123 Published: MAR 15 2017","Abstract":"As Moore's law reaches its end, traditional computing technology based on the Von Neumann architecture is facing fundamental limits. Among them is poor energy efficiency. This situation motivates the investigation of different processing information paradigms, such as the use of spiking neural networks (SNNs), which also introduce cognitive characteristics. As applications at very high scale are addressed, the energy dissipation needs to be minimized. This effort starts from the neuron cell. In this context, this paper presents the design of an original artificial neuron, in standard 65 nm CMOS technology with optimized energy efficiency. The neuron circuit response is designed as an approximation of the Morris-Lecar theoretical model. In order to implement the non-linear gating variables, which control the ionic channel currents, transistors operating in deep subthreshold are employed. Two different circuit variants describing the neuron model equations have been developed. The first one features spike characteristics, which correlate well with a biological neuron model. The second one is a simplification of the first, designed to exhibit higher spiking frequencies, targeting large scale bio-inspired information processing applications. The most important feature of the fabricated circuits is the energy efficiency of a few femtojoules per spike, which improves prior state-of-the-art by two to three orders of magnitude. This performance is achieved by minimizing two key parameters: the supply voltage and the related membrane capacitance. Meanwhile, the obtained standby power at a resting output does not exceed tens of picowatts. The two variants were sized to 200 and 35 mu m(2) with the latter reaching a spiking output frequency of 26 kHz. This performance level could address various contexts, such as highly integrated neuro-processors for robotics, neuroscience or medical applications.","Authors":"Sourikopoulos, I (Sourikopoulos, Ilias) ; Hedayat, S (Hedayat, Sara) ; Loyez, C (Loyez, Christophe) ; Danneville, F (Danneville, Francois) ; Hoel, V (Hoel, Virginie) ; Mercier, E (Mercier, Eric) ; Cappy, A (Cappy, Alain)","Title":"A 4-fJ/Spike Artificial Neuron in 65 nm CMOS Technology"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395959100101 ISSN: 0306-2619 eISSN: 1872-9118","Keywords":"Wind forecasting; Machine learning; Multi-model; Data-driven; Ensemble forecasting; Feature selection KeyWords Plus:ARTIFICIAL NEURAL-NETWORKS; SPEED PREDICTION; BAT ALGORITHM; TIME-SERIES; POWER; MODELS; REGRESSION; WAVELET; ASSIMILATION; MACHINES","Categories":"Energy & Fuels; Engineering Web of Science Categories:Energy & Fuels; Engineering, Chemical","Journal Information":"APPLIED ENERGY Volume: 190 Pages: 1245-1257 DOI: 10.1016/j.apenergy.2017.01.043 Published: MAR 15 2017","Abstract":"With the growing wind penetration into the power system worldwide, improving wind power forecasting accuracy is becoming increasingly important to ensure continued economic and reliable power system operations. In this paper, a data-driven multi-model wind forecasting methodology is developed with a two-layer ensemble machine learning technique. The first layer is composed of multiple machine learning models that generate individual forecasts. A deep feature selection framework is developed to determine the most suitable inputs to the first layer machine learning models. Then, a blending algorithm is applied in the second layer to create an ensemble of the forecasts produced by first layer models and generate both deterministic and probabilistic forecasts. This two-layer model seeks to utilize the statistically different characteristics of each machine learning algorithm. A number of machine learning algorithms are selected and compared in both layers. This developed multi-model wind forecasting methodology is compared to several benchmarks. The effectiveness of the proposed methodology is evaluated to provide 1-hour-ahead Wind speed forecasting at seven locations of the Surface Radiation network. Numerical results show that comparing to the single-algorithm models, the developed multi-model framework with deep feature selection procedure has improved the forecasting accuracy by up to 30%. (C) 2017 Elsevier Ltd. All rights reserved.","Authors":"Feng, C (Feng, Cong) ; Cui, M (Cui, Mingjian) ; Hodge, BM (Hodge, Bri-Mathias) ; Zhang, J (Zhang, Jie) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Cui, Mingjian http://orcid.org/0000-0002-3047-5141","Title":"A data-driven multi-model methodology with deep feature selection for short-term wind forecasting"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393725000004 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Abdominal adipose tissues segmentation; Coarse-to-fine segmentation; Multi-scale deep neural network; Internal boundary KeyWords Plus:SEGMENTATION; MRI; QUANTIFICATION; OBESITY; FAT; LIVER","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 229 Pages: 23-33 Special Issue: SI DOI: 10.1016/j.neucom.2016.07.059 Published: MAR 15 2017","Abstract":"Segmentation of abdominal adipose tissues (AAT) into subcutaneous adipose tissues (SAT) and visceral adipose tissues (VAT) is of crucial interest for managing the obesity. Previous methods with raw or hand-crafted features rarely work well on large-scale subject cohorts, because of the inhomogeneous image intensities, artifacts and the diverse distributions of VAT. In this paper, we propose a novel two-stage coarse-to-fine algorithm for AAT segmentation. lathe first stage, we formulate the AAT segmentation task as a pixel-wise classification problem. First, three types of features, intensity, spatial and contextual features, are extracted. Second, a new type of deep neural network, nanied multi-scale deep neural network (MSDNN), is provided to extract high-level features. In the second stage, to improve the segmentation accuracy, we refine coarse segmentation results by determining the internal boundaries of SAT based on coarse segmentation results and the continuous of SAT internal boundaries. Finally, we demonstrate the efficacy of our algorithm for both 2D and 3D cases on a wide population range. Compared with other algorithms, our method is not only more suitable for large-scale dataset, but also achieves better segmentation results. Furthermore, our system takes about 2 s to segment an abdominal image, which implies potential clinical applications.","Authors":"Jiang, F (Jiang, Fei) ; Li, HT (Li, Huating) ; Hou, XH (Hou, Xuhong) ; Sheng, B (Sheng, Bin) ; Shen, RM (Shen, Ruimin) ; Liu, XY (Liu, Xiao-Yang) ; Jia, WP (Jia, Weiping) ; Li, P (Li, Ping) ; Fang, RG (Fang, Ruogu) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Liu, Xiao-Yang http://orcid.org/0000-0002-9532-1709","Title":"Abdominal adipose tissues extraction using multi-scale deep neural network"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393725000009 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Wireless capsule endoscopy; Convolutional neural network; Topographic segmentation; Content understanding; Hidden Markov model KeyWords Plus:SEGMENTATION; IMAGES; RECOGNITION; TEXTURE; MODELS; SYSTEM","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 229 Pages: 77-87 Special Issue: SI DOI: 10.1016/j.neucom.2016.06.077 Published: MAR 15 2017","Abstract":"Capsule endoscopy (CE) is the first-line diagnostic tool for inspecting gastrointestinal (GI) tract diseases. It is a tremendous task on examining and managing the CE videos by endoscopists. Therefore, a computer-aided diagnosis system is desired and urgent. In this paper, a general cascaded spatial temporal deep framework is proposed to understand the most commonly seen contents of whole GI tract videos. First, the noisy contents such as feces, bile, bubble, and low power images are detected and removed by a Convolutional Neural Network (CNN) model. The clear images are then classified into entrance, stomach, small intestine, and colon by the second CNN. Finally, the topographic segmentation of the whole video is performed with a global temporal integration strategy by Hidden Markov Model (HMM). Compared to existing methods, the proposed framework performs noise content detection and topographic segmentation at the same time, which significantly reduces the number of images to be checked by endoscopists and segments images of different organs more accurately. Experiments on a dataset with 630K images from 14 patients demonstrate that the proposed approach achieves a promising performance in terms of effectiveness and efficiency.","Authors":"Chen, HH (Chen, Honghan) ; Wu, X (Wu, Xiao) ; Tao, G (Tao, Gan) ; Peng, Q (Peng, Qiang) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Wu, Xiao http://orcid.org/0000-0002-8322-8558","Title":"Automatic content understanding with cascaded spatial-temporal deep framework for capsule endoscopy videos"}, {"Document Information":"Document Type:Review Language:English Accession Number: WOS:000389162000001 ISSN: 0957-4174 eISSN: 1873-6793","Keywords":"Prostate cancer prediction; Predictive modeling; Computational intelligence; Machine learning; Soft computing; Disease classification; Evolutionary computation; Metaheuristic optimisation KeyWords Plus:ARTIFICIAL NEURAL-NETWORK; ANT COLONY OPTIMIZATION; CONTRAST-ENHANCED MRI; FUZZY COGNITIVE MAPS; FEATURE-SELECTION; MULTIOBJECTIVE OPTIMIZATION; LOGISTIC-REGRESSION; GENETIC ALGORITHM; BAYESIAN NETWORK; RANDOM FORESTS","Categories":"Computer Science; Engineering; Operations Research & Management Science Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science","Journal Information":"EXPERT SYSTEMS WITH APPLICATIONS Volume: 70 Pages: 1-19 DOI: 10.1016/j.eswa.2016.11.006 Published: MAR 15 2017","Abstract":"Predictive modeling in medicine involves the development of computational models which are capable of analysing large amounts of data in order to predict healthcare outcomes for individual patients. Computational intelligence approaches are suitable when the data to be modelled are too complex for conventional statistical techniques to process quickly and efficiently. These advanced approaches are based on mathematical models that have been especially developed for dealing with the uncertainty and imprecision which is typically found in clinical and biological datasets. This paper provides a survey of recent work on computational intelligence approaches that have been applied to prostate cancer predictive modeling, and considers the challenges which need to be addressed. In particular, the paper considers a broad definition of computational intelligence which includes metaheuristic optimisation algorithms (also known as nature inspired algorithms), Artificial Neural Networks, Deep Learning, Fuzzy based approaches, and hybrids of these, as well as Bayesian based approaches, and Markov models. Metaheuristic optimisation approaches, such as the Ant Colony Optimisation, Particle Swarm Optimisation, and Artificial Immune Network have been utilised for optimising the performance of prostate cancer predictive models, and the suitability of these approaches are discussed. (C) 2016 Elsevier Ltd. All rights reserved.","Authors":"Cosma, G (Cosma, Georgina) ; Brown, D (Brown, David) ; Archer, M (Archer, Matthew) ; Khan, M (Khan, Masood) ; Pockley, AG (Pockley, A. Graham) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Cosma, Georgina http://orcid.org/0000-0002-4663-6907","Title":"A survey on computational intelligence approaches for predictive modeling in prostate cancer"}, {"Document Information":"Document Type:Review Language:English Accession Number: WOS:000389162000009 ISSN: 0957-4174 eISSN: 1873-6793","Keywords":"Data mining; Multi-objective optimization; Descriptive statistics; Visual data mining; Machine learning; Knowledge-driven optimization KeyWords Plus:MANY-OBJECTIVE OPTIMIZATION; MULTIDISCIPLINARY DESIGN OPTIMIZATION; NONLINEAR DIMENSIONALITY REDUCTION; SUPPORT VECTOR MACHINES; EVOLUTIONARY ALGORITHM; DECISION-SUPPORT; NEURAL-NETWORKS; AUTOMATED INNOVIZATION; VISUALIZATION; SYSTEMS","Categories":"Computer Science; Engineering; Operations Research & Management Science Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science","Journal Information":"EXPERT SYSTEMS WITH APPLICATIONS Volume: 70 Pages: 139-159 DOI: 10.1016/j.eswa.2016.10.015 Published: MAR 15 2017","Abstract":"Real-world optimization problems typically involve multiple objectives to be optimized simultaneously under multiple constraints and with respect to several variables. While multi-objective optimization itself can be a challenging task, equally difficult is the ability to make sense of the obtained solutions. In this two-part paper, we deal with data mining methods that can be applied to extract knowledge about multi-objective optimization problems from the solutions generated during optimization. This knowledge is expected to provide deeper insights about the problem to the decision maker, in addition to assisting the optimization process in future design iterations through an expert system. The current paper surveys several existing data mining methods and classifies them by methodology and type of knowledge discovered. Most of these methods come from the domain of exploratory data analysis and can be applied to any multivariate data. We specifically look at methods that can generate explicit knowledge in a machine-usable form. A framework for knowledge-driven optimization is proposed, which involves both online and offline elements of knowledge discovery. One of the conclusions of this survey is that while there are a number of data mining methods that can deal with data involving continuous variables, only a few ad hoc methods exist that can provide explicit knowledge when the variables involved are of a discrete nature. Part B of this paper proposes new techniques that can be used with such datasets and applies them to discrete variable multi-objective problems related to production systems. (C) 2016 Elsevier Ltd. All rights reserved.","Authors":"Bandaru, S (Bandaru, Sunith) ; Ng, AHC (Ng, Amos H. C.) ; Deb, K (Deb, Kalyanmoy) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Bandaru, Sunith http://orcid.org/0000-0001-5436-2128","Title":"Data mining methods for knowledge discovery in multi-objective optimization: Part A - Survey"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397619200003 ISSN: 2041-8205 eISSN: 2041-8213","Keywords":"methods; data analysis - supernovae; general - techniques; miscellaneous KeyWords Plus:PHOTOMETRIC CLASSIFICATION","Categories":"Astronomy & Astrophysics Web of Science Categories:Astronomy & Astrophysics","Journal Information":"ASTROPHYSICAL JOURNAL LETTERS Volume: 837 Issue: 2 Article Number: L28 DOI: 10.3847/2041-8213/aa603d Published: MAR 10 2017","Abstract":"We apply deep recurrent neural networks, which are capable of learning complex sequential information, to classify supernovae (code available at https:// github.com/ adammoss/ supernovae). The observational time and filter fluxes are used as inputs to the network, but since the inputs are agnostic, additional data such as host galaxy information can also be included. Using the Supernovae Photometric Classification Challenge (SPCC) data, we find that deep networks are capable of learning about light curves, however the performance of the network is highly sensitive to the amount of training data. For a training size of 50% of the representational SPCC data set (around 104 supernovae) we obtain a type-Ia versus non-type-Ia classification accuracy of 94.7%, an area under the Receiver Operating Characteristic curve AUC of 0.986 and an SPCC figure-of-merit F1. =. 0.64. When using only the data for the early-epoch challenge defined by the SPCC, we achieve a classification accuracy of 93.1%, AUC of 0.977, and F-1. =. 0.58, results almost as good as with the whole light curve. By employing bidirectional neural networks, we can acquire impressive classification results between supernovae types I, II and III at an accuracy of 90.4% and AUC of 0.974. We also apply a pre-trained model to obtain classification probabilities as a function of time and show that it can give early indications of supernovae type. Our method is competitive with existing algorithms and has applications for future large-scale photometric surveys.","Authors":"Charnock, T (Charnock, Tom) ; Moss, A (Moss, Adam)","Title":"Deep Recurrent Neural Networks for Supernovae Classification"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396091800053 PubMed ID: 28282414 ISSN: 1932-6203","Categories":"Science & Technology - Other Topics Web of Science Categories:Multidisciplinary Sciences","Journal Information":"PLOS ONE Volume: 12 Issue: 3 Article Number: e0173579 DOI: 10.1371/journal.pone.0173579 Published: MAR 10 2017","Abstract":"People are better at remembering faces from their own race than other races-a phenomenon with significant societal implications. This Other Race Effect (ORE) in memory could arise from different attentional allocation to, and cognitive control over, same-and otherrace faces during encoding. Deeper or more differentiated processing of same-race faces could yield more robust representations of same- vs. other-race faces that could support better recognition memory. Conversely, to the extent that other-race faces may be characterized by lower perceptual expertise, attention and cognitive control may be more important for successful encoding of robust, distinct representations of these stimuli. We tested a mechanistic model in which successful encoding of same- and other-race faces, indexed by subsequent memory performance, is differentially predicted by (a) engagement of frontoparietal networks subserving top-down attention and cognitive control, and (b) interactions between frontoparietal networks and fusiform cortex face processing. European American (EA) and African American (AA) participants underwent fMRI while intentionally encoding EA and AA faces, and similar to 24 hrs later performed an \"old/new\" recognition memory task. Univariate analyses revealed greater engagement of frontoparietal top-down attention and cognitive control networks during encoding for same-vs. other-race faces, stemming particularly from a failure to engage the cognitive control network during processing of other-race faces that were subsequently forgotten. Psychophysiological interaction (PPI) analyses further revealed that OREs were characterized by greater functional interaction between medial intraparietal sulcus, a component of the top-down attention network, and fusiform cortex during same-than otherrace face encoding. Together, these results suggest that group-based face memory biases at least partially stem from differential allocation of cognitive control and top-down attention during encoding, such that same-race memory benefits from elevated top-down attentional engagement with face processing regions; conversely, reduced recruitment of cognitive control circuitry appears more predictive of memory failure when encoding out-group faces.","Authors":"Brown, TI (Brown, Thackery I.) ; Uncapher, MR (Uncapher, Melina R.) ; Chow, TE (Chow, Tiffany E.) ; Eberhardt, JL (Eberhardt, Jennifer L.) ; Wagner, AD (Wagner, Anthony D.)","Title":"Cognitive control, attention, and the other race effect in memory"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396091800068 PubMed ID: 28282439 ISSN: 1932-6203","Categories":"Science & Technology - Other Topics Web of Science Categories:Multidisciplinary Sciences","Journal Information":"PLOS ONE Volume: 12 Issue: 3 Article Number: e0173684 DOI: 10.1371/journal.pone.0173684 Published: MAR 10 2017","Abstract":"The intra-parietal lobe coupled with the Basal Ganglia forms a working memory that demonstrates strong planning capabilities for generating robust yet flexible neuronal sequences. Neurocomputational models however, often fails to control long range neural synchrony in recurrent spiking networks due to spontaneous activity. As a novel framework based on the free-energy principle, we propose to see the problem of spikes' synchrony as an optimization problem of the neurons sub-threshold activity for the generation of long neuronal chains. Using a stochastic gradient descent, a reinforcement signal (presumably dopaminergic) evaluates the quality of one input vector to move the recurrent neural network to a desired activity; depending on the error made, this input vector is strengthened to hill-climb the gradient or elicited to search for another solution. This vector can be learned then by one associative memory as a model of the basal-ganglia to control the recurrent neural network. Experiments on habit learning and on sequence retrieving demonstrate the capabilities of the dual system to generate very long and precise spatio-temporal sequences, above two hundred iterations. Its features are applied then to the sequential planning of arm movements. In line with neurobiological theories, we discuss its relevance for modeling the cortico-basal working memory to initiate flexible goal-directed neuronal chains of causation and its relation to novel architectures such as Deep Networks, Neural Turing Machines and the Free-Energy Principle.","Authors":"Pitti, A (Pitti, Alexandre) ; Gaussier, P (Gaussier, Philippe) ; Quoy, M (Quoy, Mathias)","Title":"Iterative free-energy optimization for recurrent neural networks (INFERNO)"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393017900018 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Image aesthetics; Quality assessment; High aesthetics; Low aesthetics; Image classification; Deep convolutional neural network; GoogLeNet; Feature representation KeyWords Plus:IMAGE QUALITY ASSESSMENT","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 228 Pages: 165-175 Special Issue: SI DOI: 10.1016/j.neucom.2016.08.098 Published: MAR 8 2017","Abstract":"In response to the growth of digital photography and its many related applications, researchers have been actively investigating methods for providing automated aesthetical evaluation and classification of photographs. For computational networks to recognize aesthetic qualities, the learning algorithms must be trained using sample sets of characteristics that have known aesthetic values. Traditional methods for developing this training have required manual extraction of aesthetic features for use in the practice datasets. With abundant appearance of convolutional neural networks (CNN), the networks have learned features automatically and have acted as important tools for evaluation and classification. At the time of our research, several existing convolutional neural networks for photograph aesthetical classification only used shallow depth networks, which limit the improvement of performance. In addition, most methods have extracted only one patch as a training sample, such as a down-sized crop from each image. However, a single patch might not represent the entire image accurately, which could cause ambiguity during training. What's more, for existing datasets, the numbers of high quality images of each category are mostly too small to train deep CNN networks. To solve these problems, we introduce a novel photograph aesthetic classifier with a deep and wide CNN for fine granularity aesthetical quality prediction. First, we download a large number of consumer photographic images from DPChallenge.com (a well-known online photography portal) to construct a dataset suitable for aesthetic quality assessment. Then, we zoom out the images into 256x256 by bilinear interpolation and crop 10 patches (Center+four Comers+Flipping). Once we have associated the set with the image's training labels, We feed the images with the bag of patches into the fine-tuned networks. Our proposed computational method is configured to classify the photographs into high and low aesthetic values. A training pattern specifying an output of (0, 1) indicates that the corresponding image belongs to the \"low aesthetic quality\" set. Likewise, a training pattern with an output of (1, 0) indicates that the corresponding image belongs to the \"high aesthetic quality\" set. Experimental results show that the accuracy of classification provided by our method is greater than 87.10%, which is noticeably better than the state-of-the-art methods. In addition, our experiments show that our results are fundamentally consistent with human visual perception and aesthetic judgments.","Authors":"Tan, YL (Tan, Yunlan) ; Tang, PJ (Tang, Pengjie) ; Zhou, YM (Zhou, Yimin) ; Luo, WL (Luo, Wenlang) ; Kang, YP (Kang, Yongping) ; Li, GY (Li, Guangyao)","Title":"Photograph aesthetical evaluation and classification with deep convolutional neural networks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395561000001 PubMed ID: 28326009 ISSN: 1662-453X","Keywords":"electroencephalography; driver fatigue; autoregressive model; deep belief networks; sparse-deep belief networks KeyWords Plus:NEURAL-NETWORKS; SYSTEM; INTERFACE; TIME; DETERMINANTS; ALGORITHM; WIRELESS; SPECTRUM; FEATURES; FUSION","Categories":"Neurosciences & Neurology Web of Science Categories:Neurosciences","Journal Information":"FRONTIERS IN NEUROSCIENCE Volume: 11 Article Number: 103 DOI: 10.3389/fnins.2017.00103 Published: MAR 7 2017","Abstract":"This paper presents an improvement of classification performance for electroencephalography (EEG)-based driver fatigue classification between fatigue and alert states with the data collected from 43 participants. The system employs autoregressive (AR) modeling as the features extraction algorithm, and sparse-deep belief networks (sparse-DBN) as the classification algorithm. Compared to other classifiers, sparse-DBN is a semi supervised learning method which combines unsupervised learning for modeling features in the pre-training layer and supervised learning for classification in the following layer. The sparsity in sparse-DBN is achieved with a regularization term that penalizes a deviation of the expected activation of hidden units from a fixed low-level prevents the network from overfitting and is able to learn low-level structures as well as high-level structures. For comparison, the artificial neural networks (ANN), Bayesian neural networks (BNN), and original deep belief networks (DBN) classifiers are used. The classification results show that using AR feature extractor and DBN classifiers, the classification performance achieves an improved classification performance with a of sensitivity of 90.8%, a specificity of 90.4%, an accuracy of 90.6%, and an area under the receiver operating curve (AUROC) of 0.94 compared to ANN (sensitivity at 80.8%, specificity at 77.8%, accuracy at 79.3% with AUC-ROC of 0.83) and BNN classifiers (sensitivity at 84.3%, specificity at 83%, accuracy at 83.6% with AUROC of 0.87). Using the sparse-DBN classifier, the classification performance improved further with sensitivity of 93.9%, a specificity of 92.3%, and an accuracy of 93.1% with AUROC of 0.96. Overall, the sparse-DBN classifier improved accuracy by 13.8, 9.5, and 2.5% over ANN, BNN, and DBN classifiers, respectively.","Authors":"Chai, RF (Chai, Rifai) ; Ling, SH (Ling, Sai Ho) ; San, PP (San, Phyo Phyo) ; Naik, GR (Naik, Ganesh R.) ; Nguyen, TN (Nguyen, Tuan N.) ; Tran, Y (Tran, Yvonne) ; Craig, A (Craig, Ashley) ; Nguyen, HT (Nguyen, Hung T.)","Title":"Improving EEG-Based Driver Fatigue Classification Using Sparse-Deep Belief Networks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393410000011 ISSN: 0962-8436 eISSN: 1471-2970","Keywords":"synaptic scaling; homeostatic plasticity; Hebbian plasticity; sleep KeyWords Plus:FIRING RATE HOMEOSTASIS; MOUSE VISUAL-CORTEX; SYNAPTIC PLASTICITY; NEOCORTICAL SYNAPSES; CRITICAL PERIOD; DEPENDENT PLASTICITY; EXCITATORY SYNAPSES; QUANTAL AMPLITUDE; CORTICAL-NEURONS; PYRAMIDAL NEURON","Categories":"Life Sciences & Biomedicine - Other Topics Web of Science Categories:Biology","Journal Information":"PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES Volume: 372 Issue: 1715 Article Number: 20160258 DOI: 10.1098/rstb.2016.0258 Published: MAR 5 2017","Abstract":"mechanisms work hand in glove to refine neural circuit function. Nonetheless, our understanding of how these fundamentally distinct forms of plasticity compliment (and under some circumstances interfere with) each other remains rudimentary. Here, I describe some of the recent progress of the field, as well as some of the deep puzzles that remain. These include unravelling the spatial and temporal scales of different homeostatic and Hebbian mechanisms, determining which aspects of network function are under homeostatic control, and understanding when and how homeostatic and Hebbian mechanisms must be segregated within neural circuits to prevent interference. This article is part of the themed issue 'Integrating Hebbian and homeostatic plasticity'.","Authors":"Turrigiano, GG (Turrigiano, Gina G.)","Title":"The dialectic of Hebb and homeostasis"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000398808500001 ISSN: 1433-5298 eISSN: 1614-7456","Keywords":"Deep neural network; GMDH; Medical image diagnosis; Evolutionary computation; Machine learning; GMDH-type neural network","Categories":"Robotics Web of Science Categories:Robotics","Journal Information":"ARTIFICIAL LIFE AND ROBOTICS Volume: 22 Issue: 1 Pages: 1-9 DOI: 10.1007/s10015-016-0337-y Published: MAR 2017","Abstract":"The deep feedback Group Method of Data Handling (GMDH)-type neural network is applied to the medical image recognition of kidney regions. In this algorithm, the principal component-regression analysis is used for the learning calculation of the neural network, and the accurate and stable predicted values are obtained. The neural network architecture is automatically organized so as to fit the complexity of the medical images using the prediction error criterion defined as Akaike's Information Criterion (AIC) or Prediction Sum of Squares (PSS). The recognition results show that the deep feedback GMDH-type neural network algorithm is useful for the medical image recognition of kidney regions, because the optimum neural network architecture is automatically organized.","Authors":"Kondo, T (Kondo, Tadashi) ; Kondo, S (Kondo, Sayaka) ; Ueno, J (Ueno, Junji) ; Takao, S (Takao, Shoichiro)","Title":"Medical image diagnosis of kidney regions by deep feedback GMDH-type neural network using principal component-regression analysis"}]