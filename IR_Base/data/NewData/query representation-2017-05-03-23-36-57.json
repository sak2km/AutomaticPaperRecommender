[{"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397687400003 ISSN: 0957-4174 eISSN: 1873-6793","Keywords":"Image retrieval; Image annotation; Social image tagging; Topic modeling; Probabilistic graphical model KeyWords Plus:LATENT DIRICHLET ALLOCATION; CROSS-MEDIA RETRIEVAL; TAG COMPLETION; ANNOTATION; REPRESENTATION; CLASSIFICATION","Categories":"Computer Science; Engineering; Operations Research & Management Science Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science","Journal Information":"EXPERT SYSTEMS WITH APPLICATIONS Volume: 77 Pages: 20-33 DOI: 10.1016/j.eswa.2017.01.055 Published: JUL 1 2017","Abstract":"Nowadays, due to the rapid growth of digital technologies, huge volumes of image data are created and shared on social media sites. User-provided tags attached to each social image are widely recognized as a bridge to fill the semantic gap between low-level image features and high-level concepts. Hence, a combination of images along with their corresponding tags is useful for intelligent retrieval systems, those are designed to gain high-level understanding from images and facilitate semantic search. However, user provided tags in practice are usually incomplete and noisy, which may degrade the retrieval performance. To tackle this problem, we present a novel retrieval framework that automatically associates the visual content with textual tags and enables effective image search. To this end, we first propose a probabilistic topic model learned on social images to discover latent topics from the co-occurrence of tags and image features. Moreover, our topic model is built by exploiting the expert knowledge about the correlation between tags with visual contents and the relationship among image features that is formulated in terms of spatial location and color distribution. The discovered topics then help to predict missing tags of an unseen image as well as the ones partially labeled in the database. These predicted tags can greatly facilitate the reliable measure of semantic similarity between the query and database images. Therefore, we further present a scoring scheme to estimate the similarity by fusing textual tags and visual representation. Extensive experiments conducted on three benchmark datasets show that our topic model provides the accurate annotation against the noise and incompleteness of tags. Using our generalized scoring scheme, which is particularly advantageous to many types of queries, the proposed approach also outperforms state-of-the-art approaches in terms of retrieval accuracy. (C) 2017 Elsevier Ltd. All rights reserved.","Authors":"Tu, NA (Nguyen Anh Tu) ; Khan, KU (Khan, Kifayat Ullah) ; Lee, YK (Lee, Young-Koo)","Title":"Featured correspondence topic model for semantic search on social image collections"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394070100004 ISSN: 1566-2535 eISSN: 1872-6305","Keywords":"Knowledge structuring; Latent Dirichlet Allocation (LDA); Latent Semantic Analysis (LSA); Concept extraction; Ontology; SKOS KeyWords Plus:INFORMATION; RETRIEVAL; WEB","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Theory & Methods","Journal Information":"INFORMATION FUSION Volume: 36 Pages: 52-67 DOI: 10.1016/j.inffus.2016.11.003 Published: JUL 2017","Abstract":"In the era of Web 2.0, the knowledge is the de-facto social currency in the global network environment. Knowledge is not an accumulation of data, but a relation-based representation of the information content, which needs to be distilled and arranged in a semantic infrastructure to guarantee interoperability and sharable understanding. In the light of this scenario, the paper introduces a semantically enhanced document retrieval system that describes each retrieved document with an ontological multi-grained network of the extracted conceptualization. The system is based on two well-known latent models: Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA): LSA provides a spatial distribution of the input documents, facilitating their retrieval, thanks to an ontological representation of their relationship network. LDA works instead at deeper level: it drives the ontological structuring of the knowledge inside the individual retrieved documents in terms of words, concepts and topics. The novelty of this approach is a multi-level granulation of the knowledge: from a document matching the query (coarse granularity), to the topics that join documents, until to the words describing a concept into a topic (fine granularity). The final result is a SKOS-based ontology, ad-hoc created for a document corpus; graphically supported for the navigation, it enables the exploration of the concepts at different granularity levels. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Della Rocca, P (Della Rocca, Paola) ; Senatore, S (Senatore, Sabrina) ; Loia, V (Loia, Vincenzo) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Senatore, Sabrina  http://orcid.org/0000-0002-7127-4290","Title":"A semantic-grained perspective of latent knowledge modeling"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394067700016 ISSN: 0377-0427 eISSN: 1879-1778","Keywords":"Time series; Fuzzy representation; TS comparison; TS queries KeyWords Plus:NUMBERS; MODEL; PATTERN","Categories":"Mathematics Web of Science Categories:Mathematics, Applied","Journal Information":"JOURNAL OF COMPUTATIONAL AND APPLIED MATHEMATICS Volume: 318 Pages: 156-167 Special Issue: SI DOI: 10.1016/j.cam.2016.11.003 Published: JUL 2017","Abstract":"The aim of this paper is to represent time series in a fuzzy way by means of a piecewise linear segment method that represents the time series in an efficient way. But in addition, this representation collects the uncertainty generated in the process of generation of the segments. This representation can be used, for instance, to compare time series or to query information from these. That is why in this approach we also propose a method to compare different parts of the output, obtaining as result a dissimilarity value representing how close their outputs are. Several tests have been done in order to know if input time series are represented correctly with this method and to know if the comparison process obtains the expected results. The input data selected for the tests is the real data obtained from the electric consumption of three buildings from the campus in the city of Toledo of our university. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Moreno-Garcia, A (Moreno-Garcia, Antonio) ; Moreno-Garcia, J (Moreno-Garcia, Juan) ; Jimenez-Linares, L (Jimenez-Linares, Luis) ; Rodriguez-Benitez, L (Rodriguez-Benitez, Luis)","Title":"Time series represented by means of fuzzy piecewise lineal segments"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397371800013 ISSN: 0031-3203 eISSN: 1873-5142","Keywords":"Joint and collaborative representation; Adaptive convolution feature; Sparse representation; Single sample face recognition KeyWords Plus:SPARSE REPRESENTATION; TRAINING SAMPLE; IMAGE; CLASSIFICATION","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"PATTERN RECOGNITION Volume: 66 Pages: 117-128 Special Issue: SI DOI: 10.1016/j.patcog.2016.12.028 Published: JUN 2017","Abstract":"With the aid of a universal facial variation dictionary, sparse representation based classifier (SRC) has been naturally extended for face recognition (FR) with single sample per person (SSPP) and achieved promising performance. However, extracting discriminative facial features and building powerful representation framework for classifying query face images are still the bottlenecks of improving the performance of FR with SSPP. In this paper, by densely sampling and sparsely detecting facial points, we extract complete and robust local regions and learn convolution features adaptive to the local regions and discriminative to the face identity by using convolutional neural networks (CNN). With this powerful facial description and a generic face dataset with common facial variations, a joint and collaborative representation framework, which performs representation for each local region of the query face image while requires all regions of the query face image to have similar representation coefficients, is presented to exploit the distinctiveness and commonality of different local regions. In the proposed joint and collaborative representation with local adaptive convolution feature (JCRACF), both discriminative local facial features that are robust to various facial variations and powerful representation dictionaries of facial variations that can overcome the small-sample-size problem are fully exploited. JCR-ACF has been extensively evaluated on several popular databases including AR, CMU Multi-PIE, LFW and the large-scale CASIA-WebFace databases. Experimental results demonstrate the much higher robustness and effectiveness of JCR-ACF to complex facial variations compared to the state-of-the-art methods.","Authors":"Yang, M (Yang, Meng) ; Wang, X (Wang, Xing) ; Zeng, GH (Zeng, Guohang) ; Shen, LL (Shen, Linlin)","Title":"Joint and collaborative representation with local adaptive convolution feature for face recognition with single sample per person"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397372100035 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Particular object retrieval; Bag-of-words; SIFT matching; Convolutional neural networks KeyWords Plus:NEAREST-NEIGHBOR SEARCH; IMAGE RETRIEVAL; SCALE; SIMILARITY","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 238 Pages: 399-409 DOI: 10.1016/j.neucom.2017.01.081 Published: MAY 17 2017","Abstract":"Many object instance retrieval systems are typically based on matching of local features, such as SIFT. However, these local descriptors serve as low-level clues, which are not sufficiently distinctive to prevent false matches. Recently, deep convolutional neural networks (CNN) have shown their promise as a semantic-aware representation for many computer vision tasks. In this paper, we propose a novel approach to employ CNN evidences to improve the SIFT matching accuracy, which plays a critical role in improving the object retrieval performance. To weaken the interference of noise, we extract compact CNN representations from a number of generic object regions. Then a query-adaptive method is proposed to choose appropriate CNN evidence to verify each pre-matched SIFT pair. Two different visual matching verification functions are introduced and evaluated. Moreover, we investigate the suitability of fine-tuning the CNN for our proposed approach. Extensive experiments on benchmark dataSets demonstrate the effectiveness of our method for particular object retrieval. Our results compare favorably to the state-of-the-art methods with acceptable memory usage and query time. (C) 2017 Elsevier B.V. All rights reserved.","Authors":"Zhang, GX (Zhang, Guixuan) ; Zeng, Z (Zeng, Zhi) ; Zhang, SW (Zhang, Shuwu) ; Zhang, Y (Zhang, Yuan) ; Wu, WC (Wu, Wanchun)","Title":"SIFT Matching with CNN Evidences for Particular Object Retrieval"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396952300003 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Separable vocabulary; Sparse representation; Feature fusion; Image retrieval KeyWords Plus:FACE RECOGNITION; SEARCH","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 236 Pages: 14-22 Special Issue: SI DOI: 10.1016/j.neucom.2016.08.106 Published: MAY 2 2017","Abstract":"Visual vocabulary is the core of the Bag-of-visual-words (BOW) model in image retrieval. In order to ensure the retrieval accuracy, a large vocabulary is always used in traditional methods. However, a large vocabulary will lead to a low recall. In order to improve recall, vocabularies with medium sizes are proposed, but they will lead to a low accuracy. To address these two problems, we propose a new method for image retrieval based on feature fusion and sparse representation over separable vocabulary. Firstly, a large vocabulary is generated on the training dataset. Secondly, the vocabulary is separated into a number of vocabularies with medium sizes. Thirdly, for a given query image, we adopt sparse representation to select a vocabulary for retrieval. In the proposed method, the large vocabulary can guarantee a relatively high accuracy, while the vocabularies with medium sizes are responsible for high recall. Also, in order to reduce quantization error and improve recall, sparse representation scheme is used for visual words quantization. Moreover, both the local features and the global features are fused to improve the recall. Our proposed method is evaluated on two benchmark datasets, i.e., Coi120 and Holidays. Experiments show that our proposed method achieves good performance.","Authors":"Wang, YH (Wang, Yanhong) ; Cen, YG (Cen, Yigang) ; Zhao, RZ (Zhao, Ruizhen) ; Cen, Y (Cen, Yi) ; Hu, SH (Hu, Shaohai) ; Voronin, V (Voronin, Viacheslay) ; Wang, HY (Wang, Hengyou)","Title":"Separable vocabulary and feature fusion for image retrieval based on sparse representation"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396949800012 ISSN: 0921-8890 eISSN: 1872-793X","Keywords":"Mapping; Mobile robotics; Point cloud; Segmentation; Retrieval","Categories":"Automation & Control Systems; Computer Science; Robotics Web of Science Categories:Automation & Control Systems; Computer Science, Artificial Intelligence; Robotics","Journal Information":"ROBOTICS AND AUTONOMOUS SYSTEMS Volume: 91 Pages: 139-150 DOI: 10.1016/j.robot.2016.12.013 Published: MAY 2017","Abstract":"We present a novel method for efficient querying and retrieval of arbitrarily shaped objects from large amounts of unstructured 3D point cloud data. Our approach first performs a convex segmentation of the data after which local features are extracted and stored in a feature dictionary. We show that the representation allows efficient and reliable querying of the data. To handle arbitrarily shaped objects, we propose a scheme which allows incremental matching of segments based on similarity to the query object. Further, we adjust the feature metric based on the quality of the query results to improve results in a second round of querying. We perform extensive qualitative and quantitative experiments on two datasets for both segmentation and retrieval, validating the results using ground truth data. Comparison with other state of the art methods further enforces the validity of the proposed method. Finally, we also investigate how the density and distribution of the local features within the point clouds influence the quality of the results. (C) 2017 The Author(s). Published by Elsevier B.V.","Authors":"Bore, N (Bore, Nils) ; Ambrus, R (Ambrus, Rares) ; Jensfelt, P (Jensfelt, Patric) ; Folkesson, J (Folkesson, John)","Title":"Efficient retrieval of arbitrary objects from long-term robot observations"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394068100010 ISSN: 0020-0255 eISSN: 1872-6291","Keywords":"Probabilistic range queries; Location-dependent queries; Uncertainty management KeyWords Plus:NEAREST-NEIGHBOR QUERIES; MOVING-OBJECTS; EFFICIENT; NETWORK","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"INFORMATION SCIENCES Volume: 388 Pages: 154-171 DOI: 10.1016/j.ins.2017.01.029 Published: MAY 2017","Abstract":"Location-based services have motivated intensive research in the field of mobile computing, and particularly on location-dependent queries. Existing approaches usually assume that the location data are expressed at a fine geographic precision (physical coordinates such as GPS). However, many positioning mechanisms are subject to an inherent imprecision (e.g., the cell-id mechanism used in cellular networks can only determine the cell where a certain moving object is located). Moreover, even a GPS location can be subject to an error or be obfuscated for privacy reasons. Thus, moving objects can be considered to be associated not to an exact location, but to an uncertainty area where they can be located. In this paper, we analyze the problem introduced by the imprecision of the location data available in the data sources by modeling them using uncertainty areas. To do so, we propose to use a higher-level representation of locations which includes uncertainty, formalizing the concept of uncertainty location granule. This allows us to consider probabilistic location-dependent queries, among which we will focus on probabilistic inside (range) constraints. The adopted model allows us to develop a systematic and efficient approach for processing this kind of queries. An experimental evaluation shows that these probabilistic queries can be supported efficiently. (C) 2017 Elsevier Inc. All rights reserved.","Authors":"Bernad, J (Bernad, Jorge) ; Bobed, C (Bobed, Carlos) ; Ilarri, S (Ilarri, Sergio) ; Mena, E (Mena, Eduardo)","Title":"Handling location uncertainty in probabilistic location-dependent queries"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397072800008 ISSN: 0021-9991 eISSN: 1090-2716","Keywords":"Polynomial chaos expansion; Arbitrary probability measure; Concentration of probability; Measure concentration; Generator; Probability distribution on manifolds; Random sampling generator; MCMC generator; Diffusion maps; Statistics on manifolds KeyWords Plus:PARTIAL-DIFFERENTIAL-EQUATIONS; HIGH-DIMENSION; RANDOM-FIELDS; CONVERGENCE ACCELERATION; STOCHASTIC-PROCESSES; RANDOM-COEFFICIENTS; MAXIMUM-ENTROPY; DIFFUSION MAPS; EXPANSIONS; SYSTEMS","Categories":"Computer Science; Physics Web of Science Categories:Computer Science, Interdisciplinary Applications; Physics, Mathematical","Journal Information":"JOURNAL OF COMPUTATIONAL PHYSICS Volume: 335 Pages: 201-221 DOI: 10.1016/j.jcp.2017.01.031 Published: APR 15 2017","Abstract":"Characterizing the polynomial chaos expansion (PCE) of a vector-valued random variable with probability distribution concentrated on a manifold is a relevant problem in data driven settings. The probability distribution of such random vectors is multimodal in general, leading to potentially very slow convergence of the PCE. In this paper, we build on a recent development for estimating and sampling from probabilities concentrated on a diffusion manifold. The proposed methodology constructs a PCE of the random vector together with an associated generator that samples from the target probability distribution which is estimated from data concentrated in the neighborhood of the manifold. The method is robust and remains efficient for high dimension and large datasets. The resulting polynomial chaos construction on manifolds permits the adaptation of many uncertainty quantification and statistical tools to emerging questions motivated by data-driven queries. (C) 2017 Elsevier Inc. All rights reserved.","Authors":"Soize, C (Soize, C.) ; Ghanem, R (Ghanem, R.)","Title":"Polynomial chaos representation of databases on manifolds"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395380400003 ISSN: 1473-8716 eISSN: 1473-8724","Keywords":"Virtual museum; three-dimensional reality; three-dimensional visualization; panoramic image; three-dimensional model; web client; mobile client; performance measurement KeyWords Plus:CAVE; TRANSMISSION; HERITAGE; REALITY; WEB; ART","Categories":"Computer Science Web of Science Categories:Computer Science, Software Engineering","Journal Information":"INFORMATION VISUALIZATION Volume: 16 Issue: 2 Pages: 126-138 DOI: 10.1177/1473871616655467 Published: APR 2017","Abstract":"As an important part of the public service and educational infrastructures for national culture and heritage culture, a virtual museum presents the user experience of a real museum, with visitors, educators, and tourists interacting with the prepared digital culture contents by a mouse, touch panel, and other augmented reality devices. The goal of virtual museum is to help students and visitors to move around the virtual museum space freely and generate experience and satisfaction from the fruition of cultural heritage anytime, anywhere, and from any device. This study presents a hybrid three-dimensional virtual museum based on panoramic images and three-dimensional models. A technical framework of hybrid three-dimensional virtual museum is proposed on the basis of a typical three-tier architecture, which includes the data layer, technique supporting layer, and application layer. A hybrid three-dimensional data organization approach with geo-referenced sequence panoramic images and three-dimensional models is designed to build the data layer of hybrid three-dimensional virtual museum. A three-dimensional scene of geo-referenced sequence panoramic images and three-dimensional models is created in real time using Unity three-dimensional and web service under the mobile Internet environment for hybrid three-dimensional virtual museum. The different applications of hybrid three-dimensional virtual museum based on the data layer and technique infrastructure are designed to achieve handheld virtual museum guidance and navigation, three-dimensional browsing, and heritage culture information query for visitors with smartphones to access anytime and anywhere. As an example, a hybrid three-dimensional virtual museum application for Jinsha Archaeological Site Museum is developed with the proposed approach. The geo-referenced sequence panoramic images of museum galleries, together with three-dimensional models of cultural relics, can integrate seamlessly to a three-dimensional reality-based museum space where users can move around the space actively and freely with all kinds of personal computer and smartphone clients.","Authors":"Hu, QW (Hu, Qingwu) ; Yu, DB (Yu, Dengbo) ; Wang, SH (Wang, Shaohua) ; Fu, CW (Fu, Caiwu) ; Ai, MY (Ai, Mingyao) ; Wang, WD (Wang, Wende)","Title":"Hybrid three-dimensional representation based on panoramic images and three-dimensional models for a virtual museum: Data collection, model, and visualization"}, {"Keywords":"Virtual topology; Clean-up; Decomposition; Hex meshing schemes KeyWords Plus:MESH GENERATION","Categories":"Computer Science Web of Science Categories:Computer Science, Software Engineering","Journal Information":"COMPUTER-AIDED DESIGN Volume: 85 Pages: 154-167 Special Issue: SI DOI: 10.1016/j.cad.2016.07.015 Published: APR 2017","Abstract":"Virtual topology operations have been utilized to generate an analysis topology definition suitable for downstream mesh generation. Detailed descriptions are provided for virtual topology merge and split operations for all topological entities. Current virtual topology technology is extended to allow the virtual partitioning of volume cells and the topological queries required to carry out each operation are provided. Virtual representations are robustly linked to the underlying geometric definition through an analysis topology. The analysis topology and all associated virtual and topological dependencies are automatically updated after each virtual operation, providing the link to the underlying CAD geometry. Therefore, a valid description of the analysis topology, including relative orientations, is maintained. This enables downstream operations, such as the merging or partitioning of virtual entities, and interrogations, such as determining if a specific meshing strategy can be applied to the virtual volume cells, to be performed on the analysis topology description. As the virtual representation is a non-manifold description of the sub-divided domain the interfaces between cells are recorded automatically. This enables the advantages of non-manifold modelling to be exploited within the manifold modelling environment of a major commercial CAD system, without any adaptation of the underlying CAD model. A hierarchical virtual structure is maintained where virtual entities are merged or partitioned. This has a major benefit over existing solutions as the virtual dependencies are stored in an open and accessible manner, providing the analyst with the freedom to create, modify and edit the analysis topology in any preferred sequence, whilst the original CAD geometry is not disturbed. Robust definitions of the topological and virtual dependencies enable the same virtual topology definitions to be accessed, interrogated and manipulated within multiple different CAD packages and linked to the underlying geometry. (C) 2016 Elsevier Ltd. All rights reserved.","Authors":"Tierney, CM (Tierney, Christopher M.) ; Sun, L (Sun, Liang) ; Robinson, TT (Robinson, Trevor T.) ; Armstrong, CG (Armstrong, Cecil G.)","Title":"Using virtual topology operations to generate analysis topology"}, {"Document Information":"Document Type:Article; Proceedings Paper Language:English Accession Number: WOS:000396403800017 PubMed ID: 28129169 ISSN: 1077-2626 eISSN: 1941-0506","Keywords":"Real-time interactive systems; virtual reality systems; software architecture; multimodal processing","Categories":"Computer Science Web of Science Categories:Computer Science, Software Engineering","Journal Information":"IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS Volume: 23 Issue: 4 Pages: 1407-1416 DOI: 10.1109/TVCG.2017.2657098 Published: APR 2017","Abstract":"Modularity, modifiability, reusability, and API usability are important software qualities that determine the maintainability of software architectures. Virtual, Augmented, and Mixed Reality (VR, AR, MR) systems, modern computer games, as well as interactive human-robot systems often include various dedicated input-, output-, and processing subsystems. These subsystems collectively maintain a real-time simulation of a coherent application state. The resulting interdependencies between individual state representations, mutual state access, overall synchronization, and flow of control implies a conceptual close coupling whereas software quality asks for a decoupling to develop maintainable solutions. This article presents five semantics-based software techniques that address this contradiction: Semantic grounding, code from semantics, grounded actions, semantic queries, and decoupling by semantics. These techniques are applied to extend the well-established entity-component-system (ECS) pattern to overcome some of this pattern's deficits with respect to the implied state access. A walk-through of central implementation aspects of a multimodal (speech and gesture) VR-interface is used to highlight the techniques' benefits. This use-case is chosen as a prototypical example of complex architectures with multiple interacting subsystems found in many VR, AR and MR architectures. Finally, implementation hints are given, lessons learned regarding maintainability pointed-out, and performance implications discussed.","Authors":"Fischbach, M (Fischbach, Martin) ; Wiebusch, D (Wiebusch, Dennis) ; Latoschik, ME (Latoschik, Marc Erich)","Title":"Semantic Entity-Component State Management Techniques to Enhance Software Quality for Multimodal VR-Systems"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396396700006 PubMed ID: 26955060 ISSN: 2168-2267 eISSN: 2168-2275","Keywords":"Automatic threshold selection; change detection; joint dictionary learning; multitemporal remote sensing KeyWords Plus:UNSUPERVISED CHANGE DETECTION; HYPERSPECTRAL IMAGE CLASSIFICATION; REMOTE-SENSING IMAGES; FRAMEWORK; KERNELS; FUSION; MODEL","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Cybernetics","Journal Information":"IEEE TRANSACTIONS ON CYBERNETICS Volume: 47 Issue: 4 Pages: 884-897 DOI: 10.1109/TCYB.2016.2531179 Published: APR 2017","Abstract":"Change detection is one of the most important applications of remote sensing technology. It is a challenging task due to the obvious variations in the radiometric value of spectral signature and the limited capability of utilizing spectral information. In this paper, an improved sparse coding method for change detection is proposed. The intuition of the proposed method is that unchanged pixels in different images can be well reconstructed by the joint dictionary, which corresponds to knowledge of unchanged pixels, while changed pixels cannot. First, a query image pair is projected onto the joint dictionary to constitute the knowledge of unchanged pixels. Then reconstruction error is obtained to discriminate between the changed and unchanged pixels in the different images. To select the proper thresholds for determining changed regions, an automatic threshold selection strategy is presented by minimizing the reconstruction errors of the changed pixels. Adequate experiments on multispectral data have been tested, and the experimental results compared with the state- of- the- art methods prove the superiority of the proposed method. Contributions of the proposed method can be summarized as follows: 1) joint dictionary learning is proposed to explore the intrinsic information of different images for change detection. In this case, change detection can be transformed as a sparse representation problem. To the authors' knowledge, few publications utilize joint learning dictionary in change detection; 2) an automatic threshold selection strategy is presented, which minimizes the reconstruction errors of the changed pixels without the prior assumption of the spectral signature. As a result, the threshold value provided by the proposed method can adapt to different data due to the characteristic of joint dictionary learning; and 3) the proposed method makes no prior assumption of the modeling and the handling of the spectral signature, which can be adapted to different data.","Authors":"Lu, XQ (Lu, Xiaoqiang) ; Yuan, Y (Yuan, Yuan) ; Zheng, XT (Zheng, Xiangtao)","Title":"Joint Dictionary Learning for Multispectral Change Detection"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396401600004 ISSN: 2168-2291 eISSN: 2168-2305","Keywords":"Approximate sketch matching; computer vision; databases; sketch descriptors; sketch retrieval","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Cybernetics","Journal Information":"IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS Volume: 47 Issue: 2 Pages: 194-205 DOI: 10.1109/THMS.2017.2649684 Published: APR 2017","Abstract":"Searching is a necessary tool for managing and navigating the massive amounts of data available in today's information age. While new searching methods have become increasingly popular and reliable in recent years, such as image-based searching, these methods may be more limited than text-based means in that they do not allow generic user input. Sketch-based searching is a method that allows users to draw generic search queries and return similar drawn images, giving more user control over their search content. In this paper, we present SketchSeeker, a system for indexing and searching across a large number of sketches quickly based on their similarity. SketchSeeker introduces a technique for indexing sketches in extremely compressed representations, which allows for fast, accurate retrieval augmented with a multilevel ranking subsystem. SketchSeeker was tested on a large set of sketches against existing sketch similarity metrics, and it shows significant improvements in terms of storage requirements, speed, and accuracy.","Authors":"Polsley, S (Polsley, Seth) ; Ray, J (Ray, Jaideep) ; Hammond, T (Hammond, Tracy)","Title":"SketchSeeker: Finding Similar Sketches"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392682400014 ISSN: 0031-3203 eISSN: 1873-5142","Keywords":"Object matching; Object classification; Image classification; Partial Dominant Orientation Descriptor; K-Nearest Neighbors; Adaptive object distance KeyWords Plus:IMAGE CLASSIFICATION; SPARSE REPRESENTATION; SHAPE CLASSIFICATION; REGISTRATION; RECOGNITION; ALGORITHM; FEATURES; SPACE","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"PATTERN RECOGNITION Volume: 64 Pages: 168-186 DOI: 10.1016/j.patcog.2016.11.004 Published: APR 2017","Abstract":"This paper introduces a novel approach to measure the correspondence between objects, and exploit it for object and image classification tasks, using the proposed Partial Dominant Orientation Descriptor (PDOD). In particular, the object is represented by a set of stable and informative key locations sampled using Difference of Gaussian. The proposed PDOD at each extracted key location takes into account the position and partially computes the dominant orientation of other key locations relative to it, thus, offering a global distinctive and discriminative characterization. This allows us to learn features that are largely invariant to common image transformations, including changes in object colors and textures. The correspondence in-between two objects is performed by finding for each key location in one object the key location in the other object that has the most similar descriptor. Object classification proceeds by assigning the most relevant category that has maximally similar stored prototype objects to the query object using k-Nearest Neighbors algorithm with Adaptive Object Distance. For efficiency, we further investigate PDOD for image classification by developing powerful image representations based on the popular Bag-of-Words model. The extensive experiments demonstrate that the proposed approach greatly improves the matching and classification results, while achieving the state-of-the-art performances on several challenging benchmark datasets. The obtained results suggest also broader applicability to other classification modalities.","Authors":"Elboushaki, A (Elboushaki, Abdessamad) ; Hannane, R (Hannane, Rachida) ; Afdel, K (Afdel, Karim) ; Koutti, L (Koutti, Lahcen)","Title":"A robust approach for object matching and classification using Partial Dominant Orientation Descriptor"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394061800008 ISSN: 0925-2312 eISSN: 1872-8286","Keywords":"Diversity regularizations; Soft orthogonality; Learning to match; Multimodal retrieval; Representation learning; Approximate nearest neighbors","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"NEUROCOMPUTING Volume: 230 Pages: 77-87 DOI: 10.1016/j.neucom.2016.11.057 Published: MAR 22 2017","Abstract":"Hashing based approximate nearest neighbors (ANN) search has drawn considerable attraction owing to its low-memory storage and hardware-level logical computing which is doomed to be greatly applicable to quantities of large-scale and practical scenarios, such as information retrieval, computer vision and natural language processing. However, most existing hashing methods concentrate either on images only or on pairwise image-texts (labels, short documents) and rarely utilize more common sentences. In this paper, we propose Diversity Regularized Latent Semantic Match for Hashing (DRLSMH), a new multinlodal hashing method that projects images and sentences into a shared latent semantic spate with label-supervised semantic constraints to proceed on multimodal retrieval. Notably, soft orthogonality is induced as a novel regularizer to preserve diverse hashing functions for compact and accurate representations; what's more, this kind of regularization also benefits the derivations of closed-form solutions with some proper relaxations under iterative optimization framework. Extensive experiments on two public datasets demonstrate the advantages of our method over some state-of-the-art baselines under cross-modal retrieval both on image-query-image, image-query-text and text query-image tasks.","Authors":"Chen, Y (Chen, Yong) ; Zhang, H (Zhang, Hui) ; Tong, YX (Tong, Yongxin) ; Lu, M (Lu, Ming)","Title":"Diversity Regularized Latent Semantic Match for Hashing"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397532900002 ISSN: 1758-0463","Categories":"Mathematical & Computational Biology Web of Science Categories:Mathematical & Computational Biology","Journal Information":"DATABASE-THE JOURNAL OF BIOLOGICAL DATABASES AND CURATION Article Number: bax021 DOI: 10.1093/database/bax021 Published: MAR 18 2017","Abstract":"Bioinformatics sequence databases such as Genbank or UniProt contain hundreds of millions of records of genomic data. These records are derived from direct submissions from individual laboratories, as well as from bulk submissions from large-scale sequencing centres; their diversity and scale means that they suffer from a range of data quality issues including errors, discrepancies, redundancies, ambiguities, incompleteness and inconsistencies with the published literature. In this work, we seek to investigate and analyze the data quality of sequence databases from the perspective of a curator, who must detect anomalous and suspicious records. Specifically, we emphasize the detection of inconsistent records with respect to the literature. Focusing on GenBank, we propose a set of 24 quality indicators, which are based on treating a record as a query into the published literature, and then use query quality predictors. We then carry out an analysis that shows that the proposed quality indicators and the quality of the records have a mutual relationship, in which one depends on the other. We propose to represent recordliterature consistency as a vector of these quality indicators. By reducing the dimensionality of this representation for visualization purposes using principal component analysis, we show that records which have been reported as inconsistent with the literature fall roughly in the same area, and therefore share similar characteristics. By manually analyzing records not previously known to be erroneous that fall in the same area than records know to be inconsistent, we show that one record out of four is inconsistent with respect to the literature. This high density of inconsistent record opens the way towards the development of automatic methods for the detection of faulty records. We conclude that literature inconsistency is a meaningful strategy for identifying suspicious records.","Authors":"Bouadjenek, MR (Bouadjenek, Mohamed Reda) ; Verspoor, K (Verspoor, Karin) ; Zobel, J (Zobel, Justin)","Title":"Literature consistency of bioinformatics sequence databases is effective for assessing record quality"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392793700015 ISSN: 0378-4371 eISSN: 1873-2119","Keywords":"Geometric Clifford algebras; Probability theory; Quantum algorithms; Quantum mechanics; Riemannian geometry KeyWords Plus:ZERO-POINT ENERGY; FISHER INFORMATION; SPACETIME ALGEBRA; SEARCH ALGORITHM; ERROR-CORRECTION; MATHEMATICAL LANGUAGE; STATISTICAL DISTANCE; FACTORING ALGORITHM; GROVERS ALGORITHM; ENTROPIC ANALYSIS","Categories":"Physics Web of Science Categories:Physics, Multidisciplinary","Journal Information":"PHYSICA A-STATISTICAL MECHANICS AND ITS APPLICATIONS Volume: 470 Pages: 154-196 DOI: 10.1016/j.physa.2016.11.117 Published: MAR 15 2017","Abstract":"The art of quantum algorithm design is highly nontrivial. Grover's search algorithm constitutes a masterpiece of quantum computational software. In this article, we use methods of geometric algebra (GA) and information geometry (IG) to enhance the algebraic efficiency and the geometrical significance of the digital and analog representations of Grover's algorithm, respectively. Specifically, GA is used to describe the Grover iterate and the discretized iterative procedure that exploits quantum interference to amplify the probability amplitude of the target-state before measuring the query register. The transition from digital to analog descriptions occurs via Stone's theorem which relates the (unitary) Grover iterate to a suitable (Hermitian) Hamiltonian that controls Schrodinger's quantum mechanical evolution of a quantum state towards the target state. Once the discrete-to-continuos transition is completed, IG is used to interpret Grover's iterative procedure as a geodesic path on the manifold of the parametric density operators of pure quantum states constructed from the continuous approximation of the parametric quantum output state in Grover's algorithm. Finally, we discuss the dissipationless nature of quantum computing, recover the quadratic speedup relation, and identify the superfluity of the Walsh Hadamard operation from an IG perspective with emphasis on statistical mechanical considerations. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Cafaro, C (Cafaro, Carlo)","Title":"Geometric algebra and information geometry for quantum computational software"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397508400002 PubMed ID: 28284192 ISSN: 1471-2105","Keywords":"Explicit bioactivity; False discovery rate; Logistic regression; Mechanism-of-action target; Most-similar ligand; Target prediction KeyWords Plus:MOLECULAR SIMILARITY; DRUG ACTIVITY; ELECTROSHAPE; PHARMACOLOGY; MACHINE; AGENTS","Categories":"Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Mathematical & Computational Biology Web of Science Categories:Biochemical Research Methods; Biotechnology & Applied Microbiology; Mathematical & Computational Biology","Journal Information":"BMC BIOINFORMATICS Volume: 18 Article Number: 165 DOI: 10.1186/s12859-017-1586-z Published: MAR 11 2017","Abstract":"Background: Many computational approaches have been used for target prediction, including machine learning, reverse docking, bioactivity spectra analysis, and chemical similarity searching. Recent studies have suggested that chemical similarity searching may be driven by the most-similar ligand. However, the extent of bioactivity of most-similar ligands has been oversimplified or even neglected in these studies, and this has impaired the prediction power. Results: Here we propose the MOst-Similar ligand-based Target inference approach, namely MOST, which uses fingerprint similarity and explicit bioactivity of the most-similar ligands to predict targets of the query compound. Performance of MOST was evaluated by using combinations of different fingerprint schemes, machine learning methods, and bioactivity representations. In sevenfold cross-validation with a benchmark Ki dataset from CHEMBL release 19 containing 61,937 bioactivity data of 173 human targets, MOST achieved high average prediction accuracy (0.95 for pKi >= 5, and 0.87 for pKi >= 6). Morgan fingerprint was shown to be slightly better than FP2. Logistic Regression and Random Forest methods performed better than Naive Bayes. In a temporal validation, the Ki dataset from CHEMBL19 were used to train models and predict the bioactivity of newly deposited ligands in CHEMBL20. MOST also performed well with high accuracy (0.90 for pKi >= 5, and 0.76 for pKi >= 6), when Logistic Regression and Morgan fingerprint were employed. Furthermore, the p values associated with explicit bioactivity were found be a robust index for removing false positive predictions. Implicit bioactivity did not offer this capability. Finally, p values generated with Logistic Regression, Morgan fingerprint and explicit activity were integrated with a false discovery rate (FDR) control procedure to reduce false positives in multiple-target prediction scenario, and the success of this strategy it was demonstrated with a case of fluanisone. In the case of aloeemodin's laxative effect, MOST predicted that acetylcholinesterase was the mechanism-of-action target; in vivo studies validated this prediction. Conclusions: Using the MOST approach can result in highly accurate and robust target prediction. Integrated with a FDR control procedure, MOST provides a reliable framework for multiple-target inference. It has prospective applications in drug repurposing and mechanism-of-action target prediction.","Authors":"Huang, T (Huang, Tao) ; Mi, H (Mi, Hong) ; Lin, CY (Lin, Cheng-yuan) ; Zhao, L (Zhao, Ling) ; Zhong, LDLD (Zhong, Linda L. D.) ; Liu, FB (Liu, Feng-bin) ; Zhang, G (Zhang, Ge) ; Lu, AP (Lu, An-ping) ; Bian, ZX (Bian, Zhao-xiang) Group Author(s): MZRW Grp","Title":"MOST: most-similar ligand based approach to target prediction"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395876100001 ISSN: 1939-1404 eISSN: 2151-1535","Keywords":"Terms Big data; data integration; geographic information systems (GIS); LUCAS; synthetic aperture radar (SAR) KeyWords Plus:INFORMATION-SYSTEMS","Categories":"Engineering; Physical Geography; Remote Sensing; Imaging Science & Photographic Technology Web of Science Categories:Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology","Journal Information":"IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING Volume: 10 Issue: 3 Pages: 791-801 DOI: 10.1109/JSTARS.2017.2649040 Published: MAR 2017","Abstract":"The constantly growing process of the Earth Observation (EO) data and their heterogeneity require new systems and tools for effectively querying and understanding the available data archives. In this paper, we present a tool for heterogeneous geospatial data analytics. The system implements different web technologies in a multilayer server client architecture, allowing the user to visually analyze satellite images, maps, and in-situ information. Specifically, the information managed is composed of EO multispectral and synthetic aperture radar products along with the multitemporal in-situ LUCAS surveys. The integration of these data provides a very useful information during the EO scene interpretation process. The system also otters interactive tools for the detection of optimal datasets for EO multitemporal image change detection, providing at the same time ground-truth points for both human and machine analyses. Furthermore, we show by means of visual analytic representations a way to analyze and understand the content and distribution of the EO databases.","Authors":"Alonso, K (Alonso, Kevin) ; Espinoza-Molina, D (Espinoza-Molina, Daniela) ; Datcu, M (Datcu, Mihai)","Title":"Multilayer Architecture for Heterogeneous Geospatial Data Analytics: Querying and Understanding EO Archives"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000399017800025 ISSN: 1380-7501 eISSN: 1573-7721","Keywords":"3D model retrieval; Model segmentation; Labelled attribute adjacency graph; Spectral graph theory; Bag of words; Spatial relation KeyWords Plus:DECOMPOSITION; RECOGNITION; SHAPES","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"MULTIMEDIA TOOLS AND APPLICATIONS Volume: 76 Issue: 6 Pages: 8145-8173 DOI: 10.1007/s11042-016-3456-5 Published: MAR 2017","Abstract":"In order to improve both the discriminative power for models' local parts and the searching efficiency in 3D CAD model retrieval, a novel hierarchical feature descriptor for retrieval based on spatial bag of words is proposed in this paper. By extracting the essential information from Boundary Representation (B-Rep), 3D CAD models are transformed to Labelled Attribute Adjacency Graphs (LAAGs). Next, the models in training dataset are segmented into different regions according to their corresponding LAAG with an improved segmentation method. All collections of these local regions are described as local feature vectors with graph spectrum, and the codebook is created by clustering all these vectors. Each library model is then decomposed with the same methods mentioned above and globally represented as a spatial histogram of word pairs along with the adjacent relations of its regions, called Spatial Bags-of-Words (SBoWs), and then, the hierarchical feature descriptor(HFD) of each library model composed of global SBoWs and local graph spectrum is constructed. Finally, according to HFD, a two-level searching framework is presented for CAD model retrieval: the candidates are acquired by comparing the query with each target model based on their SBoWs vectors, and the remaining candidates are verified using optimal matching algorithm according to their local features. Experimental results show that the proposed methods promote both retrieval quality and efficiency significantly, so they can support the effective reuse of CAD models for enterprises.","Authors":"Huangfu, ZM (Huangfu, Zhong-Min) ; Zhang, SS (Zhang, Shu-Sheng) ; Yan, LH (Yan, Luo-Heng)","Title":"A method of 3D CAD model retrieval based on spatial bag of words"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397687900003 ISSN: 0262-8856 eISSN: 1872-8138","Keywords":"Pose retrieval; Pose estimation; Video and image retrieval; Deep networks KeyWords Plus:SIMILARITY","Categories":"Computer Science; Engineering; Optics Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic; Optics","Journal Information":"IMAGE AND VISION COMPUTING Volume: 59 Pages: 31-43 DOI: 10.1016/j.imavis.2016.12.002 Published: MAR 2017","Abstract":"Human pose as a query modality is an alternative and rich experience for image and video retrieval. It has interesting retrieval applications in domains such as sports and dance databases. In this work we propose two novel ways for representing the image of a person striking a pose, one looking for parts and other looking at the whole image. These representations are then used for retrieval. Both the representations are obtained using deep learning methods. In the first method, we make the following contributions: (a) We introduce 'deep poselets' for pose sensitive detection of various body parts, built on convolutional neural network (CNN) features. These deep poselets significantly outperform previous instantiations of Berkeley poselets [6], and (b) Using these detector responses, we construct a pose representation that is suitable for pose search, and show that pose retrieval performance is on par with the previous methods. In the second method, we make the following contributions: (a) We design an optimized neural network which maps the input image to a very low dimensional space where similar poses are close by and dissimilar poses are farther away, and (b) We show that pose retrieval system using these low dimensional representation is on par with the deep poselet representation and is on par with the previous methods. The previous works with which the above two methods are compared include bag of visual words [44], Berkeley poselets [6] and human pose estimation algorithms [52]. All the methods are quantitatively evaluated on a large dataset of images built from a number of standard benchmarks together with frames from Hollywood movies. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Jammalamadaka, N (Jammalamadaka, Nataraj) ; Zisserman, A (Zisserman, Andrew) ; Jawahar, CV (Jawahar, C. V.)","Title":"Human pose search using deep networks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397124300002 ISSN: 0362-5915 eISSN: 1557-4644","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems; Computer Science, Software Engineering","Journal Information":"ACM TRANSACTIONS ON DATABASE SYSTEMS Volume: 42 Issue: 1 Article Number: 1 DOI: 10.1145/2984632 Published: MAR 2017","Abstract":"We prove exponential lower bounds on the running time of the state-of-the-art exact model counting algorithms-algorithms for exactly computing the number of satisfying assignments, or the satisfying probability, of Boolean formulas. These algorithms can be seen, either directly or indirectly, as building Decision-Decomposable Negation Normal Form (decision-DNNF) representations of the input Boolean formulas. Decision-DNNFs are a special case of d-DNNFs where d stands for deterministic. We show that any knowledge compilation representations from a class (called DLDDs in this article) that contain decisionDNNFs can be converted into equivalent Free Binary Decision Diagrams (FBDDs), also known as Read-Once Branching Programs, with only a quasi-polynomial increase in representation size. Leveraging known exponential lower bounds for FBDDs, we then obtain similar exponential lower bounds for decision-DNNFs, which imply exponential lower bounds for model-counting algorithms. We also separate the power of decisionDNNFs from d-DNNFs and a generalization of decision-DNNFs known as AND-FBDDs. We then prove new lower bounds for FBDDs that yield exponential lower bounds on the running time of these exact model counters when applied to the problem of query evaluation in tuple-independent probabilistic databases-computing the probability of an answer to a query given independent probabilities of the individual tuples in a database instance. This approach to the query evaluation problem, in which one first obtains the lineage for the query and database instance as a Boolean formula and then performs weighted model counting on the lineage, is known as grounded inference. A second approach, known as lifted inference or extensional query evaluation, exploits the high-level structure of the query as a first-order formula. Although it has been widely believed that lifted inference is strictly more powerful than grounded inference on the lineage alone, no formal separation has previously been shown for query evaluation. In this article, we show such a formal separation for the first time. In particular, we exhibit a family of database queries for which polynomial-time extensional query evaluation techniques were previously known but for which query evaluation via grounded inference using the state-of-the-art exact model counters requires exponential time.","Authors":"Beame, P (Beame, Paul) ; Li, J (Li, Jerry) ; Roy, S (Roy, Sudeepa) ; Suciu, D (Suciu, Dan)","Title":"Exact Model Counting of Query Expressions: Limitations of Propositional Methods"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397124300007 ISSN: 0362-5915 eISSN: 1557-4644","Keywords":"Ad hoc data processing; query optimization; program semantics KeyWords Plus:QUERIES","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems; Computer Science, Software Engineering","Journal Information":"ACM TRANSACTIONS ON DATABASE SYSTEMS Volume: 42 Issue: 1 Article Number: 6 DOI: 10.1145/3009957 Published: MAR 2017","Abstract":"Instead of constructing complex declarative queries, many users prefer to write their programs using procedural code embedded with simple queries. Since many users are not expert programmers or the programs are written in a rush, these programs usually exhibit poor performance in practice and it is a challenge to automatically and efficiently optimize these programs. In this article, we present UniAD, which stands for Unified execution for Ad hoc Data processing, a system designed to simplify the programming of data processing tasks and provide efficient execution for user programs. We provide the background of program semantics and propose a novel intermediate representation, called Unified Intermediate Representation (UniIR), which utilizes a simple and expressive mechanism HOQ to describe the operations performed in programs. By combining both procedural and declarative logics with the proposed intermediate representation, we can perform various optimizations across the boundary between procedural and declarative code. We propose a transformation-based optimizer to automatically optimize programs and implement the UniAD system. The extensive experimental results on various benchmarks demonstrate that our techniques can significantly improve the performance of a wide range of data processing programs.","Authors":"Shi, XG (Shi, Xiaogang) ; Cui, B (Cui, Bin) ; Dobbie, G (Dobbie, Gillian) ; Ooi, BC (Ooi, Beng Chin)","Title":"A Unified Ad Hoc Data Processing System"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394500300032 ISSN: 0268-3768 eISSN: 1433-3015","Keywords":"Machining features; Automatic feature recognition; Interacting features; Semantic representation; Knowledge reasoning KeyWords Plus:MANUFACTURING FEATURES; FRAMEWORK; MODEL; SYSTEM","Categories":"Automation & Control Systems; Engineering Web of Science Categories:Automation & Control Systems; Engineering, Manufacturing","Journal Information":"INTERNATIONAL JOURNAL OF ADVANCED MANUFACTURING TECHNOLOGY Volume: 89 Issue: 1-4 Pages: 417-437 DOI: 10.1007/s00170-016-9056-8 Published: MAR 2017","Abstract":"Machining features contain considerable implicit semantic information on shape and machining processes and are dependent on a specific application domain. It is necessary to research and develop an open, shared, and scalable semantic approach to the automatic recognition of machining features. In this paper, the concepts of machining faces and machining features are analyzed, and a novel semantic approach to the automatic recognition of machining features is proposed. The semantic approach provides an ontology-based concept model for representing the machining faces and machining features. The implicit semantics of machining faces and machining features are defined by a set of explicit Semantics Web Rule Language (SWRL) rules. All of the geometric surfaces to be machined are annotated as a set of instances of the face concept and a set of semantic relationships between them, which constitute the fact base for semantic reasoning. Furthermore, an approach to automatic feature recognition based on semantic query and reasoning is proposed. A case study demonstrates that the presented approach can effectively recognize and interpret interacting features and has good openness and scalability.","Authors":"Zhang, YZ (Zhang, Yingzhong) ; Luo, XF (Luo, Xiaofang) ; Zhang, BY (Zhang, Baiyun) ; Zhang, SH (Zhang, Shaohua)","Title":"Semantic approach to the automatic recognition of machining features"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394524400008 ISSN: 0963-9314 eISSN: 1573-1367","Keywords":"Software debugging; Declarative debugging; Execution trace analysis KeyWords Plus:QUERY","Categories":"Computer Science Web of Science Categories:Computer Science, Software Engineering","Journal Information":"SOFTWARE QUALITY JOURNAL Volume: 25 Issue: 1 Pages: 201-229 DOI: 10.1007/s11219-016-9311-0 Published: MAR 2017","Abstract":"With newer complex multi-core systems, it is important to understand an application's runtime behavior to be able to debug its execution, detect possible problems and bottlenecks and finally identify potential root causes. Execution traces usually contain precise data about an application execution. Their analysis and abstraction at multiple levels can provide valuable information and insights about an application's runtime behavior. However, with multiple abstraction levels, it becomes increasingly difficult to find the exact location of detected performance or security problems. Tracing tools provide various analysis views to help users to understand their application problems. However, these pre-defined views are often not sufficient to reveal all analysis aspects of the underlying application. A declarative approach that enables users to specify and build their own custom analysis and views based on their knowledge, requirements and problems can be more useful and effective. In this paper, we propose a generic declarative trace analysis framework to analyze, comprehend and visualize execution traces. This enhanced framework builds custom analyses based on a specified modeled state, extracted from a system execution trace and stored in a special purpose database. The proposed solution enables users to first define their different analysis models based on their application and requirements, then visualize these models in many alternate representations (Gantt chart, XY chart, etc.), and finally filter the data to get some highlights or detect some potential patterns. Several sample applications with different operating systems are shown, using trace events gathered from Linux and Windows, at the kernel and user-space levels.","Authors":"Wininger, F (Wininger, Florian) ; Ezzati-Jivan, N (Ezzati-Jivan, Naser) ; Dagenais, MR (Dagenais, Michel R.)","Title":"A declarative framework for stateful analysis of execution traces"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395896500095 ISSN: 1568-4946 eISSN: 1872-9681","Keywords":"Hybrid methods; Semi-supervised learning; Classification; Summit-training; Query-by-Committee; Unlabeled data KeyWords Plus:CLASSIFICATION; ALGORITHM; SVM","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications","Journal Information":"APPLIED SOFT COMPUTING Volume: 52 Pages: 1296-1315 DOI: 10.1016/j.asoc.2016.05.041 Published: MAR 2017","Abstract":"With an explosion in the amount of data available for classification problems it becomes more and more complicated to obtain labels for the whole dataset. Because of this we are frequently presented with only a fraction of labeled data from the whole dataset. To leverage the presence of unlabeled examples Semi-Supervised methods could be used to increase the precision of used classifiers. Therefore, we propose a novel hybrid technique which extends the concept of Self-Training and Help-Training used in Semi-Supervised techniques by incorporating the Active Learning approach for determining the confidence of the classifier in the testing set samples. Specifically we employ the Query-by-Committee (QbC) approach and we call the final method Summit-Training. We apply this method to a range of classifiers(generative and discriminative) and evaluate it on several datasets and real world problems. The formulation of the Summit-Training method especially allows us to use Semi-Supervised approaches for purely discriminative classifiers in which no probabilistic representation of the evaluated classes exists. Compared to other Semi-Supervised techniques (Self-Training and Help-Training), the proposed new method achieves superior performance It also has better generalization properties since it reduces the number of hyper-parameters and relaxes the conditions for classifiers on which it could be applied. (C) 2016 Published by Elsevier B. V.","Authors":"Tencer, L (Tencer, L.) ; Reznakova, M (Reznakova, M.) ; Cheriet, M (Cheriet, M.)","Title":"UFuzzy: Fuzzy Models with Universum"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394630400016 ISSN: 0004-3702 eISSN: 1872-7921","Keywords":"Constraint programming; Constraint learning KeyWords Plus:CONSISTENCY; QUERIES","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"ARTIFICIAL INTELLIGENCE Volume: 244 Pages: 315-342 DOI: 10.1016/j.artint.2015.08.001 Published: MAR 2017","Abstract":"Constraint programming is used to model and solve complex combinatorial problems. The modeling task requires some expertise in constraint programming. This requirement is a bottleneck to the broader uptake of constraint technology. Several approaches have been proposed to assist the non-expert user in the modeling task. This paper presents the basic architecture for acquiring constraint networks from examples classified by the user. The theoretical questions raised by constraint acquisition are stated and their complexity is given. We then propose CONACQ, a system that uses a concise representation of the learner's version space into a clausal formula. Based on this logical representation, our architecture uses strategies for eliciting constraint networks in both the passive acquisition context, where the learner is only provided a pool of examples, and the active acquisition context, where the learner is allowed to ask membership queries to the user. The computational properties of our strategies are analyzed and their practical effectiveness is experimentally evaluated. (C) 2015 Elsevier By. All rights reserved.","Authors":"Bessiere, C (Bessiere, Christian) ; Koriche, F (Koriche, Frederic) ; Lazaar, N (Lazaar, Nadjib) ; O'Sullivan, B (O'Sullivan, Barry)","Title":"Constraint acquisition"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395359500009 ISSN: 0164-1212 eISSN: 1873-1228","Keywords":"Component-based software engineering; Semantic Web; Ontology; Reasoners; Web services; Linked Data KeyWords Plus:CHALLENGES; SERVICES; TECHNOLOGIES; STANDARDS; ONTOLOGY","Categories":"Computer Science Web of Science Categories:Computer Science, Software Engineering; Computer Science, Theory & Methods","Journal Information":"JOURNAL OF SYSTEMS AND SOFTWARE Volume: 125 Pages: 152-169 DOI: 10.1016/j.jss.2016.11.028 Published: MAR 2017","Abstract":"With the advent of Component-based software engineering (CBSE), large software systems are being built by integrating pre-built software components. The Semantic Web in association with CBSE has shown to offer powerful representation facilities and reasoning techniques to enhance and support querying, reasoning, discovery, etc. of software components. The goal of this paper is to research the applicability of Semantic Web technologies in performing the various tasks of CBSE and review the experimental results of the same in an easy and effective manner. To the best of our knowledge, this is the first study which provides an extensive review of the application of Semantic Web in CBSE from different perspectives. A systematic literature review of the Semantic Web approaches, employed for use in CBSE, reported from 2001 until 2015, is conducted in this research article. Empirical results have been drawn through the question-answer based analysis of the research, which clearly tells the year wise trend of the research articles, with the possible justification of the usage of Semantic Web technology and tools for a particular phase of CBSE. To conclude, gaps in the current research and potential future prospects have been discussed. (C) 2016 Elsevier Inc. All rights reserved.","Authors":"Kaur, L (Kaur, Loveleen) ; Mishra, A (Mishra, Ashutosh)","Title":"Software component and the semantic Web: An in-depth content analysis and integration history"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394424900021 ISSN: 1863-1703 eISSN: 1863-1711","Keywords":"Content-based video retrieval; Perceptual video synopsis; Feature extraction; Mutual information; Motion trajectory KeyWords Plus:IMAGE RETRIEVAL; TEXTURE FEATURES; REPRESENTATION; OBJECT; COLOR; QUERY","Categories":"Engineering; Imaging Science & Photographic Technology Web of Science Categories:Engineering, Electrical & Electronic; Imaging Science & Photographic Technology","Journal Information":"SIGNAL IMAGE AND VIDEO PROCESSING Volume: 11 Issue: 3 Pages: 549-555 DOI: 10.1007/s11760-016-0993-3 Published: MAR 2017","Abstract":"Content-based video retrieval and video synopsis are generally considered as two different areas. In this paper, we present an efficient approach for video retrieval based on the perceptual synopsis database of the videos. Video synopsis encapsulates an overview of a shot in a single frame. This is the first time video synopsis is used for video indexing providing the user an intuitive link for accessing actions in the video. We propose an enhanced synopsis called meta synopsis for the video database index, which will contain all essential information for retrieval. Various information such as background of a scene, motion trajectory of the foreground objects, color, texture, and mutual information in the synopsis database will empower us to retrieve relevant video content from huge video databases. Experiments were conducted on theOVP, BBC Motion Gallery, TRECVID data set, and other videos. Instead of using key frames as the query frames, the method accepts any arbitrary query frames. The experimental results illustrate that our proposed method can accurately identify a pertinent video from huge video databases.","Authors":"Thomas, SS (Thomas, Sinnu Susan) ; Gupta, S (Gupta, Sumana) ; Venkatesh, KS (Venkatesh, K. S.)","Title":"Perceptual synoptic view-based video retrieval using metadata"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394464600003 ISSN: 0947-3602 eISSN: 1432-010X","Keywords":"Mobile app requirements; Mobile interaction; Linked Data; Requirements modelling; Semantic traceability KeyWords Plus:FRAMEWORK","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems; Computer Science, Software Engineering","Journal Information":"REQUIREMENTS ENGINEERING Volume: 22 Issue: 1 Pages: 41-75 DOI: 10.1007/s00766-015-0235-1 Published: MAR 2017","Abstract":"The paper presents a modelling method aimed to support the definition and elicitation of requirements for mobile apps through an approach that enables semantic traceability for the requirements representation. Business process-centricity is employed in order to capture requirements in a knowledge structure that retains procedural knowledge from stakeholders and can be traversed by semantic queries in order to trace domain-specific contextual information for the modelled requirements. Consequently, instead of having requirements represented as natural language items that are documented by diagrammatic models, the communication channels are switched: semantically interlinked conceptual models become the requirements representation, while free text can be used for requirements annotations/metadata. Thus, the method establishes a knowledge externalization channel between business stakeholders and app developers, also tackling the Twin Peaks bridging challenge (between requirements and early designs). The method is presented using its modelling procedure as a guiding thread, with each step illustrated by case-based samples of the modelling language and auxiliary functionality. The design work is encompassed by an existing metamodelling framework and introduces a taxonomy for modelling relations, since the metamodel is the key enabler for the goal of semantic traceability. The research was driven by the ComVantage EU research project, concerned with mobile app support for collaborative business process execution. Therefore, the project provides context for the illustrating examples; however, generalization possibilities beyond the project scope will also be discussed, with respect to both motivation and outcome.","Authors":"Buchmann, RA (Buchmann, Robert Andrei) ; Karagiannis, D (Karagiannis, Dimitris)","Title":"Modelling mobile app requirements for semantic traceability"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394341300002 ISSN: 1384-5810 eISSN: 1573-756X","Keywords":"Graph analysis; Cut-norm; Approximation algorithms; Summarization; Regularity lemma KeyWords Plus:FACILITY LOCATION; REGULARITY LEMMA; APPROXIMATION; ALGORITHMS; NETWORKS; BOUNDS","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems","Journal Information":"DATA MINING AND KNOWLEDGE DISCOVERY Volume: 31 Issue: 2 Pages: 314-349 DOI: 10.1007/s10618-016-0468-8 Published: MAR 2017","Abstract":"We study the problem of graph summarization. Given a large graph we aim at producing a concise lossy representation (a summary) that can be stored in main memory and used to approximately answer queries about the original graph much faster than by using the exact representation. In this work we study a very natural type of summary: the original set of vertices is partitioned into a small number of supernodes connected by superedges to form a complete weighted graph. The superedge weights are the edge densities between vertices in the corresponding supernodes. To quantify the dissimilarity between the original graph and a summary, we adopt the reconstruction error and the cut-norm error. By exposing a connection between graph summarization and geometric clustering problems (i.e., k-means and k-median), we develop the first polynomial-time approximation algorithms to compute the best possible summary of a certain size under both measures. We discuss how to use our summaries to store a (lossy or lossless) compressed graph representation and to approximately answer a large class of queries about the original graph, including adjacency, degree, eigenvector centrality, and triangle and subgraph counting. Using the summary to answer queries is very efficient as the running time to compute the answer depends on the number of supernodes in the summary, rather than the number of nodes in the original graph.","Authors":"Riondato, M (Riondato, Matteo) ; Garcia-Soriano, D (Garcia-Soriano, David) ; Bonchi, F (Bonchi, Francesco)","Title":"Graph summarization with quality guarantees"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395837700026 ISSN: 1057-7149 eISSN: 1941-0042","Keywords":"Landmark photo retrieval; multi-query expansions; collaborative deep networks KeyWords Plus:IMAGE CLASSIFICATION","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON IMAGE PROCESSING Volume: 26 Issue: 3 Pages: 1393-1404 DOI: 10.1109/TIP.2017.2655449 Published: MAR 2017","Abstract":"Given a query photo issued by a user (q-user), the landmark retrieval is to return a set of photos with their landmarks similar to those of the query, while the existing studies on the landmark retrieval focus on exploiting geometries of landmarks for similarity matches between candidate photos and a query photo. We observe that the same landmarks provided by different users over social media community may convey different geometry information depending on the viewpoints and/or angles, and may, subsequently, yield very different results. In fact, dealing with the landmarks with low quality shapes caused by the photography of q-users is often nontrivial and has seldom been studied. In this paper, we propose a novel framework, namely, multi-query expansions, to retrieve semantically robust landmarks by two steps. First, we identify the top-k photos regarding the latent topics of a query landmark to construct multi-query set so as to remedy its possible low quality shape. For this purpose, we significantly extend the techniques of Latent Dirichlet Allocation. Then, motivated by the typical collaborative filtering methods, we propose to learn a collaborative deep networks-based semantically, nonlinear, and high-level features over the latent factor for landmark photo as the training set, which is formed by matrix factorization over collaborative user-photo matrix regarding the multi-query set. The learned deep network is further applied to generate the features for all the other photos, meanwhile resulting into a compact multi-query set within such space. Then, the final ranking scores are calculated over the high-level feature space between the multi-query set and all other photos, which are ranked to serve as the final ranking list of landmark retrieval. Extensive experiments are conducted on real-world social media data with both landmark photos together with their user information to show the superior performance over the existing methods, especially our recently proposed multiquery based mid-level pattern representation method [1].","Authors":"Wang, Y (Wang, Yang) ; Lin, XM (Lin, Xuemin) ; Wu, L (Wu, Lin) ; Zhang, WJ (Zhang, Wenjie)","Title":"Effective Multi-Query Expansions: Collaborative Deep Networks for Robust Landmark Retrieval"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394103100005 ISSN: 0178-4617 eISSN: 1432-0541","Keywords":"External memory algorithms; Path traversal; Planar graphs; Succinct data structures KeyWords Plus:TREES; REPRESENTATIONS; TRIANGULATIONS; BLOCKING","Categories":"Computer Science; Mathematics Web of Science Categories:Computer Science, Software Engineering; Mathematics, Applied","Journal Information":"ALGORITHMICA Volume: 77 Issue: 3 Pages: 714-755 DOI: 10.1007/s00453-015-0086-7 Published: MAR 2017","Abstract":"We present a technique for representing bounded-degree planar graphs in a succinct fashion while permitting I/O-efficient traversal of paths. Using our representation, a graph with N vertices, (In this paper denotes ) each with an associated key of bits, can be stored in bits and traversing a path of length K takes I/Os, where B denotes the disk block size. By applying our construction to the dual of a terrain represented as a triangular irregular network, we can represent the terrain in the above space bounds and support path traversals on the terrain using I/Os, where K is the number of triangles visited by the path. This is useful for answering a number of queries on the terrain, such as reporting terrain profiles, trickle paths, and connected components.","Authors":"Dillabaugh, C (Dillabaugh, Craig) ; He, M (He, Meng) ; Maheshwari, A (Maheshwari, Anil) ; Zeh, N (Zeh, Norbert)","Title":"I/O-Efficient Path Traversal in Succinct Planar Graphs"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393241500002 ISSN: 0306-4573 eISSN: 1873-5371","Keywords":"Medical information retrieval; Domain expertise; Query formulations; Relevance assessment; Retrieval performance KeyWords Plus:EVIDENCE-BASED MEDICINE; WEB SEARCH; BELIEF DYNAMICS; PATIENT-CARE; QUESTIONS; CATEGORIZATION; INFORMATION; PHYSICIANS; CRITERIA; MEDLINE","Categories":"Computer Science; Information Science & Library Science Web of Science Categories:Computer Science, Information Systems; Information Science & Library Science","Journal Information":"INFORMATION PROCESSING & MANAGEMENT Volume: 53 Issue: 2 Pages: 332-350 DOI: 10.1016/j.ipm.2016.11.004 Published: MAR 2017","Abstract":"The large volumes of medical information available on the web may provide answers for a wide range of users attempting to solve health-related problems. While experts generally utilize reliable resources for diagnosis search and professional development, novices utilize different (social) web resources to obtain information that helps them manage their health or the health of people who they care for. A diverse number of related search topics address clinical diagnosis, advice searching, information sharing, connecting with experts, etc. This paper focuses on the extent to which expertise can impact clinical query formulation, document relevance assessment and retrieval performance in the context of tailoring retrieval models and systems to experts vs. non-experts. The results show that medical domain expertise 1) plays an important role in the lexical representations of information needs; 2) significantly influences the perception of relevance even among users with similar levels of expertise and 3) reinforces the idea that a single ground truth does not exist, thereby leading to the variability of system rankings with respect to the level of user's expertise. The findings of this study presents opportunities for the design of personalized health-related IR systems, but also for providing insights about the evaluation of such systems. (C) 2016 Elsevier Ltd. All rights reserved.","Authors":"Tamine, L (Tamine, Lynda) ; Chouquet, C (Chouquet, Cecile)","Title":"On the impact of domain expertise on query formulation, relevance assessment and retrieval performance in clinical settings"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392374300008 ISSN: 1012-2443 eISSN: 1573-7470","Keywords":"Probabilistic knowledge representation; Principle of maximum entropy; Computer algebra; Grobner basis KeyWords Plus:LOGICS","Categories":"Computer Science; Mathematics Web of Science Categories:Computer Science, Artificial Intelligence; Mathematics, Applied","Journal Information":"ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE Volume: 79 Issue: 1-3 Pages: 163-179 DOI: 10.1007/s10472-015-9457-7 Published: MAR 2017","Abstract":"An often used methodology for reasoning with probabilistic conditional knowledge bases is provided by the principle of maximum entropy (so-called MaxEnt principle) that realises an idea of least amount of assumed information and thus of being as unbiased as possible. In this paper we exploit the fact that MaxEnt distributions can be computed by solving nonlinear equation systems that reflect the conditional logical structure of these distributions. We apply the theory of Grobner bases that is well known from computational algebra to the polynomial system which is associated with a MaxEnt distribution, in order to obtain results for reasoning with maximum entropy. We develop a three-phase compilation scheme extracting from a knowledge base consisting of probabilistic conditionals the information which is crucial for MaxEnt reasoning and transforming it to a Grobner basis. Based on this transformation, a necessary condition for knowledge bases to be consistent is derived. Furthermore, approaches to answering MaxEnt queries are presented by demonstrating how inferring the MaxEnt probability of a single conditional from a given knowledge base is possible. Finally, we discuss computational methods to establish general MaxEnt inference rules.","Authors":"Kern-Isberner, G (Kern-Isberner, Gabriele) ; Wilhelm, M (Wilhelm, Marco) ; Beierle, C (Beierle, Christoph)","Title":"Probabilistic knowledge representation using the principle of maximum entropy and Grobner basis theory"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000390070800008 ISSN: 0167-739X eISSN: 1872-7115","Keywords":"Scientific workflows; Software product line; Workflow algebra; Workflow derivation KeyWords Plus:PROVENANCE; IMPLEMENTATION; INTEGRATION; TOOL","Categories":"Computer Science Web of Science Categories:Computer Science, Theory & Methods","Journal Information":"FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE Volume: 68 Pages: 111-127 DOI: 10.1016/j.future.2016.08.016 Published: MAR 2017","Abstract":"The exploratory nature of a scientific computational experiment involves executing variations of the same workflow with different approaches, programs, and parameters. However, current approaches do not systematize the derivation process from the experiment definition to the concrete workflows and do not track the experiment provenance down to the workflow executions. Therefore, the composition, execution, and analysis for the entire experiment become a complex task. To address this issue, we propose the Algebraic Experiment Line (AEL). AEL uses a data-centric workflow algebra, which enriches the experiment representation by introducing a uniform data model and its corresponding operators. This representation and the AEL provenance model map concepts from the workflow execution data to the AEL derived workflows with their corresponding experiment abstract definitions. We show how AEL has improved the understanding of a real experiment in the bioinformatics area. By combining provenance data from the experiment and its corresponding executions, AEL provenance queries navigate from experiment concepts defined at high abstraction level to derived workflows and their execution data. It also shows a direct way of querying results from different trials involving activity variations and optionalities, only present at the experiment level of abstraction. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Marinho, A (Marinho, Anderson) ; de Oliveira, D (de Oliveira, Daniel) ; Ogasawara, E (Ogasawara, Eduardo) ; Silva, V (Silva, Vitor) ; Ocana, K (Ocana, Kary) ; Murta, L (Murta, Leonardo) ; Braganholo, V (Braganholo, Vanessa) ; Mattoso, M (Mattoso, Marta)","Title":"Deriving scientific workflows from algebraic experiment lines: A practical approach"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000389785900017 ISSN: 0031-3203 eISSN: 1873-5142","Keywords":"Sparse representation; Image set classification; Dictionary learning; Scene classification KeyWords Plus:FACE RECOGNITION; CATEGORIZATION; APPEARANCE","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"PATTERN RECOGNITION Volume: 63 Pages: 206-217 DOI: 10.1016/j.patcog.2016.09.043 Published: MAR 2017","Abstract":"Image set classification has been widely applied to many real-life scenarios including surveillance videos, multi view camera networks and personal albums. Compared with single image based classification, it is more promising and therefore has attracted significant research attention in recent years. Traditional (forward) sparse representation (fSR) just makes use of training images to represent query ones. If we can find complementary information from backward sparse representation (bSR) which represents query images with training ones, the performance will be likely to be improved. However, for image set classification, the way to produce additional bases for bSR is a problem concerned as there is no other bases than the query set itself. In this paper, we extend cooperative sparse representation (CoSR) method, which integrates fSR and bSR together, to image set classification. In this process, we propose two schemes, namely 'Learning Bases' and \"Training Sets Division', to produce the additional dictionary for bSR. And different from previous work, our work considers scene classification as a problem of image set classification, which will provide a new insight for scene classification. Experimental results show that the proposed model can obtain competitive recognition rates for image set classification. By combining information from these two opposite SRs, better results can be achieved. Also the feasibility for the formulation of image set classification on scene classification is validated.","Authors":"Zheng, P (Zheng, Peng) ; Zhao, ZQ (Zhao, Zhong-Qiu) ; Gao, J (Gao, Jun) ; Wu, XD (Wu, Xindong)","Title":"Image set classification based on cooperative sparse representation"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394191100002 ISSN: 0097-8493 eISSN: 1873-7684","Keywords":"Eye-tracking data; Visual analytics; Graph layout; Saccade outlier detection; Repeated scanpath detection; Participant comparison and clustering KeyWords Plus:WANDERING MIND; MOVEMENTS; PERFORMANCE; ATTENTION; GAZE","Categories":"Computer Science Web of Science Categories:Computer Science, Software Engineering","Journal Information":"COMPUTERS & GRAPHICS-UK Volume: 62 Pages: 1-14 DOI: 10.1016/j.cag.2016.11.001 Published: FEB 2017","Abstract":"Mind wander(ing) (MW) or zoning out is a ubiquitous phenomenon where attention involuntary shifts from task-related processing to task-unrelated thoughts. Unfortunately, MW is a highly internal state so it cannot be readily inferred from overt behaviors and expressions. To help experts investigate mind wanderings, we present a graph-based approach for visual analytics of eye-tracking data, which utilizes the graph representations to illustrate the reading patterns and further help experts detect and verify mind wanderings based on the graph structures and other graph attributes. The input data are collected from multiple participants reading multiple pages of a book on a computer screen. Our approach first clusters fixations into fixation clusters, then creates the eye-tracking graph, i.e., ETGraph, for use in conjunction with the standard page view, time view, and statistics view. The graph view presents a visual representation of the actual reading patterns of a single participant or multiple participants and therefore serves as the main visual interface for exploration and navigation. We design a suite of techniques to help users identify common reading patterns and outliers for analytical reasoning at three different levels of detail: single participant single page, single participant multiple pages, and multiple participants single page. Interactive querying and filtering functions are provided for reducing visual clutter in the visualization and enabling users to answer questions and glean insights. Our tool also facilitates the detection and verification of mind wandering that the experts seek to investigate. We conduct a user study and an expert evaluation to assess the effectiveness of ETGraph in terms of its visual summarization and comparison capabilities. (C) 2016 Elsevier Ltd. All rights reserved.","Authors":"Gu, Y (Gu, Yi) ; Wang, CL (Wang, Chaoli) ; Bixler, R (Bixler, Robert) ; D'Mello, S (D'Mello, Sidney)","Title":"ETGraph: A graph-based approach for visual analytics of eye-tracking data"}, {"Document Information":"Document Type:Review Language:English Accession Number: WOS:000394618100004 ISSN: 0266-4720 eISSN: 1468-0394","Keywords":"dimensionality reduction; Haar wavelets; multi-resolution; time series; vector quantization","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Theory & Methods","Journal Information":"EXPERT SYSTEMS Volume: 34 Issue: 1 Article Number: UNSP e12171 DOI: 10.1111/exsy.12171 Published: FEB 2017","Abstract":"Time series representation methods are widely used to handle time series data by projecting them onto low-dimensional spaces where queries are processed. Multi-resolution representation methods speed up the similarity search process by using pre-computed distances, which are calculated and stored at the indexing stage and then used at the query stage, together with filters in the form of exclusion conditions. In this paper, we present a new multi-resolution representation method that combines the Haar wavelet-based multi-resolution method with vector quantization to maximize the pruning power of the similarity search algorithm. The new method is validated through extensive experiments on different datasets from several time series repositories. The results obtained prove the efficiency of the new method.","Authors":"Fuad, MMM (Fuad, Muhammad Marwan Muhammad)","Title":"Aggressive pruning strategy for time series retrieval using a multi-resolution representation based on vector quantization coupled with discrete wavelet transform"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395553000003 ISSN: 1751-8806 eISSN: 1751-8814","Keywords":"software prototyping; software quality; learning (artificial intelligence); knowledge representation; statistical analysis; probability; fuzzy set theory; decision making; optimisation; product correlation behaviour identification; product individual characteristics identification; feedback queries; integration index; usability goals achievement metric; optimisation; search; Turing test; intelligent decision making; fuzzy method; probabilistic methods; statistical model; knowledge representation; ML; machine learning; intelligent agents; SW development processes; AI activities; product quality; agile models; extended waterfall model; integration effectiveness measurement; software development processes; artificial intelligence activity integration KeyWords Plus:MODEL","Categories":"Computer Science Web of Science Categories:Computer Science, Software Engineering","Journal Information":"IET SOFTWARE Volume: 11 Issue: 1 Pages: 18-26 DOI: 10.1049/iet-sen.2016.0095 Published: FEB 2017","Abstract":"Recently, the modelling of whole process of software (SW) development is performed using extended waterfall and agile models. The further advancement of extended waterfall and agile models in the main phases like communication, planning, modelling, construction and deployment can improve the overall quality of the product. Accordingly, in this study, artificial intelligence (AI) activities are integrated into SW development processes. The important AI activities like intelligent agents, machine learning (ML), knowledge representation, statistical model, probabilistic methods, and fuzzy are integrated into the extended waterfall model. Again, AI activities like intelligent decision making, ML, Turing test, search and optimisation are integrated into the agile model. Two metrics such as, Usability Goals Achievement Metric and Index of Integration are evaluated in five independent SW projects. Once SW projects are developed using these models, feedback queries have been collected formally and the collected data are extensively analysed to identify the individual characteristics of products, identifying correlation behaviour of products with respect to model and metrics.","Authors":"Kulkarni, RH (Kulkarni, Rajesh H.) ; Padmanabham, P (Padmanabham, Palacholla)","Title":"Integration of artificial intelligence activities in software development processes and measuring effectiveness of integration"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395844700006 ISSN: 0262-8856 eISSN: 1872-8138","Keywords":"Face recognition; Image set; Joint regularized nearest points; Sparse representation KeyWords Plus:REPRESENTATION","Categories":"Computer Science; Engineering; Optics Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic; Optics","Journal Information":"IMAGE AND VISION COMPUTING Volume: 58 Pages: 47-60 DOI: 10.1016/j.imavis.2016.07.008 Published: FEB 2017","Abstract":"Face recognition based on image set has attracted much attention due to its promising performance to overcome various variations. Recently, classifiers of regularized nearest points, including sparse approximated nearest points (SANP), regularized nearest points (RNP) and collaborative regularized nearest points (CRNP), have achieved state-of-the-art performance for image set based face recognition. From a query set and a single-class gallery set, SANP and RNP both generate a pair of nearest points, between which the distance is regarded as the between-set distance. However, the computing of nearest points for each single-class gallery set in SANP and RNP ignores collaboration and competition with other classes, which may cause a wrong-class gallery set to have a small between-set distance. CRNP used collaborative representation to overcome this shortcoming but it doesn't explicitly minimize the between-set distance. In order to solve these issues and fully exploit the advantages of nearest points based approaches, in this paper a novel joint regularized nearest points (JRNP) is proposed for face recognition based on image sets. In JRNP, the nearest point in the query set is generated by considering the entire gallery set of all classes; at the same time, JRNP explicitly minimizes the between-set distance of the query set and a single-class gallery set. Furthermore, we proposed algorithms of greedy JRNP and adaptive JRNP to solve the presented model, and the classification is then based on the joint distance between the regularized nearest points in image sets. Extensive experiments were conducted on benchmark databases (e.g., Honda/UCSD, CMU Mobo, You Tube Celebrities databases, and the large-scale You Tube Face datasets). The experimental results clearly show that our JRNP leads the performance in face recognition based on image sets. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Yang, M (Yang, Meng) ; Wang, X (Wang, Xing) ; Liu, WY (Liu, Weiyang) ; Shen, LL (Shen, Linlin)","Title":"Joint regularized nearest points for image set based face recognition"}, {"Keywords":"Graph based representation; Graph indexation; Information spotting in Document recognition KeyWords Plus:EDIT DISTANCE; PATTERN-RECOGNITION; COLLECTIONS","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"PATTERN RECOGNITION LETTERS Volume: 87 Pages: 203-211 Special Issue: SI DOI: 10.1016/j.patrec.2016.06.015 Published: FEB 1 2017","Abstract":"Graph-based representations are experiencing a growing usage in visual recognition and retrieval due to their representational power in front of classical appearance-based representations. However, retrieving a query graph from a large dataset of graphs implies a high computational complexity. The most important property for a large-scale retrieval is the search time complexity to be sub-linear in the number of database examples. With this aim, in this paper we propose a graph indexation formalism applied to visual retrieval. A binary embedding is defined as hashing keys for graph nodes. Given a database of labeled graphs, graph nodes are complemented with vectors of attributes representing their local context. Then, each attribute vector is converted to a binary code applying a binary-valued hash function. Therefore, graph retrieval is formulated in terms of finding target graphs in the database whose nodes have a small Hamming distance from the query nodes, easily computed with bitwise logical operators. As an application example, we validate the performance of the proposed methods in different real scenarios such as handwritten word spotting in images of historical documents or symbol spotting in architectural floor plans. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Riba, P (Riba, Pau) ; Llados, J (Llados, Josep) ; Fornes, A (Fornes, Alicia) ; Dutta, A (Dutta, Anjan) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Dutta, Anjan  http://orcid.org/0000-0002-1667-2245","Title":"Large-scale graph indexing using binary embeddings of node contexts for information spotting in document image databases"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397389800003 ISSN: 2150-8097","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"PROCEEDINGS OF THE VLDB ENDOWMENT Volume: 10 Issue: 6 Pages: 661-672 Published: FEB 2017","Abstract":"In the context of interactive query sessions, it is common to issue a succession of queries, transforming a dataset to the desired result. It is often difficult to comprehend a succession of transformations, especially for complex queries. Thus, to facilitate understanding of each data transformation and to provide continuous feedback, we introduce the concept of \"data tweening\", i.e., interpolating between resultsets, presenting to the user a series of incremental visual representations of a resultset transformation. We present tweening methods that consider not just the changes in the result, but also the changes in the query. Through user studies, we show that data tweening allows users to efficiently comprehend data transforms, and also enables them to gain a better understanding of the underlying query operations.","Authors":"Khan, M (Khan, Meraj) ; Xu, L (Xu, Larry) ; Nandi, A (Nandi, Arnab) ; Hellerstein, JM (Hellerstein, Joseph M.)","Title":"Data Tweening: Incremental Visualization of Data Transforms"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395001600005 ISSN: 1066-8888 eISSN: 0949-877X","Keywords":"Hierarchical data; Indexing; Nested intervals; Tree data; Tree indexing; Nested intervals KeyWords Plus:EFFICIENT XML QUERY; SCHEME","Categories":"Computer Science Web of Science Categories:Computer Science, Hardware & Architecture; Computer Science, Information Systems","Journal Information":"VLDB JOURNAL Volume: 26 Issue: 1 Pages: 55-80 Special Issue: SI DOI: 10.1007/s00778-016-0436-3 Published: FEB 2017","Abstract":"Maintaining and querying hierarchical data in a relational database system is an important task in many business applications. This task is especially challenging when considering dynamic use cases with a high rate of complex, possibly skewed structural updates. Labeling schemes are widely considered the indexing technique of choice for hierarchical data, and many different schemes have been proposed. However, they cannot handle dynamic use cases well due to various problems, which we investigate in this paper. We therefore propose Order Indexes-a dynamic representation of the nested intervals encoding-which offer competitive query performance, unprecedented update efficiency, and robustness for highly dynamic workloads.","Authors":"Finis, J (Finis, Jan) ; Brunel, R (Brunel, Robert) ; Kemper, A (Kemper, Alfons) ; Neumann, T (Neumann, Thomas) ; May, N (May, Norman) ; Faerber, F (Faerber, Franz)","Title":"Order Indexes: supporting highly dynamic hierarchical data in relational main-memory database systems"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397260800023 PubMed ID: 28172469 ISSN: 1367-4803 eISSN: 1460-2059","Categories":"Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics Web of Science Categories:Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability","Journal Information":"BIOINFORMATICS Volume: 33 Issue: 3 Pages: 450-452 DOI: 10.1093/bioinformatics/btw635 Published: FEB 1 2017","Abstract":"Motivation: Biclustering is widely used to identify co-expressed genes under subsets of all the conditions in a large-scale transcriptomic dataset. The program, QUBIC, is recognized as one of the most efficient and effective biclustering methods for biological data interpretation. However, its availability is limited to a C implementation and to a low-throughput web interface. Results: An R implementation of QUBIC is presented here with two unique features: (i) a 82% average improved efficiency by refactoring and optimizing the source C code of QUBIC; and (ii) a set of comprehensive functions to facilitate biclustering-based biological studies, including the qualitative representation (discretization) of expression data, query-based biclustering, bicluster expanding, biclusters comparison, heatmap visualization of any identified biclusters and co-expression networks elucidation.","Authors":"Zhang, Y (Zhang, Yu) ; Xie, J (Xie, Juan) ; Yang, JY (Yang, Jinyu) ; Fennell, A (Fennell, Anne) ; Zhang, C (Zhang, Chi) ; Ma, Q (Ma, Qin)","Title":"QUBIC: a bioconductor package for qualitative biclustering analysis of gene co-expression data"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397037900012 ISSN: 0955-792X eISSN: 1465-363X","Keywords":"Description logics; DL-Lite; possibility theory; reasoning under uncertainty KeyWords Plus:DESCRIPTION LOGICS; KNOWLEDGE BASES; UNCERTAINTY; REASONER; FAMILY","Categories":"Computer Science; Science & Technology - Other Topics Web of Science Categories:Computer Science, Theory & Methods; Logic","Journal Information":"JOURNAL OF LOGIC AND COMPUTATION Volume: 27 Issue: 1 Pages: 261-297 DOI: 10.1093/logcom/exv014 Published: FEB 2017","Abstract":"DL-Lite is one of the most important fragments of description logics that allows a flexible representation of knowledge with a tractable computational complexity of the reasoning process. This article investigates an extension of the main fragments of DL-Lite to deal with uncertainty associated with objects, concepts or relations using a possibility theory framework. Possibility theory offers a natural framework for representing uncertain and incomplete information. It is particularly useful for handling inconsistent knowledge. We first provide foundations of possibilistic DL-Lite, denoted by pi-DL-Lite, by extending the DLLitecore logic, the core fragment of all DL-Lite logics, within possibility theory setting. We present syntax and semantics of pi-DL-Lite(core), study the reasoning tasks and show how to compute the inconsistency degree of a pi-DL-Lite(core) knowledge base. We then extend our possibilistic approach to DL-Lite(F) and DL- Lite(R), two important fragments of DL- Lite family. Finally, we address the problem of query answering over a pi-DL-Lite knowledge base. An important result of the article is that the extension of the expressive power of DL-Lite is done without additional extra-computational costs.","Authors":"Benferhat, S (Benferhat, Salem) ; Bouraoui, Z (Bouraoui, Zied)","Title":"Min-based possibilistic DL-Lite"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393987400008 ISSN: 1041-4347 eISSN: 1558-2191","Keywords":"Representation learning; external knowledge base; sentence modeling KeyWords Plus:WEB","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems; Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING Volume: 29 Issue: 2 Pages: 344-358 DOI: 10.1109/TKDE.2016.2622705 Published: FEB 2017","Abstract":"Sentence auto-completion is an important feature that saves users many keystrokes in typing the entire sentence by providing suggestions as they type. Despite its value, the existing sentence auto-completion methods, such as query completion models, can hardly be applied to solving the object completion problem in sentences with the form of (subject, verb, object), due to the complex natural language description and the data deficiency problem. Towards this goal, we treat an SVO sentence as a three-element triple (subject, sentence pattern, object), and cast the sentence object completion problem as an element inference problem. These elements in all triples are encoded into a unified low-dimensional embedding space by our proposed TRANSFER model, which leverages the external knowledge base to strengthen the representation learning performance. With such representations, we can provide reliable candidates for the desired missing element by a linear model. Extensive experiments on a real-world dataset have well-validated our model. Meanwhile, we have successfully applied our proposed model to factoid question answering systems for answer candidate selection, which further demonstrates the applicability of the TRANSFER model.","Authors":"Wei, XC (Wei, Xiaochi) ; Huang, HY (Huang, Heyan) ; Nie, LQ (Nie, Liqiang) ; Zhang, HW (Zhang, Hanwang) ; Mao, XL (Mao, Xian-Ling) ; Chua, TS (Chua, Tat-Seng)","Title":"I Know What You Want to Express: Sentence Element Inference by Incorporating External Knowledge Base"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393908500002 ISSN: 0890-0604 eISSN: 1469-1760","Keywords":"Case-Based Reasoning; Conceptual Design; Function-Behavior-Structure Knowledge Cell; Hybrid Similarity Measure KeyWords Plus:REPRESENTATION; SIMILARITY; RETRIEVAL; FRAMEWORK; ONTOLOGY; ANALOGY; CBR","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Engineering, Multidisciplinary; Engineering, Manufacturing","Journal Information":"AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING Volume: 31 Issue: 1 Pages: 16-29 DOI: 10.1017/S0890060416000159 Published: FEB 2017","Abstract":"Creative conceptual design requires significant previous design knowledge. Case-based reasoning enables learning from previous design experience and has a great potential in supporting creative conceptual design by means of seeking to retrieve, reuse, and revise most appropriate cases to generate inspired solutions. However, traditional case-based reasoning based creative conceptual design models focus on design strategies research, pay little attention to defining a consistent knowledge representation model, and neglect the research to make various types of knowledge retrieval tractable. Faced with such drawbacks, the expected design knowledge cannot be retrieved properly, especially in cases where multidisciplinary knowledge is concerned or exact query terms are absent. In order to solve these issues, this paper presents a combined approach to support creative conceptual design process. First, function-behavior-structure knowledge cell is introduced as a unified consistent design knowledge representation model. Second, a hybrid similarity measure is proposed to increase the overall possibility of obtaining useful design knowledge by considering semantic understanding ability. Third, an intelligent creative conceptual design system has been developed with a case study of a novel insulin pump design to demonstrate its usage, and two experiments are conducted to evaluate the performance of the proposed approach. The results show that the proposed approach outperforms other case-based reasoning based creative conceptual design models.","Authors":"Hu, J (Hu, Jie) ; Ma, J (Ma, Jin) ; Feng, JF (Feng, Jin-Feng) ; Peng, YH (Peng, Ying-Hong)","Title":"Research on new creative conceptual design system using adapted case-based reasoning technique"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393689700009 PubMed ID: 27678255 ISSN: 0897-1889 eISSN: 1618-727X","Keywords":"CT images; Content-based image retrieval; Diagnosis of lung cancer; Lung cancer; Pulmonary nodules; Self-learning tool of radiology; CBIR based CAD system KeyWords Plus:COMPUTER-AIDED DIAGNOSIS; CT IMAGES; SEGMENTATION; SCANS","Categories":"Radiology, Nuclear Medicine & Medical Imaging Web of Science Categories:Radiology, Nuclear Medicine & Medical Imaging","Journal Information":"JOURNAL OF DIGITAL IMAGING Volume: 30 Issue: 1 Pages: 63-77 DOI: 10.1007/s10278-016-9904-y Published: FEB 2017","Abstract":"Visual information of similar nodules could assist the budding radiologists in self-learning. This paper presents a content-based image retrieval (CBIR) system for pulmonary nodules, observed in lung CT images. The reported CBIR systems of pulmonary nodules cannot be put into practice as radiologists need to draw the boundary of nodules during query formation and feature database creation. In the proposed retrieval system, the pulmonary nodules are segmented using a semi-automated technique, which requires a seed point on the nodule from the end-user. The involvement of radiologists in feature database creation is also reduced, as only a seed point is expected from radiologists instead of manual delineation of the boundary of the nodules. The performance of the retrieval system depends on the accuracy of the segmentation technique. Several 3D features are explored to improve the performance of the proposed retrieval system. A set of relevant shape and texture features are considered for efficient representation of the nodules in the feature space. The proposed CBIR system is evaluated for three configurations such as configuration-1 (composite rank of malignancy \"1\",\"2\" as benign and \"4\",\"5\" as malignant), configuration-2 (composite rank of malignancy \"1\",\"2\", \"3\" as benign and \"4\",\"5\" as malignant), and configuration-3 (composite rank of malignancy \"1\",\"2\" as benign and \"3\",\"4\",\"5\" as malignant). Considering top 5 retrieved nodules and Euclidean distance metric, the precision achieved by the proposed method for configuration-1, configuration-2, and configuration-3 are 82.14, 75.91, and 74.27 %, respectively. The performance of the proposed CBIR system is close to the most recent technique, which is dependent on radiologists for manual segmentation of nodules. A computer-aided diagnosis (CAD) system is also developed based on CBIR paradigm. Performance of the proposed CBIR-based CAD system is close to performance of the CAD system using support vector machine.","Authors":"Dhara, AK (Dhara, Ashis Kumar) ; Mukhopadhyay, S (Mukhopadhyay, Sudipta) ; Dutta, A (Dutta, Anirvan) ; Garg, M (Garg, Mandeep) ; Khandelwal, N (Khandelwal, Niranjan)","Title":"Content-Based Image Retrieval System for Pulmonary Nodules: Assisting Radiologists in Self-Learning and Diagnosis of Lung Cancer"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393088900015 PubMed ID: 27661563 ISSN: 1547-5654 eISSN: 1547-5646","Keywords":"orthopedic surgery; surgeon ratings; Consumers' Checkbook; ProPublica; website KeyWords Plus:SPINE SURGERY; PERIOPERATIVE COMPLICATIONS; HIP FRACTURE; POSTOPERATIVE COMPLICATIONS; PREOPERATIVE DIAGNOSIS; JOINT REPLACEMENT; KNEE ARTHROPLASTY; CLINICAL ARTICLE; LUMBAR SPINE; OUTCOME DATA","Categories":"Neurosciences & Neurology; Surgery Web of Science Categories:Clinical Neurology; Surgery","Journal Information":"JOURNAL OF NEUROSURGERY-SPINE Volume: 26 Issue: 2 Pages: 235-242 DOI: 10.3171/2016.7.SPINE16183 Published: FEB 2017","Abstract":"OBJECTIVE Recently, 2 surgeon rating websites (Consumers' Checkbook and ProPublica) were published to allow the public to compare surgeons through identifying surgeon volume and complication rates. Among neurosurgeons and orthopedic surgeons, only cervical and lumbar spine, hip, and knee procedures were included in this assessment. METHODS The authors examined the methodology of each website to assess potential sources of inaccuracy. Each online tool was queried for reports on neurosurgeons specializing in spine surgery and orthopedic surgeons specializing in spine, hip, or knee surgery. Surgeons were chosen from top-ranked hospitals in the US, as recorded by a national consumer publication ranking system, within the fields of neurosurgery and orthopedic surgery. The results were compared for accuracy and surgeon representation, and the results of the 2 websites were also compared. RESULTS The methodology of each site was found to have opportunities for bias and limited risk adjustment. The end points assessed by each site were actually not complications, but proxies of complication occurrence. A search of 510 surgeons (401 orthopedic surgeons [79%] and 109 neurosurgeons [21%]) showed that only 28% and 56% of surgeons had data represented on Consumers' Checkbook and ProPublica, respectively. There was a significantly higher chance of finding surgeon data on ProPublica (p < 0.001). Of the surgeons from top-ranked programs with data available, 17% were quoted to have high complication rates, 13% with lower volume than other surgeons, and 79% had a 3-star out of 5-star rating. There was no significant correlation found,between the number of stars a surgeon received on Consumers' Checkbook and his or her adjusted complication rate on ProPublica. CONCLUSIONS Both the Consumers' Checkbook and ProPublica websites have significant methodological issues. Neither site assessed complication occurrence, but rather readmissions or prolonged length of stay. Risk adjustment was limited or nonexistent. A substantial number of neurosurgeons and orthopedic surgeons from top-ranked hospitals have no ratings on either site, or have data that suggests they are low-volume surgeons or have higher complication rates. Consumers' Checkbook and ProPublica produced different results with little correlation between the 2 websites in how surgeons were graded. Given the significant methodological issues, incomplete data, and lack of appropriate risk stratification of patients, the featured websites may provide erroneous information to the public.","Authors":"Xu, LW (Xu, Linda W.) ; Li, A (Li, Amy) ; Swinney, C (Swinney, Christian) ; Babu, M (Babu, Maya) ; Veeravagu, A (Veeravagu, Anand) ; Wolfe, SQ (Wolfe, Stacey Quintero) ; Nahed, BV (Nahed, Brian V.) ; Ratliff, JK (Ratliff, John K.) Group Author(s): Council State Neurosurg Soc","Title":"An assessment of data and methodology of online surgeon scorecards"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393116900015 ISSN: 1863-1703 eISSN: 1863-1711","Keywords":"Image retrieval; Vocabulary construction; Bags of visual words; Saliency map KeyWords Plus:IMAGE; REPRESENTATION","Categories":"Engineering; Imaging Science & Photographic Technology Web of Science Categories:Engineering, Electrical & Electronic; Imaging Science & Photographic Technology","Journal Information":"SIGNAL IMAGE AND VIDEO PROCESSING Volume: 11 Issue: 2 Pages: 309-316 DOI: 10.1007/s11760-016-0938-x Published: FEB 2017","Abstract":"Content-based image retrieval systems are meant to retrieve the most similar images of a collection to a query image. One of the most well-known models widely applied for this task is the bag of visual words (BoVW) model. In this paper, we introduce a study of different information gain models used for the construction of a visual vocabulary. In the proposed framework, information gain models are used as a discriminative information to index image features and select the ones that have the highest information gain values. We introduce some extensions to further improve the performance of the proposed framework: mixing different vocabularies and extending the BoVW to bag of visual phrases. Exhaustive experiments show the interest of information gain models on our retrieval framework.","Authors":"Le, HT (Huu Ton Le) ; Urruty, T (Urruty, Thierry) ; Gbehounou, S (Gbehounou, Syntyche) ; Lecellier, F (Lecellier, Francois); Martinet, J (Martinet, Jean) ; Fernandez-Maloigne, C (Fernandez-Maloigne, Christine)","Title":"Improving retrieval framework using information gain models"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000388777700010 ISSN: 0957-4174 eISSN: 1873-6793","Keywords":"Deep Learning; Query-oriented Summarization; Extractive Summarization; Ensemble Noisy Auto-Encoder KeyWords Plus:NEURAL-NETWORKS; ALGORITHM; MODELS","Categories":"Computer Science; Engineering; Operations Research & Management Science Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science","Journal Information":"EXPERT SYSTEMS WITH APPLICATIONS Volume: 68 Pages: 93-105 DOI: 10.1016/j.eswa.2016.10.017 Published: FEB 2017","Abstract":"We present methods of extractive query-oriented single-document summarization using a deep auto encoder (AE) to compute a feature space from the term-frequency (tf) input. Our experiments explore both local and global vocabularies. We investigate the effect of adding small random noise to local tf as the input representation of AE, and propose an ensemble of such noisy AEs which we call the Ensemble Noisy Auto-Encoder (ENAE). ENAE is a stochastic version of an AE that adds noise to the input text and selects the top sentences from an ensemble of noisy runs. In each individual experiment of the ensemble, a different randomly generated noise is added to the input representation. This architecture changes the application of the AE from a deterministic feed-forward network to a stochastic runtime model. Experiments show that the AE using local vocabularies clearly provide a more discriminative feature space and improves the recall on average 11.2%. The ENAE can make further improvements, particularly in selecting informative sentences. To cover a wide range of topics and structures, we perform experiments on two different publicly available email corpora that are specifically designed for text summarization. We used ROUGE as a fully automatic metric in text summarization and we presented the average ROUGE-2 recall for all experiments. (C) 2016 Elsevier Ltd. All rights reserved.","Authors":"Yousefi-Azar, M (Yousefi-Azar, Mahmood) ; Hamey, L (Hamey, Len) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number yousefiazar, Mahmood  http://orcid.org/0000-0002-1029-6584","Title":"Text summarization using unsupervised deep learning"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000389095200027 ISSN: 0020-0255 eISSN: 1872-6291","Keywords":"Conditional random fields; Neural network; Acronym; Sequence labeling; Multi-granularity KeyWords Plus:HIDDEN MARKOV-MODELS; INFORMATION EXTRACTION","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"INFORMATION SCIENCES Volume: 378 Pages: 462-474 DOI: 10.1016/j.ins.2016.06.045 Published: FEB 1 2017","Abstract":"Identifying expansion forms for acronyms is beneficial to many natural language processing and information retrieval tasks. In this work, we study the problem of finding expansions in texts for given acronym queries by modeling the problem as a sequence labeling task. However, it is challenging for traditional sequence labeling models like Conditional Random Fields (CRF) due to the complexity of the input sentences and the substructure of the categories. In this paper, we propose a Latent-state Neural Conditional Random Fields model (LNCRF) to deal with the challenges. On one hand, we extend CRF by coupling it with nonlinear hidden layers to learn multi-granularity hierarchical representations of the input data under the framework of Conditional Random Fields. On the other hand, we introduce latent variables to capture the fine granular information from the intrinsic substructures within the structured output labels implicitly. The experimental results on real data show that our model achieves the best performance against the state-of-the-art baselines. (C) 2016 Elsevier Inc. All rights reserved.","Authors":"Liu, J (Liu, Jie) ; Liu, CH (Liu, Caihua) ; Huang, YL (Huang, Yalou)","Title":"Multi-granularity sequence labeling model for acronym expansion identification"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000387429100010 ISSN: 0031-3203 eISSN: 1873-5142","Keywords":"Face recognition; Collaborative representation; Label propagation; Probabilistic graph; Single training sample per person KeyWords Plus:EXPRESSION VARIANT FACES; DISCRIMINANT-ANALYSIS; TRAINING SAMPLE; IMAGE; REPRESENTATION; ILLUMINATION; DICTIONARY; EIGENFACES; POSE","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic","Journal Information":"PATTERN RECOGNITION Volume: 62 Pages: 125-134 DOI: 10.1016/j.patcog.2016.08.007 Published: FEB 2017","Abstract":"Single sample per person (SSPP) recognition is one of the most challenging problems in face recognition (FR) due to the lack of information to predict the variations in the query sample. To address this problem, we propose in this paper a novel face recognition algorithm based on a robust collaborative representation (CR) and probabilistic graph model, which is called Collaborative Probabilistic Labels (CPL). First, by utilizing label propagation, we construct probabilistic labels for the samples in the generic training set corresponding to those in the gallery set, thus the discriminative information of the unlabeled data can be effectively explored in our method. Then, the adaptive variation type for a given test sample is automatically estimated. Finally, we propose a novel reconstruction-based classifier for the test sample with its corresponding adaptive dictionary and probabilistic labels. The proposed probabilistic graph based model is adaptively robust to various variations in face images, including illumination, expression, occlusion, pose, etc., and is able to reduce required training images to one sample per class. Experimental results on five widely used face databases are presented to demonstrate the efficacy of the proposed approach. (C) 2016 Elsevier Ltd. All rights reserved.","Authors":"Ji, HK (Ji, Hong-Kun) ; Sun, QS (Sun, Quan-Sen) ; Ji, ZX (Ji, Ze-Xuan) ; Yuan, YH (Yuan, Yun-Hao) ; Zhang, GQ (Zhang, Guo-Qing)","Title":"Collaborative probabilistic labels for face recognition from single sample per person"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000391771100001 PubMed ID: 28074879 ISSN: 2045-2322","Categories":"Science & Technology - Other Topics Web of Science Categories:Multidisciplinary Sciences","Journal Information":"SCIENTIFIC REPORTS Volume: 7 Article Number: 40629 DOI: 10.1038/srep40629 Published: JAN 11 2017","Abstract":"Protein tertiary structure prediction methods have matured in recent years. However, some proteins defy accurate prediction due to factors such as inadequate template structures. While existing model quality assessment methods predict global model quality relatively well, there is substantial room for improvement in local quality assessment, i.e. assessment of the error at each residue position in a model. Local quality is a very important information for practical applications of structure models such as interpreting/designing site-directed mutagenesis of proteins. We have developed a novel local quality assessment method for protein tertiary structure models. The method, named Graph-based Model Quality assessment method (GMQ), explicitly considers the predicted quality of spatially neighboring residues using a graph representation of a query protein structure model. GMQ uses conditional random field as its core of the algorithm, and performs a binary prediction of the quality of each residue in a model, indicating if a residue position is likely to be within an error cutoff or not. The accuracy of GMQ was improved by considering larger graphs to include quality information of more surrounding residues. Moreover, we found that using different edge weights in graphs reflecting different secondary structures further improves the accuracy. GMQ showed competitive performance on a benchmark for quality assessment of structure models from the Critical Assessment of Techniques for Protein Structure Prediction (CASP).","Authors":"Shin, WH (Shin, Woong-Hee) ; Kang, XJ (Kang, Xuejiao) ; Zhang, J (Zhang, Jian) ; Kihara, D (Kihara, Daisuke)","Title":"Prediction of Local Quality of Protein Structure Models Considering Spatial Neighbors in Graphical Models"}, {"Keywords":"Information retrieval; Term proximity; Discrete wavelet transform; Term signal KeyWords Plus:TERM PROXIMITY; RANKING METHOD; TRANSFORM; MODEL","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods","Journal Information":"PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON ADVANCED INTELLIGENT SYSTEMS AND INFORMATICS 2016 Book Series: Advances in Intelligent Systems and Computing Volume: 533 Pages: 3-11 DOI: 10.1007/978-3-319-48308-5_1 Published: 2017","Abstract":"In most of the classical information retrieval models, documents are represented as bag-of words which takes into account the term frequencies (tf) and inverse document frequencies (idf) while they ignore the term proximity. Recently, term proximity among query terms has been observed to be beneficial for improving performance of document retrieval. Several applications of the retrieval have implemented tools to determine term proximity at the query formulation level. They rank documents based on the relative positions of the query terms within the documents. They must store all proximity data in the index, leading to a large index, which slows the search. Recently, many models use term signal representation to represent a query term, the query is transformed from the time domain to the frequency domain using transformation techniques such as wavelet. Discrete Wavelet Transform (DWT) uses multiple resolutions technique by which different frequencies are analyzed with different resolutions. The advantage of the DWT is to consider the spatial information of the query terms within the document rather than using only the count of terms. In this paper, in order to improve ranking score as well as improve the run-time efficiency to resolve the query, and maintain a reasonable space for the index, three different types of spectral analysis based on semantic segmentation are carried out namely: sentence-based segmentation, paragraph-based segmentation and fixed length segmentation; and also different term weighting is performed according to term position.","Authors":"Dahab, MY (Dahab, Mohamed Yehia) ; Kamel, M (Kamel, Mahmoud) ; Alnofaie, S (Alnofaie, Sara) Edited by:Hassanien, AE; Shaalan, K; Gaber, T; Azar, AT; Tolba, MF","Title":"Further Investigations for Documents Information Retrieval Based on DWT"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395475200006 ISSN: 1046-8188 eISSN: 1558-2868","Keywords":"Compact data structure; top-k document retrieval KeyWords Plus:SEARCH-TREES; DOCUMENT-RETRIEVAL; QUERIES; INDEXES; CODES","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"ACM TRANSACTIONS ON INFORMATION SYSTEMS Volume: 35 Issue: 3 Article Number: 22 DOI: 10.1145/3007186 Published: JAN 2017","Abstract":"We introduce a new representation of the inverted index that performs faster ranked unions and intersections while using similar space. Our index is based on the treap data structure, which allows us to intersect/merge the document identifiers while simultaneously thresholding by frequency, instead of the costlier two-step classical processing methods. To achieve compression, we represent the treap topology using different alternative compact data structures. Further, the treap invariants allow us to elegantly encode differentially both document identifiers and frequencies. We also show how to extend this representation to support incremental updates over the index. Results show that, under the tf-idf scoring scheme, our index uses about the same space as state-of-the-art compact representations, while performing up to 2-20 times faster on ranked single-word, union, or intersection queries. Under the BM25 scoring scheme, our index may use up to 40% more space than the others and outperforms them less frequently but still reaches improvement factors of 2-20 in the best cases. The index supporting incremental updates poses an overhead of 50%-100% over the static variants in terms of space, construction, and query time.","Authors":"Konow, R (Konow, Roberto) ; Navarro, G (Navarro, Gonzalo) ; Clarke, CLA (Clarke, Charles L. A.); Lopez-Ortiz, A (Lopez-Ortiz, Alejandro)","Title":"Inverted Treaps"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000397388800009 ISSN: 2150-8097","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"PROCEEDINGS OF THE VLDB ENDOWMENT Volume: 10 Issue: 5 Pages: 565-576 Published: JAN 2017","Abstract":"Question answering (QA) has become a popular way for humans to access billion-scale knowledge bases. Unlike web search, QA over a knowledge base gives out accurate and concise results, provided that natural language questions can be understood and mapped precisely to structured queries over the knowledge base. The challenge, however, is that a human can ask one question in many different ways. Previous approaches have natural limits due to their representations: rule based approaches only understand a small set of \"canned\" questions, while keyword based or synonym based approaches cannot fully understand the questions. In this paper, we design a new kind of question representation: templates, over a billion scale knowledge base and a million scale QA corpora. For example, for questions about a city's population, we learn templates such as What's the population of $city?, How many people are there in $city?. We learned 27 million templates for 2782 intents. Based on these templates, our QA system KBQA effectively supports binary factoid questions, as well as complex questions which are composed of a series of binary factoid questions. Furthermore, we expand predicates in RDF knowledge base, which boosts the coverage of knowledge base by 57 times. Our QA system beats all other state-of-art works on both effectiveness and efficiency over QALD benchmarks.","Authors":"Cui, WY (Cui, Wanyun) ; Xiao, YH (Xiao, Yanghua) ; Wang, HX (Wang, Haixun) ; Song, YQ (Song, Yangqiu) ; Hwang, SW (Hwang, Seung-won) ; Wang, W (Wang, Wei)","Title":"KBQA: Learning Question Answering over QA Corpora and Knowledge Bases"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395834100001 ISSN: 1568-4946 eISSN: 1872-9681","Keywords":"Hybrid methods; Semi-supervised learning; Classification; Summit-Training; Query-by-Committee; Unlabeled data KeyWords Plus:ALGORITHM; LIBRARY; SVM","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications","Journal Information":"APPLIED SOFT COMPUTING Volume: 50 Pages: 1-20 DOI: 10.1016/j.asoc.2016.06.008 Published: JAN 2017","Abstract":"With an explosion in the amount of data available for classification problems it becomes more and more complicated to obtain labels for the whole dataset. Because of this we are frequently presented with only a fraction of labeled data from the whole dataset. To leverage the presence of unlabeled examples Semi-Supervised methods could be used to increase the precision of used classifiers. Therefore, we propose a novel hybrid technique which extends the concept of Self-Training and Help-Training used in Semi-Supervised techniques by incorporating the Active Learning approach for determining the confidence of the classifier in the testing set samples. Specifically we employ the Query-by-Committee (QbC) approach and we call the final method Summit-Training. We apply this method to a range of classifiers (generative and discriminative) and evaluate it on several datasets and real world problems. The formulation of the Summit-Training method especially allows us to use Semi-Supervised approaches for purely discriminative classifiers in which no probabilistic representation of the evaluated classes exists. Compared to other Semi-Supervised techniques (Self-Training and Help-Training), the proposed new method achieves superior performance. It also has better generalization properties since it reduces the number of hyper-parameters and relaxes the conditions for classifiers on which it could be applied. (C) 2016 Published by Elsevier B.V.","Authors":"Tencer, L (Tencer, L.) ; Reznakova, M (Reznakova, M.) ; Cheriet, M (Cheriet, M.)","Title":"Summit-Training: A hybrid Semi-Supervised technique and its application to classification tasks"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396952600007 ISSN: 2213-1337 eISSN: 2213-1345","Keywords":"Methods; Data analysis; Techniques; Photometry; Astrometry KeyWords Plus:SKY SURVEY; REPRESENTATIONS; FITS","Categories":"Astronomy & Astrophysics; Computer Science Web of Science Categories:Astronomy & Astrophysics; Computer Science, Interdisciplinary Applications","Journal Information":"ASTRONOMY AND COMPUTING Volume: 18 Pages: 47-53 DOI: 10.1016/j.ascom.2016.11.002 Published: JAN 2017","Abstract":"PHOTOMETRYPIPELINE (PP) is an automated pipeline that produces calibrated photometry from imaging data through image registration, aperture photometry, photometric calibration, and target identification with only minimal human interaction. PP utilizes the widely used Source Extractor software for source identification and aperture photometry; SCAMP is used for image registration. Both image registration and photometric calibration are based on matching field stars with star catalogs, requiring catalog coverage of the respective field. A number of different astrometric and photometric catalogs can be queried online. Relying on a sufficient number of background stars for image registration and photometric calibration, PP is well-suited to analyze data from small to medium-sized telescopes. Calibrated magnitudes obtained by PP are typically accurate within <0.03 mag and astrometric accuracies are of the order of 0.3 arcsec relative to the catalogs used in the registration. The pipeline consists of an open-source software suite written in Python 2.7, can be run on Unix-based systems on a simple desktop machine, and is capable of realtime data analysis. PP has been developed for observations of moving targets, but can be used for analyzing point source observations of any kind. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Mommert, M (Mommert, M.)","Title":"PHOTOMETRYPIPELINE: An automated pipeline for calibrated photometry"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396866400002 ISSN: 1570-0844 eISSN: 2210-4968","Keywords":"Question answering; Question analysis; Single Classification Ripple Down Rules; Knowledge acquisition; Ontology; Vietnamese; English; DBpedia; Biomedical KeyWords Plus:SEMANTIC WEB; SYSTEM","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods","Journal Information":"SEMANTIC WEB Volume: 8 Issue: 4 Pages: 511-532 Special Issue: SI Published: 2017","Abstract":"Recent years have witnessed a new trend of building ontology-based question answering systems. These systems use semantic web information to produce more precise answers to users' queries. However, these systems are mostly designed for English. In this paper, we introduce an ontology-based question answering system named KbQAS which, to the best of our knowledge, is the first one made for Vietnamese. KbQAS employs our question analysis approach that systematically constructs a knowledge base of grammar rules to convert each input question into an intermediate representation element. KbQAS then takes the intermediate representation element with respect to a target ontology and applies concept-matching techniques to return an answer. On a wide range of Vietnamese questions, experimental results show that the performance of KbQAS is promising with accuracies of 84.1% and 82.4% for analyzing input questions and retrieving output answers, respectively. Furthermore, our question analysis approach can easily be applied to new domains and new languages, thus saving time and human effort.","Authors":"Nguyen, DQ (Dat Quoc Nguyen) ; Nguyen, DQ (Dai Quoc Nguyen) ; Pham, SB (Son Bao Pham)","Title":"Ripple Down Rules for Question Answering"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396866400005 ISSN: 1570-0844 eISSN: 2210-4968","Keywords":"Natural Language Processing; SPARQL; biomedical domain; semantic resources KeyWords Plus:INTERFACES","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods","Journal Information":"SEMANTIC WEB Volume: 8 Issue: 4 Pages: 581-+ Special Issue: SI Published: 2017","Abstract":"Recent and intensive research in the biomedical area enabled to accumulate and disseminate biomedical knowledge through various knowledge bases increasingly available on the Web. The exploitation of this knowledge requires to create links between these bases and to use them jointly. Linked Data, the SPARQL language and interfaces in natural language question answering provide interesting solutions fur querying such knowledge bases. However, while using biomedical Linked Data is crucial, life-science researchers may have difficulties using lime SPARQL language. Interfaces based on natural language question answering are recognized to be suitable for querying knowledge bases. In this paper, we propose a method fur translating natural language questions into SPARQL queries. We use Natural Language Processing tools, semantic resources and RDF triple descriptions. We designed a four-step method which allows to linguistically and semantically annotate questions, to perform an abstraction of these questions, then to build a representation of the SPARQL queries, and finally to generate the queries. The method is designed on 50 questions over three biomedical knowledge bases used in the task 2 of the QALD-4 challenge framework and evaluated on 27 new questions. It achieves good performance with 0.78 F-meastire on the test set.","Authors":"Hamon, T (Hamon, Thierry) ; Grabar, N (Grabar, Natalia) ; Mougin, F (Mougin, Fleur)","Title":"Querying Biornedical Linked Data with Natural Language Questions"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396693700004 ISSN: 2155-6377 eISSN: 2155-6385","Keywords":"Discourse Structure Based Summary Generation; Query Focused Summary Generation; Rhetorical Structure Theory; Sutra Based Summary Generation; Universal Networking Language","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"INTERNATIONAL JOURNAL OF INFORMATION RETRIEVAL RESEARCH Volume: 7 Issue: 1 Pages: 49-69 DOI: 10.4018/IJIRR.2017010104 Published: JAN-MAR 2017","Abstract":"In this paper, the authors propose a query focussed summary generation system which is constructed on top of a unique language-independent discourse structure. The discourse structure is comprised of three text representation techniques, namely, Universal Networking Language (UNL), Rhetorical Structure Theory (RST) and sangatis. The discourse structure is indexed based on a concept called sutra. Both sutra and san.gatis have been used in ancient Indian literatures. The proposed approach is tested using Forum for Information Retrieval (FIRE) corpus and a performance comparison has been done with the one of the state-of-art approaches.","Authors":"Subalalitha, CN (Subalalitha, C. N.) ; Parthasarathi, R (Parthasarathi, Ranjani)","Title":"Query Focused Summary Generation System using Unique Discourse Structure"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000394151700009 ISSN: 1876-1364 eISSN: 1876-1372","Keywords":"RDF streams; diabetes; sensors; personalized health; semantic sensor networks KeyWords Plus:ARTIFICIAL PANCREAS; ONTOLOGY","Categories":"Computer Science; Telecommunications Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems; Telecommunications","Journal Information":"JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS Volume: 9 Issue: 1 Pages: 97-109 DOI: 10.3233/AIS-160420 Published: 2017","Abstract":"Diabetes Type 1 is a metabolic disease which results in a lack of insulin production, causing high glucose levels in the blood. It is crucial for diabetic patients to balance this glucose level, and they depend on external substances to do so. In order to keep this level under control, they usually need to resort to invasive glucose control methods, such as taking a sample drop of blood from their finger and have it analyzed. Recently, other directions emerged to offer alternative ways to estimate glucose level, using indirect sensor measurements including ECG monitoring and other physiological parameters. This paper showcases a framework for inferring semantically annotated glycemic events on the patient, which leverages data from mobile wearable sensors deployed on a sport-belt. This work is part of the D1namo project for non-invasive diabetes monitoring, and focuses on the representation and query processing of the data produced by the wearable sensors, using semantic technologies and vocabularies that extend existing Web standards. Furthermore, this work shows how different layers of data, from raw measurements to complex events can be represented and linked in this framework, and experimental evidence is provided of how these layers can be efficiently exploited using an RDF Stream Processing engine.","Authors":"Calbimonte, JP (Calbimonte, Jean-Paul) ; Ranvier, JE (Ranvier, Jean-Eudes) ; Dubosson, F (Dubosson, Fabien) ; Aberer, K (Aberer, Karl)","Title":"Semantic representation and processing of hypoglycemic events derived from wearable sensor data"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000396863700009 ISSN: 1570-0844 eISSN: 2210-4968","Keywords":"Ontop; OBDA; Databases; RDF; SPARQL; Ontologies; R2RML; OWL KeyWords Plus:DL-LITE FAMILY; DATA ACCESS; ONTOLOGIES; SCHEMA; MATCH","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods","Journal Information":"SEMANTIC WEB Volume: 8 Issue: 3 Pages: 471-487 Published: 2017","Abstract":"We present Ontop, an open-source Ontology-Based Data Access (OBDA) system that allows for querying relational data sources through a conceptual representation of the domain of interest, provided in terms of an ontology, to which the data sources are mapped. Key features of Ontop are its solid theoretical foundations, a virtual approach to OBDA, which avoids materializing triples and is implemented through the query rewriting technique, extensive optimizations exploiting all elements of the OBDA architecture, its compliance to all relevant W3C recommendations (including SPARQL queries, R2RML mappings, and OWL2QL and RDFS ontologies), and its support for all major relational databases.","Authors":"Calvanese, D (Calvanese, Diego) ; Cogrel, B (Cogrel, Benjamin) ; Komla-Ebri, S (Komla-Ebri, Sarah) ; Kontchakov, R (Kontchakov, Roman) ; Lanti, D (Lanti, Davide) ; Rezk, M (Rezk, Martin) ; Rodriguez-Muro, M (Rodriguez-Muro, Mariano) ; Xiao, GH (Xiao, Guohui)","Title":"Ontop: Answering SPARQL Queries over Relational Databases"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000395676900013 PubMed ID: 26812733 ISSN: 1545-5963 eISSN: 1557-9964","Keywords":"Phylogenetics; declarative queries; datalog; visual querying; data integration KeyWords Plus:INTERACTIVE TREE; WEB SERVER; NETWORKS; GENE; EXPLORATION; ANNOTATION; BROWSER; TOOL","Categories":"Biochemistry & Molecular Biology; Computer Science; Mathematics Web of Science Categories:Biochemical Research Methods; Computer Science, Interdisciplinary Applications; Mathematics, Interdisciplinary Applications; Statistics & Probability","Journal Information":"IEEE-ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS Volume: 14 Issue: 1 Pages: 131-144 DOI: 10.1109/TCBB.2016.2520943 Published: JAN-FEB 2017","Abstract":"Despite the recent growth in the number of phylogenetic databases, access to these wealth of resources remain largely tool or form- based interface driven. It is our thesis that the flexibility afforded by declarative query languages may offer the opportunity to access these repositories in a better way, and to use such a language to pose truly powerful queries in unprecedented ways. In this paper, we propose a substantially enhanced closed visual query language, called PhyQL, that can be used to query phylogenetic databases represented in a canonical form. The canonical representation presented helps capture most phylogenetic tree formats in a convenient way, and is used as the storage model for our PhyloBase database for which PhyQL serves as the query language. We have implemented a visual interface for the end users to pose PhyQL queries using visual icons, and drag and drop operations defined over them. Once a query is posed, the interface translates the visual query into a Datalog query for execution over the canonical database. Responses are returned as hyperlinks to phylogenies that can be viewed in several formats using the tree viewers supported by PhyloBase. Results cached in PhyQL buffer allows secondary querying on the computed results making it a truly powerful querying architecture.","Authors":"Jamil, HM (Jamil, Hasan M.)","Title":"A Visual Interface for Querying Heterogeneous Phylogenetic Databases"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393796500006 ISSN: 1051-8215 eISSN: 1558-2205","Keywords":"Cloud computing; correntropy; mobile image; labeling; sparse coding KeyWords Plus:NONNEGATIVE MATRIX FACTORIZATION; DISCRIMINANT-ANALYSIS; ANNOTATION; REPRESENTATION; SELECTION; VIDEO; PERSPECTIVE; ALGORITHMS; REGRESSION; RETRIEVAL","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY Volume: 27 Issue: 1 Pages: 62-72 Special Issue: SI DOI: 10.1109/TCSVT.2016.2539778 Published: JAN 2017","Abstract":"With the rapid development of the mobile service and online social networking service, a large number of mobile images are generated and shared on the social networks every day. The visual content of these images contains rich knowledge for many uses, such as social categorization and recommendation. Mobile image labeling has, therefore, been proposed to understand the visual content and received intensive attention in recent years. In this paper, we present a novel mobile image labeling scheme on the cloud, in which mobile images are first and efficiently transmitted to the cloud by Hamming compressed sensing, such that the heavy computation for image understanding is transferred to the cloud for quick response to the queries of the users. On the cloud, we design a sparse correntropy framework for robustly learning the semantic content of mobile images, based on which the relevant tags are assigned to the query images. The proposed framework (called maximum correntropy-based mobile image labeling) is very insensitive to the noise and the outliers, and is optimized by a half-quadratic optimization technique. We theoretically show that our image labeling approach is more robust than the squared loss, absolute loss, Cauchy loss, and many other robust loss function-based sparse coding methods. To further understand the proposed algorithm, we also derive its robustness and generalization error bounds. Finally, we conduct experiments on the PASCAL VOC' 07 data set and empirically demonstrate the effectiveness of the proposed robust sparse coding method for mobile image labeling.","Authors":"Tao, DP (Tao, Dapeng) ; Cheng, J (Cheng, Jun) ; Gao, XB (Gao, Xinbo) ; Li, XL (Li, Xuelong) ; Deng, C (Deng, Cheng)","Title":"Robust Sparse Coding for Mobile Image Labeling on the Cloud"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393881000005 ISSN: 0143-1161 eISSN: 1366-5901","Categories":"Remote Sensing; Imaging Science & Photographic Technology Web of Science Categories:Remote Sensing; Imaging Science & Photographic Technology","Journal Information":"INTERNATIONAL JOURNAL OF REMOTE SENSING Volume: 38 Issue: 3 Pages: 923-948 DOI: 10.1080/01431161.2016.1277042 Published: 2017","Abstract":"In order to address the challenge of hyperspectral image (HSI) classification with very limited labelled samples, active learning (AL) has become a hot research issue in recent years. Although lots of AL approaches have been proposed in the literature, most of them concentrate on how to select the most informative samples, while ignore the significance of the input feature. We believe that the input feature and the query selection are both crucial for constituting an efficient AL algorithm. In this article, we propose a new discriminative feature, sparse code histogram (SCH), to conduct the AL procedure. SCH exhibits a much stronger distinguishability than several other widely used features, and thus a better AL performance could be expected. With this novel input feature, a probabilistic classifier, multinomial logistic regression, is trained to obtain the class probability of each sample. Considering that the class probability is usually biased due to the limited labelled samples, a graph-based spatial refinement is proposed to refine the class probability by exploiting the contextual information. Based on the refined class probability, informative samples are selected for manual labelling and classifier retraining. Such a process is iterated until a stopping criterion is met. Experimental results demonstrate that the proposed method could usually achieve above 90% classification accuracy with only two iterations, which significantly outperforms several stateof-the-art approaches.","Authors":"Ni, D (Ni, Ding) ; Ma, HB (Ma, Hongbing)","Title":"Active learning for hyperspectral image classification using sparse code histogram and graph-based spatial refinement"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393648400012 ISSN: 0219-1377 eISSN: 0219-3116","Keywords":"Enterprise modelling; Model analysis; Model integration; Semantic techniques; Ontology; ArchiMate; e3value; Business model canvas KeyWords Plus:PRINCIPLES","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems","Journal Information":"KNOWLEDGE AND INFORMATION SYSTEMS Volume: 50 Issue: 1 Pages: 315-346 DOI: 10.1007/s10115-016-0933-0 Published: JAN 2017","Abstract":"Enterprise models assist the governance and transformation of organizations through the specification, communication and analysis of strategy, goals, processes, information, along with the underlying application and technological infrastructure. Such models cross-cut different concerns and are often conceptualized using domain-specific modelling languages. This paper explores the application of graph-based semantic techniques to specify, integrate and analyse multiple, heterogeneous enterprise models. In particular, the proposal described in this paper (1) specifies enterprise models as ontological schemas, (2) uses transformation mapping functions to integrate the ontological schemas and (3) analyses the integrated schemas with graph querying and logical inference. The proposal is evaluated through a scenario that integrates three distinct enterprise modelling languages: the business model canvas, e3value, and the business layer of the ArchiMate language. The results show, on the one hand, that the graph-based approach is able to handle the specification, integration and analysis of enterprise models represented with different modelling languages and, on the other, that the integration challenge resides in defining appropriate mapping functions between the schemas.","Authors":"Caetano, A (Caetano, Artur) ; Antunes, G (Antunes, Goncalo) ; Pombinho, J (Pombinho, Joao) ; Bakhshandeh, M (Bakhshandeh, Marzieh) ; Granjo, J (Granjo, Jose) ; Borbinha, J (Borbinha, Jose) ; da Silva, MM (da Silva, Miguel Mira) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Mira da Silva, Miguel  http://orcid.org/0000-0002-0489-4465","Title":"Representation and analysis of enterprise models with semantic techniques: an application to ArchiMate, e3value and business model canvas"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393526000003 ISSN: 0169-023X eISSN: 1872-6933","Keywords":"Ontology engineering; Competence-based management; Semantic web; Context space theory; Fuzzy logic","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems","Journal Information":"DATA & KNOWLEDGE ENGINEERING Volume: 107 Pages: 51-66 DOI: 10.1016/j.datak.2016.12.001 Published: JAN 2017","Abstract":"In the last years, the need for developing strategies, models and tools to manage competences clearly emerges in numerous scenarios. For instance, this emergence especially raises when it is required to realize effective recruiting platforms, decision support systems for human resource management, learning management systems and so on. This work proposes an ontology-based model for the representation of competences able to support a wide range of scenarios where it is fundamental to model, organize and represent professional competences, enable interoperability and co-operation among different and heterogeneous tools and, lastly, execute queries and inference operations over these competences. The proposed model starts from the outcomes of the specialized literature and the related R & D projects and produces a novel integrated model that represents both job offers and demands to support recruiting initiatives and to develop employability strategies aiming at a best matching as well as a careful skill gap analysis. The model has been evaluated by means of a three-level approach also in the context of the SIRET project whose goal is defining a recruiting and training integrated system able to represent the professional competences of users and to understand the supplies and the demands in order to find optimal agreements in the job market.","Authors":"Miranda, S (Miranda, Sergio) ; Orciuoli, F (Orciuoli, Francesco) ; Loia, V (Loia, Vincenzo) ; Sampson, D (Sampson, Demetrios)","Title":"An ontology-based model for competence management"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392876500012 ISSN: 2330-1635 eISSN: 2330-1643","Categories":"Computer Science; Information Science & Library Science Web of Science Categories:Computer Science, Information Systems; Information Science & Library Science","Journal Information":"JOURNAL OF THE ASSOCIATION FOR INFORMATION SCIENCE AND TECHNOLOGY Volume: 68 Issue: 1 Pages: 154-167 DOI: 10.1002/asi.23574 Published: JAN 2017","Abstract":"Topic models have been shown to be a useful way of representing the content of large document collections, for example, via visualization interfaces (topic browsers). These systems enable users to explore collections by way of latent topics. A standard way to represent a topic is using a term list; that is the top-n words with highest conditional probability within the topic. Other topic representations such as textual and image labels also have been proposed. However, there has been no comparison of these alternative representations. In this article, we compare 3 different topic representations in a document retrieval task. Participants were asked to retrieve relevant documents based on predefined queries within a fixed time limit, presenting topics in one of the following modalities: (a) lists of terms, (b) textual phrase labels, and (c) image labels. Results show that textual labels are easier for users to interpret than are term lists and image labels. Moreover, the precision of retrieved documents for textual and image labels is comparable to the precision achieved by representing topics using term lists, demonstrating that labeling methods are an effective alternative topic representation.","Authors":"Aletras, N (Aletras, Nikolaos) ; Baldwin, T (Baldwin, Timothy) ; Lau, JH (Lau, Jey Han) ; Stevenson, M (Stevenson, Mark)","Title":"Evaluating Topic Representations for Exploring Document Collections"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392978500029 ISSN: 1099-4300","Keywords":"big health data; big data stores; triple store performance; Semantic Web; healthcare ontology","Categories":"Physics Web of Science Categories:Physics, Multidisciplinary","Journal Information":"Entropy Volume: 19 Issue: 1 Article Number: 30 DOI: 10.3390/e19010030 Published: JAN 2017","Abstract":"Today's technological improvements have made ubiquitous healthcare systems that converge into smart healthcare applications in order to solve patients' problems, to communicate effectively with patients, and to improve healthcare service quality. The first step of building a smart healthcare information system is representing the healthcare data as connected, reachable, and sharable. In order to achieve this representation, ontologies are used to describe the healthcare data. Combining ontological healthcare data with the used and obtained data can be maintained by storing the entire health domain data inside big data stores that support both relational and graph-based ontological data. There are several big data stores and different types of big data sets in the healthcare domain. The goal of this paper is to determine the most applicable ontology data store for storing the big healthcare data. For this purpose, AllegroGraph and Oracle 12c data stores are compared based on their infrastructural capacity, loading time, and query response times. Hence, healthcare ontologies (GENE Ontology, Gene Expression Ontology (GEXO), Regulation of Transcription Ontology (RETO), Regulation of Gene Expression Ontology (REXO)) are used to measure the ontology loading time. Thereafter, various queries are constructed and executed for GENE ontology in order to measure the capacity and query response times for the performance comparison between AllegroGraph and Oracle 12c triple stores.","Authors":"Can, O (Can, Ozgu) ; Sezer, E (Sezer, Emine) ; Bursa, O (Bursa, Okan) ; Unalir, MO (Unalir, Murat Osman) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Can, Ozgu  http://orcid.org/0000-0002-8064-2905","Title":"Comparing Relational and Ontological Triple Stores in Healthcare Domain"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392397200015 ISSN: 0920-8542 eISSN: 1573-0484","Keywords":"Parallel access method; High dimensional spaces; Multiple graphs index","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"JOURNAL OF SUPERCOMPUTING Volume: 73 Issue: 1 Pages: 176-189 DOI: 10.1007/s11227-016-1673-3 Published: JAN 2017","Abstract":"Access methods are a fundamental tool on Information Retrieval. However, most of these methods suffer the problem known as the curse of dimensionality when they are applied to objects with very high dimensionality representation spaces, such as text documents. In this paper we introduce a new parallel access method that uses several graphs as distributed index structure and a kNN search algorithm. Two parallel versions of the search method are presented, one based on master-slave scheme and the other based on a pipeline. A thorough experimental analysis on different datasets shows that our method can process efficiently large flows of queries, compete with other parallel algorithms and obtain at the same time very high quality results.","Authors":"Artigas-Fuentes, FJ (Artigas-Fuentes, F. J.) ; Badia, JM (Badia, J. M.)","Title":"Accessing very high dimensional spaces in parallel"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392514400006 ISSN: 0038-0644 eISSN: 1097-024X","Keywords":"semantic web; web applications; description logics; probabilistic description logics; SWI-Prolog; logic programming KeyWords Plus:KNOWLEDGE REPRESENTATION; DESCRIPTION LOGICS; SWI-PROLOG; LANGUAGE","Categories":"Computer Science Web of Science Categories:Computer Science, Software Engineering","Journal Information":"SOFTWARE-PRACTICE & EXPERIENCE Volume: 47 Issue: 1 Pages: 125-142 DOI: 10.1002/spe.2410 Published: JAN 2017","Abstract":"We present the web application Tableau Reasoner for descrIption Logics in proLog on SWI-Prolog for SHaring (TRILL on SWISH) which allows the user to write probabilistic description logic (DL) theories and compute the probability of queries with just a web browser. Various probabilistic extensions of DLs have been proposed in the recent past, because uncertainty is a fundamental component of the Semantic Web. We consider probabilistic DL theories following our distribution semantics for probabilistic ontologies (DISPONTE) semantics. Axioms of a DISPONTE knowledge base can be annotated with a probability, and the probability of queries can be computed with inference algorithms. TRILL is a probabilistic reasoner for DISPONTE knowledge base that is implemented in Prolog and exploits its backtracking facilities for handling the non-determinism of the tableau algorithm. TRILL on SWISH is based on SWISH, a recently proposed web framework for logic programming, based on various features and packages of SWI-Prolog (e.g., a web server and a library for creating remote Prolog engines and posing queries to them). TRILL on SWISH also allows users to cooperate in writing a probabilistic DL theory. It is free, open, and accessible on the Web at the url: ; it includes a number of examples that cover a wide range of domains and provide interesting Probabilistic Semantic Web applications. By building a web-based system, we allow users to experiment with probabilistic DLs without the need to install a complex software stack. In this way, we aim to reach out to a wider audience and popularize the Probabilistic Semantic Web. Copyright (c) 2016 John Wiley & Sons, Ltd.","Authors":"Bellodi, E (Bellodi, Elena) ; Lamma, E (Lamma, Evelina) ; Riguzzi, F (Riguzzi, Fabrizio) ; Zese, R (Zese, Riccardo) ; Cota, G (Cota, Giuseppe)","Title":"A web system for reasoning with probabilistic OWL"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392106200008 ISSN: 1000-9000 eISSN: 1860-4749","Keywords":"XML; twig query; distributed computing; node distribution","Categories":"Computer Science Web of Science Categories:Computer Science, Hardware & Architecture; Computer Science, Software Engineering","Journal Information":"JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY Volume: 32 Issue: 1 Pages: 78-92 DOI: 10.1007/s11390-017-1707-1 Published: JAN 2017","Abstract":"Massive XML data are increasingly generated for the representation, storage and exchange of web information. Twig query processing over massive XML data has become a research focus. However, most traditional algorithms cannot be directly implemented in a distributed manner. Some of the existing distributed algorithms generate a lot of useless intermediate results and execute many join operations of partial results in most cases; others require the priori knowledge of query pattern before XML partition, storage and query processing, which is impractical in the cases of large-scale data or frequent incoming new queries. To improve efficiency and scalability, in this paper, we propose a 3-phase distributed algorithm DisT3 based on node distribution mechanism to avoid unnecessary intermediate results. Furthermore, we propose a lightweight local index ReP with an enhanced XML partitioning approach using arbitrary partitioning strategy, and based on ReP we propose an improved 2-phase distributed algorithm DisT2ReP to further reduce the communication cost. After the performance guarantees are analyzed, extensive experiments are conducted to verify the efficiency and scalability of our proposed algorithms in distributed twig query applications.","Authors":"Bi, X (Bi, Xin) ; Zhao, XG (Zhao, Xiang-Guo) ; Wang, GR (Wang, Guo-Ren)","Title":"Efficient Processing of Distributed Twig Queries Based on Node Distribution"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392422400010 ISSN: 0952-813X eISSN: 1362-3079","Keywords":"Criminal identification; dynamic query optimisation; knowledge extraction; knowledge representation; classification; reasoning; expert system KeyWords Plus:FACE SKETCH SYNTHESIS; RECOGNITION; RETRIEVAL","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence","Journal Information":"JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE Volume: 29 Issue: 1 Pages: 175-196 DOI: 10.1080/0952813X.2015.1132266 Published: 2017","Abstract":"The visual perception of eyewitness plays a vital role in criminal identification scenario. It helps law enforcement authorities in searching particular criminal from their previous record. It has been reported that searching a criminal record manually requires too much time to get the accurate result. We have proposed a query-based approach which minimises the computational cost along with the reduction of search space. A symbolic database has been created to perform a stringent analysis on 150 public (Bollywood celebrities and Indian cricketers) and 90 local faces (our data-set). An expert knowledge has been captured to encapsulate every criminal's anatomical and facial attributes in the form of symbolic representation. A fast query-based searching strategy has been implemented using dynamic decision tree data structure which allows four levels of decomposition to fetch respective criminal records. Two types of case studies - viewed and forensic sketches have been considered to evaluate the strength of our proposed approach. We have derived 1200 views of the entire population by taking into consideration 80 participants as eyewitness. The system demonstrates an accuracy level of 98.6% for test case I and 97.8% for test case II. It has also been reported that experimental results reduce the search space up to 30 most relevant records.","Authors":"Singh, AK (Singh, Avinash Kumar) ; Nandi, GC (Nandi, G. C.)","Title":"Visual perception-based criminal identification: a query-based approach"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392553100015 ISSN: 1016-2364","Keywords":"XQuery; SQL; query translation; schema mapping; query tree","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"JOURNAL OF INFORMATION SCIENCE AND ENGINEERING Volume: 33 Issue: 1 Pages: 251-270 Published: JAN 2017","Abstract":"Query translation has been a central task for data integration or information sharing. In this paper, we assume that the source and target databases may be XML or relational, with the query languages XQuery and SQL, respectively. Besides, there may exist many possible mappings with different weights between two schemas, which are usually produced by automatic schema matching tools. We intend to output the most preferable query based on weights among all equivalent ones, so we need to properly represent the weighted schema mappings and input queries. Our approach is to first classify the schema representation into two basic units, i.e., collections and values, and apply mapping functions to represent the correspondence for each basic unit with weight information. We then propose a set of tree structures, collectively called CanForest, which show the structural constraints and semantics of the input query and assist in producing the output queries. We have constructed the complete translation system and shown its effectiveness. Experimental results also demonstrate that the system is very efficient.","Authors":"Chang, YH (Chang, Ya-Hui) ; Lee, CZ (Lee, Chia-Zhen) ; Zhuang, SY (Zhuang, Si-Yen)","Title":"A Tree-Based Approach to Support Query Translation for Schema Mappings with Weights"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392292000040 ISSN: 1380-7501 eISSN: 1573-7721","Keywords":"Sketch based image retrieval; Histogram of orientations; Sketch tokens KeyWords Plus:PERFORMANCE EVALUATION; DESCRIPTOR; HISTOGRAM","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"MULTIMEDIA TOOLS AND APPLICATIONS Volume: 76 Issue: 1 Pages: 931-951 DOI: 10.1007/s11042-015-3076-5 Published: JAN 2017","Abstract":"Sketch-based image retrieval (SBIR) is an emergent research area with a variety of applications, specially when an example image is not available for querying. Moreover, making a sketch has become a very attractive and simple task due to the already ubiquitous touch-screen and mobile technologies. Although a sketch is a natural way for representing the structure of a thought object, it may easily get confused in a dataset with high variability turning the retrieval task a quite challenging problem. Indeed, the state-of-the-art methods still show low performance on diverse evaluation datasets. Thereby, a robust sketch descriptor together with a better strategy for representing regular images as sketches are demanded. In this work, we present RST-SHELO, and improved version of SHELO (Soft Histogram of Edge Logal Orientations), an efficient state-of-the-art method for describing sketches. The proposed improvements comes from two aspects: a better technique for obtaining sketch-like representations and a better normalization strategy of SHELO. For the first case, we propose to use the sketch token approach [21], aiming to detect image contours by means of mid-level features. For the second case, we demonstrate that a square root normalization positively affect the effectiveness on the retrieval task. Based on our improvements, we present new state-of-the-art performance. To validate our achievements, we have conducted diverse experiments using two public datasets, Flickr15K and Saavedra's. Our results show an effectiveness gain of 62 % in the first and 5 % in the second dataset.","Authors":"Saavedra, JM (Saavedra, Jose M.)","Title":"RST-SHELO: sketch-based image retrieval using sketch tokens and square root normalization"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392305000060 ISSN: 1380-7501 eISSN: 1573-7721","Keywords":"Medical image retrieval; Retrieval in medical texts; Image modality classification; Visual image descriptors KeyWords Plus:SEARCH ENGINE; FEATURES; CLASSIFICATION","Categories":"Computer Science; Engineering Web of Science Categories:Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic","Journal Information":"MULTIMEDIA TOOLS AND APPLICATIONS Volume: 76 Issue: 2 Pages: 2955-2978 DOI: 10.1007/s11042-016-3261-1 Published: JAN 2017","Abstract":"In this paper we depict an implemented system for medical image retrieval. Our system performs retrieval based on both textual and visual content, separately and combined, using advanced encoding and quantization techniques. The text-based retrieval subsystem uses textual data acquired from an image's corresponding article to generate a suitable representation. Using a vector space model, the generated representations structure is altered to increase performance. Query expansion with pseudo-relevance feedback is applied to fine-tune the results. The content-based retrieval subsystem performs retrieval based on visual features extracted from the images. A Gaussian Mixture Model is constructed from the extracted visual features, in our case - RGB histograms, and is used in encoding the same features into Fisher Vectors. With scalability and speed in mind, we utilized a product quantization technique over the generated vectors, which provides fast response times over large image collections. Product quantization drastically reduces the size of the image representation at almost no cost to accuracy, thus improving the scalability factor of our system. Our system uses modality classification to further improve retrieval results. This subsystem labels the image modality based on their visual content. The images are described using state-of-the-art opponentSIFT visual features. Classification was performed using Support Vector Machines (SVMs). The predictions from the SVMs are used for re-ranking the resulting images based on their modality and the modality of the query. The system was evaluated against the standardized ImageCLEF 2013, 2012 and 2011 medical datasets and it reported state-of-the-art performance for all datasets.","Authors":"Kitanovski, I (Kitanovski, Ivan) ; Strezoski, G (Strezoski, Gjorgji) ; Dimitrovski, I (Dimitrovski, Ivica) ; Madjarov, G (Madjarov, Gjorgji) ; Loskovska, S (Loskovska, Suzana)","Title":"Multimodal medical image retrieval system"}, {"Keywords":"Valuation algebra; Automaton representation; Semiring valued constraint KeyWords Plus:SOFT CONSTRAINTS; FRAMEWORK; INFERENCE; SYSTEMS","Categories":"Computer Science; Robotics Web of Science Categories:Computer Science, Artificial Intelligence; Robotics","Journal Information":"QUANTITATIVE LOGIC AND SOFT COMPUTING 2016 Book Series: Advances in Intelligent Systems and Computing Volume: 510 Pages: 203-212 DOI: 10.1007/978-3-319-46206-6_21 Published: 2017","Abstract":"A splitting algorithm is developed for solving single-query projection problems in valuation algebras. This method is based on a generalized combination theorem. It is shown that by using this new kind of combination property, a given single-query projection problem can be broken into pieces of subprojection problems which might be solved simultaneously by different computational resources. At last, as an application of splitting algorithms, we develop an optimized procedure for automaton representation of semiring valued constraint problems.","Authors":"Han, BH (Han, Bang-He) ; Li, YM (Li, Yong-Ming) ; Chen, QN (Chen, Qiong-Na) Edited by:Fan, TH; Chen, SL; Wang, SM; Li, YM","Title":"Splitting Algorithm of Valuation Algebra and Its Application in Automaton Representation of Semiring Valued Constraint Problems"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392533000006 ISSN: 1432-7643 eISSN: 1433-7479","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications","Journal Information":"SOFT COMPUTING Volume: 21 Issue: 2 Pages: 353-368 Special Issue: SI DOI: 10.1007/s00500-015-1632-6 Published: JAN 2017","Abstract":"The web becomes an overwhelmingly huge repository of data. At the same time, users demand access to the information on the web in a more natural way. In other words, users require interaction with the web using natural linguistic terms and expect human comprehensive answers. The introduction of Resource Description Framework (RDF) is a promising step towards significant changes how systems can utilize the web. The very nature of RDF format that ensures high interconnectivity of pieces of data creates an opportunity to process and analyse data in a different way. In this paper, we address the problem of processing web information using fuzzy-based technologies. In particular, we adopt a linguistic representation model to determining alternatives that match a given reference with the highest possible degree and satisfying some specific criteria. The process of comparing alternatives to the reference is feature-driven while an entity is described by its features. The proposed methodology is able to deal with features of different nature and utilize comparison mechanisms suitable for each type of features. The utilization of 2-tuple allows for comparing and aggregating linguistic-based descriptions of features, especially when the reference does not specify values of features explicitly. In experiments, we show the utilization of our approach in the domain of pharmacy. The obtained results show the advantage of using the feature-based comparison process and linguistic aggregation procedure over results obtained using the RDF query language SPARQL (SPARQL Protocol and RDF Query Language).","Authors":"Zadeh, PDH (Zadeh, Parisa D. Hossein) ; Zadeh, MDH (Zadeh, Mahsa D. Hossein) ; Reformat, MZ (Reformat, Marek Z.)","Title":"Feature-driven linguistic-based entity matching in linked data with application in pharmacy"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000391647900011 ISSN: 0010-4485 eISSN: 1879-2685","Keywords":"NURBS; Implicitization; Signed distance field; Implicit Boolean compositions; Surface-surface intersection; Isogeometric analysis KeyWords Plus:HIERARCHICAL PARTITION; OPTIMAL-DESIGN; DISTANCE; MESHLESS; INTERSECTIONS; GEOMETRY; CURVE; POINT","Categories":"Computer Science Web of Science Categories:Computer Science, Software Engineering","Journal Information":"COMPUTER-AIDED DESIGN Volume: 82 Pages: 112-126 DOI: 10.1016/j.cad.2016.09.006 Published: JAN 2017","Abstract":"Boundary-representation (B-rep) geometrical models, often mathematically represented using Non Uniform Rational B-Spline (NURBS) surfaces, are the starting point for complex downstream product life cycle evaluations including Computer-Aided Engineering (CAE). Boolean operations during B-rep model generation require surface intersection computations to describe the composed entity. However, for parametric NURBS surfaces, intersection operations are non-trivial and typically carried out numerically. The numerical intersection computations introduce challenges relating to the accuracy of the resulting representation, efficiency with which the computation is carried out, and robustness of the result to small variations in geometry. Often, for downstream CAE evaluations, an implicit, procedural knowledge of the Boolean operations between the composed objects that can resolve point containment queries (exact to the original NURBS bounding surfaces) maybe sufficient during quadrature. However, common point containment queries on B-rep models are numerical, iterative and relatively expensive. Thus, the first goal of the present paper is to describe a purely algebraic, and therefore non-iterative, approach to carrying out point containment queries on complex B-rep models built using low-degree NURBS surfaces. For CAE operations, the boundary representation of B-rep solids is, in general, not convenient and as a result, the B-rep model is converted to a meshed volumetric approximation. The major challenges to such a conversion include capturing the geometric features accurately when constructing the secondary (meshed) representation, apart from the efficiency of carrying out such a mesh generation step repeatedly as the geometric shape evolves. Thus, an ideal analysis procedure would operate directly on B-rep CAD models, without needing a secondary mesh, and would procedurally unify the geometric operations during CAD as well as CAE stages. Therefore, the second and broader goal of the present paper is to demonstrate CAD-CAE integration using signed algebraic level set operations directly on B-rep models by embedding or immersing the bounding surfaces within a discretized domain while preserving the geometric accuracy of the surfaces exact to the original NURBS representation during analysis. (C) 2016 Elsevier Ltd. All rights reserved.","Authors":"Upreti, K (Upreti, K.) ; Subbarayan, G (Subbarayan, G.)","Title":"Signed algebraic level sets on NURBS surfaces and implicit Boolean compositions for isogeometric CAD-CAE integration"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000391781000006 ISSN: 0022-4456 eISSN: 0975-1084","Keywords":"Query Search; IR ranking; Structured Robustness; Ontological Annotation KeyWords Plus:DATABASES","Categories":"Engineering Web of Science Categories:Engineering, Multidisciplinary","Journal Information":"JOURNAL OF SCIENTIFIC & INDUSTRIAL RESEARCH Volume: 76 Issue: 1 Pages: 38-43 Published: JAN 2017","Abstract":"Keyword queries on databases provide easy access to data, but often suffer from low ranking quality, i.e., low precision and/or recall, as shown in recent benchmarks. It would be useful to identify queries that are likely to have low ranking quality to improve the user satisfaction. For instance, the system may suggest to the user alternative queries for such hard queries. In the existing work, analyzes the characteristics of hard queries and propose a novel framework to measure the degree of difficulty for a keyword query over a database, considering both the structure and the content of the database and the query results. However, in this system numbers of issues are there to address. One of the main issues present in the existing work is that, at the time keyword prediction only user submitted keyword will be used for the prediction of the results. The existing work won't concentrate about the semantic meaning present among the key words that are submitted by the users, which will lead to inaccurate result retrieval. To overcome this problem in the proposed work, the semantic based key word prediction is proposed by using ontology-based representation in which the semantic meaning of the keywords will be analyzed by using the Word Net tool. This will lead to an accurate to k retrieval of document due to consideration of the semantic meaning of the documents in search engine.","Authors":"Selvi, MS (Selvi, M. S.) ; Deepa, K (Deepa, K.) ; Sangari, MS (Sangari, M. S.) ; Mohankumar, B (Mohankumar, B.)","Title":"Improved Structured Robustness (I-SR): A Novel Approach to Predict Hard Keyword Queries"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000390507700009 ISSN: 0921-8890 eISSN: 1872-793X","Keywords":"State estimation; Localization; Continuous time; SLAM; Gaussian process regression KeyWords Plus:DEGREE ORDERING ALGORITHM; SIMULTANEOUS LOCALIZATION","Categories":"Automation & Control Systems; Computer Science; Robotics Web of Science Categories:Automation & Control Systems; Computer Science, Artificial Intelligence; Robotics","Journal Information":"ROBOTICS AND AUTONOMOUS SYSTEMS Volume: 87 Pages: 120-132 DOI: 10.1016/j.robot.2016.10.004 Published: JAN 2017","Abstract":"Recent work on simultaneous trajectory estimation and mapping (STEAM) for mobile robots has used Gaussian processes (GPs) to efficiently represent the robot's trajectory through its environment. GPs have several advantages over discrete-time trajectory representations: they can represent a continuous-time trajectory, elegantly handle asynchronous and sparse measurements, and allow the robot to query the trajectory to recover its estimated position at any time of interest. A major drawback of the GP approach to STEAM is that it is formulated as a batch trajectory estimation problem. In this paper we provide the critical extensions necessary to transform the existing GP-based batch algorithm for STEAM into an extremely efficient incremental algorithm. In particular, we are able to vastly speed up the solution time through efficient variable reordering and incremental sparse updates, which we believe will greatly increase the practicality of Gaussian process methods for robot mapping and localization. Finally, we demonstrate the approach and its advantages on both synthetic and real datasets. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Yan, XY (Yan, Xinyan) ; Indelman, V (Indelman, Vadim) ; Boots, B (Boots, Byron)","Title":"Incremental sparse GP regression for continuous-time trajectory estimation and mapping"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000389716900009 ISSN: 1552-6283 eISSN: 1552-6291","Keywords":"Digital Libraries; Information Retrieval; Knowledge Extraction; Knowledge Representation; Linked Open Data; Ontologies; Semantic Web KeyWords Plus:WEB; WIKIDATA","Categories":"Computer Science Web of Science Categories:Computer Science, Artificial Intelligence; Computer Science, Information Systems","Journal Information":"INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS Volume: 13 Issue: 1 Pages: 128-146 Special Issue: SI DOI: 10.4018/IJSWIS.2017010108 Published: JAN-MAR 2017","Abstract":"Producing the Linked Open Data (LOD) is getting potential to publish high- quality interlinked data. Publishing such data facilitates intelligent searching from the Web of data. In the context of scientific publications, data about millions of scientific documents published by hundreds and thousands of publishers is in silence as it is not published as open data and ultimately is not linked to other datasets. In this paper the authors present SPedia: a semantically enriched knowledge base of data about scientific documents. SPedia knowledge base provides information on more than nine million scientific documents, consisting of more than three hundred million RDF triples. These extracted datasets, allow users to put sophisticated queries by employing semantic Web techniques instead of relying on keyword-based searches. This paper also shows the quality of extracted data by performing sample queries through SPedia SPARQL Endpoint and analyzing results. Finally, the authors describe that how SPedia can serve as central hub for the cloud of LOD of scientific publications.","Authors":"Aslam, MA (Aslam, Muhammad Ahtisham) ; Aljohani, NR (Aljohani, Naif Radi)","Title":"SPedia: A Central Hub for the Linked Open Data of Scientific Publications"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000388615700003 ISSN: 1365-8816 eISSN: 1362-3087","Keywords":"Similarity; holed polygon; position graph; Fourier descriptors KeyWords Plus:SPATIAL OBJECTS; IMAGE RETRIEVAL; REPRESENTATION; CLASSIFICATION; DISTANCE","Categories":"Computer Science; Geography; Physical Geography; Information Science & Library Science Web of Science Categories:Computer Science, Information Systems; Geography; Geography, Physical; Information Science & Library Science","Journal Information":"INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE Volume: 31 Issue: 2 Pages: 253-279 DOI: 10.1080/13658816.2016.1192637 Published: 2017","Abstract":"In geographic information retrieval and spatial data mining, similarity is used to resolve shape matching and clustering. Many approaches have been developed to calculate similarity between simple geometric shapes. However, complex spatial objects are common in spatial database systems, spatial query languages and Geographic Information Science (GIS) applications. With holed polygons, many similarity measurement approaches are restricted to address the relationships between holes or between the holes and the entire complex geometric shape. A successful method should remove the restrictions due to these complex relations and retain invariant during geometric translation (rotation, moving and scaling). To overcome these deficiencies, we utilize position graphs to describe the distribution of holes in complex geometric shapes by storing invariants, such as angles and distances. In addition, Fourier descriptors and the position graph-based method are used to measure the similarity between holed polygons. Experiments show that the proposed method takes into account the relationships in an entire complex geometric shape. It can effectively calculate the similarity of holed polygons, even if they contain different numbers of holes.","Authors":"Xu, YY (Xu, Yongyang) ; Xie, Z (Xie, Zhong) ; Chen, ZL (Chen, Zhanlong) ; Wu, L (Wu, Liang)","Title":"Shape similarity measurement model for holed polygons based on position graphs and Fourier descriptors"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000390971000010 ISSN: 0304-3975 eISSN: 1879-2294","Keywords":"Lyndon factorization; Straight line program; Lempel-Ziv 78 factorization KeyWords Plus:ALPHABET; WORDS","Categories":"Computer Science Web of Science Categories:Computer Science, Theory & Methods","Journal Information":"THEORETICAL COMPUTER SCIENCE Volume: 656 Pages: 215-224 Part: B Special Issue: SI DOI: 10.1016/j.tcs.2016.03.005 Published: DEC 20 2016","Abstract":"We present two efficient algorithms which, given a compressed representation of a string w of length N, compute the Lyndon factorization of w. Given a straight line program (SLP) S of size n that describes w, the first algorithm runs in O (n(2) + P (n, N) Q (n, N)n logn) time and O (n(2) + S(n, N)) space, where P(n, N), S(n, N), Q(n, N) are respectively the preprocessing time, space, and query time of a data structure for longest common extensions (LCE) on SLPs. Given the Lempel-Ziv 78 encoding of size s for w, the second algorithm runs in O (s logs) time and space. (C) 2016 Elsevier B.V. All rights reserved.","Authors":"Tomohiro, I (Tomohiro, I) ; Nakashima, Y (Nakashima, Yuto) ; Inenaga, S (Inenaga, Shunsuke) ; Bannai, H (Bannai, Hideo) ; Takeda, M (Takeda, Masayuki)","Title":"Faster Lyndon factorization algorithms for SLP and LZ78 compressed text"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000386645800003 ISSN: 0020-0255 eISSN: 1872-6291","Keywords":"Two-dimensional normal cloud; Representation; Dimensionality reduction; Similarity measure; Cloud model; Time-series data mining KeyWords Plus:SPARSE REPRESENTATION; CLASSIFICATION; PREDICTION; MODEL; QUERIES","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"INFORMATION SCIENCES Volume: 374 Pages: 32-50 DOI: 10.1016/j.ins.2016.09.027 Published: DEC 20 2016","Abstract":"Many high-level dimensionality reduction approaches for mining time series have been proposed, e.g., SAX, PWCA, and Feature-based. Due to the rapid performance degradation of time-series data mining in much lower dimensionality and the continuously increasing amount of time series data with uncertainty, there remains a burning need to develop new time-series representations that can retain good performance in much lower reduced space and address uncertainty efficiently. In this work, we propose a novel time series representation, namely Two-dimensional Normal Cloud Representation (2D-NCR), based on cloud model theory. The representation achieves dimensionality reduction by transforming the raw time series into a sequence of two-dimensional normal cloud models. Moreover, a new similarity measure between the transformed time series is presented. The proposed method can reflect the characteristic data distribution of the time series and capture the variation with time. We validate the performance of our representation on the various data mining tasks of classification, clustering, and query by content. The experimental results demonstrate that 2D-NCR is an effective and competitive representation for time-series data mining. (C) 2016 Elsevier Inc. All rights reserved.","Authors":"Deng, WH (Deng, Weihui) ; Wang, GY (Wang, Guoyin) ; Xu, J (Xu, Ji)","Title":"Piecewise two-dimensional normal cloud representation for time-series data mining"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000389380900001 PubMed ID: 27919219 ISSN: 1471-2105","Keywords":"Systems biology graphical notation; Neo4j; Graph database; Systems biology; Systems medicine KeyWords Plus:DISEASE; NETWORKS; GENES; MAPS; KEGG","Categories":"Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Mathematical & Computational Biology Web of Science Categories:Biochemical Research Methods; Biotechnology & Applied Microbiology; Mathematical & Computational Biology","Journal Information":"BMC BIOINFORMATICS Volume: 17 Article Number: 494 DOI: 10.1186/s12859-016-1394-x Published: DEC 5 2016","Abstract":"Background: When modeling in Systems Biology and Systems Medicine, the data is often extensive, complex and heterogeneous. Graphs are a natural way of representing biological networks. Graph databases enable efficient storage and processing of the encoded biological relationships. They furthermore support queries on the structure of biological networks. Results: We present the Java-based framework STON (SBGN TO Neo4j). STON imports and translates metabolic, signalling and gene regulatory pathways represented in the Systems Biology Graphical Notation into a graph-oriented format compatible with the Neo4j graph database. Conclusion: STON exploits the power of graph databases to store and query complex biological pathways. This advances the possibility of: i) identifying subnetworks in a given pathway; ii) linking networks across different levels of granularity to address difficulties related to incomplete knowledge representation at single level; and iii) identifying common patterns between pathways in the database.","Authors":"Toure, V (Toure, Vasundra) ; Mazein, A (Mazein, Alexander) ; Waltemath, D (Waltemath, Dagmar) ; Balaur, I (Balaur, Irina) ; Saqi, M (Saqi, Mansoor) ; Henkel, R (Henkel, Ron) ; Pellet, J (Pellet, Johann) ; Auffray, C (Auffray, Charles) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Mazein, Alexander  C-1369-2013 http://orcid.org/0000-0001-7137-4171 Toure, Vasundra  http://orcid.org/0000-0003-4639-4431","Title":"STON: exploring biological pathways using the SBGN standard and graph databases"}, {"Keywords":"Theory; Languages; Attribute value scaling; change preservation; interval adjustment; query processing; sequenced semantics; temporal databases; timestamp propagation KeyWords Plus:DATABASES","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems; Computer Science, Software Engineering","Journal Information":"ACM TRANSACTIONS ON DATABASE SYSTEMS Volume: 41 Issue: 4 Article Number: 26 DOI: 10.1145/2967608 Published: DEC 2016","Abstract":"Many databases contain temporal, or time-referenced, data and use intervals to capture the temporal aspect. While SQL-based database management systems (DBMSs) are capable of supporting the management of interval data, the support they offer can be improved considerably. A range of proposed temporal data models and query languages offer ample evidence to this effect. Natural queries that are very difficult to formulate in SQL are easy to formulate in these temporal query languages. The increased focus on analytics over historical data where queries are generally more complex exacerbates the difficulties and thus the potential benefits of a temporal query language. Commercial DBMSs have recently started to offer limited temporal functionality in a step-by-step manner, focusing on the representation of intervals and neglecting the implementation of the query evaluation engine. This article demonstrates how it is possible to extend the relational database engine to achieve a fullfledged, industrial-strength implementation of sequenced temporal queries, which intuitively are queries that are evaluated at each time point. Our approach reduces temporal queries to nontemporal queries over data with adjusted intervals, and it leaves the processing of nontemporal queries unaffected. Specifically, the approach hinges on three concepts: interval adjustment, timestamp propagation, and attribute scaling. Interval adjustment is enabled by introducing two new relational operators, a temporal normalizer and a temporal aligner, and the latter two concepts are enabled by the replication of timestamp attributes and the use of so-called scaling functions. By providing a set of reduction rules, we can transform any temporal query, expressed in terms of temporal relational operators, to a query expressed in terms of relational operators and the two new operators. We prove that the size of a transformed query is linear in the number of temporal operators in the original query. An integration of the new operators and the transformation rules, along with query optimization rules, into the kernel of PostgreSQL is reported. Empirical studies with the resulting temporal DBMS are covered that offer insights into pertinent design properties of the article's proposal. The new system is available as open-source software.","Authors":"Dignos, A (Dignos, Anton) ; Bohlen, MH (Bohlen, Michael H.) ; Gamper, J (Gamper, Johann) ; Jensen, CS (Jensen, Christian S.) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Dignos, Anton  http://orcid.org/0000-0002-7621-967X","Title":"Extending the Kernel of a Relational DBMS with Comprehensive Support for Sequenced Temporal Queries"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000393337500001 ISSN: 0954-4119 eISSN: 2041-3033","Keywords":"Relational medical diagnosis; data collection; data organization; data storage; web-based learning; knowledge creation; knowledge sharing; medical training; medical research KeyWords Plus:DECISION-SUPPORT; DISCOVERY","Categories":"Engineering Web of Science Categories:Engineering, Biomedical","Journal Information":"PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART H-JOURNAL OF ENGINEERING IN MEDICINE Volume: 230 Issue: 12 Pages: 1061-1073 DOI: 10.1177/0954411916673398 Published: DEC 2016","Abstract":"This article presents the design of a web-based knowledge management system as a training and research tool for the exploration of key relationships between Western and Traditional Chinese Medicine, in order to facilitate relational medical diagnosis integrating these mainstream healing modalities. The main goal of this system is to facilitate decision-making processes, while developing skills and creating new medical knowledge. Traditional Chinese Medicine can be considered as an ancient relational knowledge-based approach, focusing on balancing interrelated human functions to reach a healthy state. Western Medicine focuses on specialties and body systems and has achieved advanced methods to evaluate the impact of a health disorder on the body functions. Identifying key relationships between Traditional Chinese and Western Medicine opens new approaches for health care practices and can increase the understanding of human medical conditions. Our knowledge management system was designed from initial datasets of symptoms, known diagnosis and treatments, collected from both medicines. The datasets were subjected to process-oriented analysis, hierarchical knowledge representation and relational database interconnection. Web technology was implemented to develop a user-friendly interface, for easy navigation, training and research. Our system was prototyped with a case study on chronic prostatitis. This trial presented the system's capability for users to learn the correlation approach, connecting knowledge in Western and Traditional Chinese Medicine by querying the database, mapping validated medical information, accessing complementary information from official sites, and creating new knowledge as part of the learning process. By addressing the challenging tasks of data acquisition and modeling, organization, storage and transfer, the proposed web-based knowledge management system is presented as a tool for users in medical training and research to explore, learn and update relational information for the practice of integrated medical diagnosis. This proposal in education has the potential to enable further creation of medical knowledge from both Traditional Chinese and Western Medicine for improved care providing. The presented system positively improves the information visualization, learning process and knowledge sharing, for training and development of new skills for diagnosis and treatment, and a better understanding of medical diseases.","Authors":"Herrera-Hernandez, MC (Herrera-Hernandez, Maria C.) ; Lai-Yuen, SK (Lai-Yuen, Susana K.) ; Piegl, LA (Piegl, Les A.) ; Zhang, X (Zhang, Xiao)","Title":"A web-based knowledge management system integrating Western and Traditional Chinese Medicine for relational medical diagnosis"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392749500013 PubMed ID: 27531100 ISSN: 1367-4803 eISSN: 1460-2059","Categories":"Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics Web of Science Categories:Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability","Journal Information":"BIOINFORMATICS Volume: 32 Issue: 23 Pages: 3635-3644 DOI: 10.1093/bioinformatics/btw529 Published: DEC 1 2016","Abstract":"Motivation: Automatically quantifying semantic similarity and relatedness between clinical terms is an important aspect of text mining from electronic health records, which are increasingly recognized as valuable sources of phenotypic information for clinical genomics and bioinformatics research. A key obstacle to development of semantic relatedness measures is the limited availability of large quantities of clinical text to researchers and developers outside of major medical centers. Text from general English and biomedical literature are freely available; however, their validity as a substitute for clinical domain to represent semantics of clinical terms remains to be demonstrated. Results: We constructed neural network representations of clinical terms found in a publicly available benchmark dataset manually labeled for semantic similarity and relatedness. Similarity and relatedness measures computed from text corpora in three domains (Clinical Notes, PubMed Central articles and Wikipedia) were compared using the benchmark as reference. We found that measures computed from full text of biomedical articles in PubMed Central repository (rho = 0.62 for similarity and 0.58 for relatedness) are on par with measures computed from clinical reports (rho = 0.60 for similarity and 0.57 for relatedness). We also evaluated the use of neural network based relatedness measures for query expansion in a clinical document retrieval task and a biomedical term word sense disambiguation task. We found that, with some limitations, biomedical articles may be used in lieu of clinical reports to represent the semantics of clinical terms and that distributional semantic methods are useful for clinical and biomedical natural language processing applications.","Authors":"Pakhomov, SVS (Pakhomov, Serguei V. S.) ; Finley, G (Finley, Greg) ; McEwan, R (McEwan, Reed) ; Wang, Y (Wang, Yan) ; Melton, GB (Melton, Genevieve B.)","Title":"Corpus domain effects on distributional semantic modeling of medical terms"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392749500019 PubMed ID: 27522081 ISSN: 1367-4803 eISSN: 1460-2059","Categories":"Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics Web of Science Categories:Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability","Journal Information":"BIOINFORMATICS Volume: 32 Issue: 23 Pages: 3670-3672 DOI: 10.1093/bioinformatics/btw525 Published: DEC 1 2016","Abstract":"Gene Slider helps visualize the conservation and entropy of orthologous DNA and protein sequences by presenting them as one long sequence logo that can be zoomed in and out of, from an overview of the entire sequence down to just a few residues at a time. A search function enables users to find motifs such as cis-elements in promoter regions by simply 'drawing' a sequence logo representation of the desired motif as a query. In addition to displaying user-supplied FASTA files, our demonstration version of Gene Slider loads and displays a rich database of 90 000+conserved non-coding regions across the Brassicaceae indexed to the TAIR10 Col-0 Arabidopsis thaliana sequence. It also displays transcription factor binding sites, enabling easy identification of regions that are both conserved across multiple species and may contain transcription factor binding sites.","Authors":"Waese, J (Waese, Jamie) ; Pasha, A (Pasha, Asher) ; Wang, TT (Wang, Ting Ting) ; van Weringh, A (van Weringh, Anna) ; Guttman, DS (Guttman, David S.) ; Provart, NJ (Provart, Nicholas J.)","Title":"Gene Slider: sequence logo interactive data-visualization for education and research"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000392925500004 ISSN: 2307-1877 eISSN: 2307-1885","Keywords":"Attribute-based classification; label embedding; word image retrieval; word spotting KeyWords Plus:PRINTED DOCUMENT IMAGES; HIDDEN MARKOV-MODELS; QUANTIZATION; RETRIEVAL; RECOGNITION","Categories":"Engineering Web of Science Categories:Engineering, Multidisciplinary","Journal Information":"Journal of Engineering Research Volume: 4 Issue: 4 Pages: 66-80 Published: DEC 2016","Abstract":"The aim of word spotting, as a particular case of semantic content based image retrieval (CBIR), is to find instances of a word query given as an image or text (string) in a document. In this paper, we propose a holistic approach for Persian handwritten word spotting. In addition, the proposed method can be used for handwritten word recognition. This is achieved by a combination of label embedding; attribute based classification and common subspace regression. In this subspace, image and string representation of the same word are close together, so it allows for considering recognition and retrieval task as a nearest neighbor problem. Unlike existing methods in word spotting, the suggested representation for a word has fixed length and low dimensionality. On the other hand, the feature extraction process is very fast. We used Farsa and Iranshahr, two common datasets of isolated Persian handwritten words, to evaluate the proposed method. The result of experiments for word spotting is promising. The recognition rates for isolated handwritten words in Farsa and Iranshahr datasets are 100% and 97% respectively.","Authors":"Mobarakeh, MI (Mobarakeh, Majid Iranpour) ; Ahmadyfard, A (Ahmadyfard, Alireza)","Title":"Isolated Persian/Arabic word spotting by label embedding"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000391726300004 ISSN: 1556-4673 eISSN: 1556-4711","Keywords":"Fisher vector; convolutional neural network; epigraphy; Latin and Greek inscriptions KeyWords Plus:QUANTIZATION","Categories":"Arts & Humanities - Other Topics; Computer Science Web of Science Categories:Humanities, Multidisciplinary; Computer Science, Interdisciplinary Applications","Journal Information":"ACM JOURNAL ON COMPUTING AND CULTURAL HERITAGE Volume: 9 Issue: 4 Article Number: 21 DOI: 10.1145/2964911 Published: DEC 2016","Abstract":"By bringing together the most prominent European institutions and archives in the field of Classical Latin and Greek epigraphy, the EAGLE project has collected the vast majority of the surviving Greco-Latin inscriptions into a single readily-searchable database. Text-based search engines are typically used to retrieve information about ancient inscriptions (or about other artifacts). These systems require that the users formulate a text query that contains information such as the place where the object was found or where it is currently located. Conversely, visual search systems can be used to provide information to users (like tourists and scholars) in a most intuitive and immediate way, just using an image as query. In this article, we provide a comparison of several approaches for visual recognizing ancient inscriptions. Our experiments, conducted on 17, 155 photos related to 14, 560 inscriptions, show that BoW and VLAD are outperformed by both Fisher Vector (FV) and Convolutional Neural Network (CNN) features. More interestingly, combining FV and CNN features into a single image representation allows achieving very high effectiveness by correctly recognizing the query inscription in more than 90% of the cases. Our results suggest that combinations of FV and CNN can be also exploited to effectively perform visual retrieval of other types of objects related to cultural heritage such as landmarks and monuments.","Authors":"Amato, G (Amato, Giuseppe) ; Falchi, F (Falchi, Fabrizio) ; Vadicamo, L (Vadicamo, Lucia) Hide ResearcherID and ORCIDView ResearcherID and ORCID Author ResearcherID ORCID Number Amato, Giuseppe  F-2227-2013 http://orcid.org/0000-0003-0171-4315 Falchi, Fabrizio  J-2920-2012 http://orcid.org/0000-0001-6258-5313 Falchi, Fabrizio  B-5160-2015 http://orcid.org/0000-0001-6258-5313","Title":"Visual Recognition of Ancient Inscriptions Using Convolutional Neural Network and Fisher Vector"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000391724600007 ISSN: 1046-8188 eISSN: 1558-2868","Keywords":"Learning to rank; additive ensembles of regression trees; document scoring; efficiency; cache-awareness","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"ACM TRANSACTIONS ON INFORMATION SYSTEMS Volume: 35 Issue: 2 Article Number: 15 DOI: 10.1145/2987380 Published: DEC 2016","Abstract":"Learning-to-Rank models based on additive ensembles of regression trees have been proven to be very effective for scoring query results returned by large-scale Web search engines. Unfortunately, the computational cost of scoring thousands of candidate documents by traversing large ensembles of trees is high. Thus, several works have investigated solutions aimed at improving the efficiency of document scoring by exploiting advanced features of modern CPUs and memory hierarchies. In this article, we present QUICKSCORER, a new algorithm that adopts a novel cache-efficient representation of a given tree ensemble, performs an interleaved traversal by means of fast bitwise operations, and supports ensembles of oblivious trees. An extensive and detailed test assessment is conducted on two standard Learning-to-Rank datasets and on a novel very large dataset we made publicly available for conducting significant efficiency tests. The experiments show unprecedented speedups over the best state-of-the-art baselines ranging from 1.9x to 6.6x. The analysis of low-level profiling traces shows that QUICKSCORER efficiency is due to its cache-aware approach in terms of both data layout and access patterns and to a control flow that entails very low branch mis-prediction rates.","Authors":"Dato, D (Dato, Domenico) ; Lucchese, C (Lucchese, Claudio) ; Nardini, FM (Nardini, Franco Maria) ; Orlando, S (Orlando, Salvatore) ; Perego, R (Perego, Raffaele) ; Tonellotto, N (Tonellotto, Nicola) ; Venturini, R (Venturini, Rossano)","Title":"Fast Ranking with Additive Ensembles of Oblivious and Non-Oblivious Regression Trees"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000391188500003 ISSN: 1861-2032 eISSN: 1861-2040","Keywords":"Query language; Spatio-temporal ontology KeyWords Plus:ALLENS INTERVAL ALGEBRA; DESCRIPTION LOGICS; TEMPORAL RELATIONS; COMPLEXITY","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems","Journal Information":"JOURNAL ON DATA SEMANTICS Volume: 5 Issue: 4 Pages: 249-269 DOI: 10.1007/s13740-016-0064-5 Published: DEC 2016","Abstract":"We introduce SOWL QL, a query language for spatio-temporal information in ontologies. Building-upon SOWL (Spatio-Temporal OWL), an ontology for handling spatio-temporal information in OWL, SOWL QL supports querying over qualitative spatio-temporal information (expressed using natural language expressions such as \"before\", \"after\", \"north of\", \"south of\") rather than merely quantitative information (exact dates, times, locations). SOWL QL extends SPARQL with a powerful set of temporal and spatial operators, including temporal Allen topological, spatial directional and topological operations or combinations of the above. SOWL QL maintains simplicity of expression, and also upward and downward compatibility with SPARQL. Query translation in SOWL QL yields SPARQL queries, implying that querying spatio-temporal ontologies using SPARQL is still feasible but suffers from several drawbacks, the most important of them being that, queries in SPARQL become particularly complicated and users must be familiar with the underlying spatio-temporal representation (the \"N-ary relations\" or the \"4D-fluents\" approach in this work). Finally, querying in SOWL QL is supported by the SOWL reasoner which is not part of the standard SPARQL translation. The run-time performance of SOWL QL has been assessed experimentally in a real data setting. A critical analysis of its performance is also presented.","Authors":"Stravoskoufos, K (Stravoskoufos, Konstantinos) ; Petrakis, EGM (Petrakis, Euripides G. M.) ; Mainas, N (Mainas, Nikolaos) ; Batsakis, S (Batsakis, Sotirios) ; Samoladas, V (Samoladas, Vasilis)","Title":"SOWL QL: Querying Spatio-Temporal Ontologies in OWL"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000390423900013 ISSN: 1051-8215 eISSN: 1558-2205","Keywords":"Airborne; CCTV; dynamic programming (DP); surveillance; tracklets; video retrieval KeyWords Plus:UNUSUAL-EVENT DETECTION","Categories":"Engineering Web of Science Categories:Engineering, Electrical & Electronic","Journal Information":"IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY Volume: 26 Issue: 12 Pages: 2313-2327 DOI: 10.1109/TCSVT.2015.2473295 Published: DEC 2016","Abstract":"We present a content-based retrieval method for long-surveillance videos in wide-area (airborne) and nearfield [closed-circuit television (CCTV)] imagery. Our goal is to retrieve video segments, with a focus on detecting objects moving on routes, that match user-defined events of interest. The sheer size and remote locations where surveillance videos are acquired necessitates highly compressed representations that are also meaningful for supporting user-defined queries. To address these challenges, we archive long-surveillance video through lightweight processing based on low-level local spatiotemporal extraction of motion and object 2. These are then hashed into an inverted index using locality-sensitive hashing. This local approach allows for query flexibility and leads to significant gains in compression. Our second task is to extract partial matches to user-created queries and assemble them into full matches using dynamic programming (DP). DP assembles the indexed low-level features into a video segment that matches the query route by exploiting causality. We examine CCTV and airborne footage, whose low contrast makes motion extraction more difficult. We generate robust motion estimates for airborne data using a tracklets generation algorithm, while we use the Horn and Schunck approach to generate motion estimates for CCTV. Our approach handles long routes, low contrasts, and occlusion. We derive bounds on the rate of false positives and demonstrate the effectiveness of the approach for counting, motion pattern recognition, and abandoned object applications.","Authors":"Castanon, G (Castanon, Gregory) ; Elgharib, M (Elgharib, Mohamed) ; Saligrama, V (Saligrama, Venkatesh) ; Jodoin, PM (Jodoin, Pierre-Marc)","Title":"Retrieval in Long-Surveillance Videos Using User-Described Motion and Object Attributes"}, {"Document Information":"Document Type:Article Language:English Accession Number: WOS:000388972000025 ISSN: 1386-7857 eISSN: 1573-7543","Keywords":"Triangular mesh; LIDAR; CSG-BRep; Topological model; Boolean operations; Spatial topological query KeyWords Plus:BOOLEAN OPERATIONS","Categories":"Computer Science Web of Science Categories:Computer Science, Information Systems; Computer Science, Theory & Methods","Journal Information":"Cluster Computing-The Journal of Networks Software Tools and Applications Volume: 19 Issue: 4 Pages: 2027-2037 Special Issue: SI DOI: 10.1007/s10586-016-0634-1 Published: DEC 2016","Abstract":"Ordinary triangular mesh model can be constructed from discrete point cloud. However, this kind of model contains large amount of data. It is not only difficult to split, but also lacks topological relation information. We proposed a CSG-BRep (Constructive Solid Geometry-Boundary Representation) topological model to overcome these problems. CSG-BRep model can record topological relationship of 3D model in great detail. We first introduced aspects of the topological model: location, orientation and sub-shape. Then we proposed two algorithms to access topological structure. We also proposed algorithms for performing Boolean operation on CSG-BRep models. Finally, we demonstrated CSG-BRep construction using LIDAR point cloud as a data source. We would show that, compared to ordinary triangular mesh model, CSG-BRep model is composable and can effectively reduce data volume. In addition, CSG-BRep model has detailed topological relation information, allowing further querying and analysis of 3D spatial topological information.","Authors":"Huang, M (Huang Ming) ; Du, YZ (Du Yanzhu) ; Zhang, JG (Zhang Jianguang) ; Zhang, Y (Zhang Yong)","Title":"A topological enabled three-dimensional model based on constructive solid geometry and boundary representation"}]