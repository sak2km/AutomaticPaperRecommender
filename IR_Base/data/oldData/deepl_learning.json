{"Reviews":[{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"0","Author":"","Content":"Predictive modeling in medicine involves the development of computational models which are capable of analysing large amounts of data in order to predict healthcare outcomes for individual patients. Computational intelligence approaches are suitable when the data to be modelled are too complex for conventional statistical techniques to process quickly and efficiently. These advanced approaches are based on mathematical models that have been especially developed for dealing with the uncertainty and imprecision which is typically found in clinical and biological datasets. This paper provides a survey of recent work on computational intelligence approaches that have been applied to prostate cancer predictive modeling, and considers the challenges which need to be addressed. In particular, the paper considers a broad definition of computational intelligence which includes metaheuristic optimisation algorithms (also known as nature inspired algorithms), Artificial Neural Networks, Deep Learning, Fuzzy based approaches, and hybrids of these, as well as Bayesian based approaches, and Markov models. Metaheuristic optimisation approaches, such as the Ant Colony Optimisation, Particle Swarm Optimisation, and Artificial Immune Network have been utilised for optimising the performance of prostate cancer predictive models, and the suitability of these approaches are discussed. (C) 2016 Elsevier Ltd. All rights reserved.","Title":"A survey on computational intelligence approaches for predictive modeling in prostate cancer"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"1","Author":"","Content":"Real-world optimization problems typically involve multiple objectives to be optimized simultaneously under multiple constraints and with respect to several variables. While multi-objective optimization itself can be a challenging task, equally difficult is the ability to make sense of the obtained solutions. In this two-part paper, we deal with data mining methods that can be applied to extract knowledge about multi-objective optimization problems from the solutions generated during optimization. This knowledge is expected to provide deeper insights about the problem to the decision maker, in addition to assisting the optimization process in future design iterations through an expert system. The current paper surveys several existing data mining methods and classifies them by methodology and type of knowledge discovered. Most of these methods come from the domain of exploratory data analysis and can be applied to any multivariate data. We specifically look at methods that can generate explicit knowledge in a machine-usable form. A framework for knowledge-driven optimization is proposed, which involves both online and offline elements of knowledge discovery. One of the conclusions of this survey is that while there are a number of data mining methods that can deal with data involving continuous variables, only a few ad hoc methods exist that can provide explicit knowledge when the variables involved are of a discrete nature. Part B of this paper proposes new techniques that can be used with such datasets and applies them to discrete variable multi-objective problems related to production systems. (C) 2016 Elsevier Ltd. All rights reserved.","Title":"Data mining methods for knowledge discovery in multi-objective optimization: Part A - Survey"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"2","Author":"","Content":"A novel technique of quantitative EEG for differentiating patients with early-stage Creutzfeldt-Jakob disease (CJD) from other forms of rapidly progressive dementia (RPD) is proposed. The discrimination is based on the extraction of suitable features from the time-frequency representation of the EEG signals through continuous wavelet transform (CWT). An average measure of complexity of the EEG signal obtained by permutation entropy (PE) is also included. The dimensionality of the feature space is reduced through a multilayer processing system based on the recently emerged deep learning (DL) concept. The DL processor includes a stacked auto-encoder, trained by unsupervised learning techniques, and a classifier whose parameters are determined in a supervised way by associating the known category labels to the reduced vector of high-level features generated by the previous processing blocks. The supervised learning step is carried out by using either support vector machines (SVM) or multilayer neural networks (MLP-NN). A subset of EEG from patients suffering from Alzheimer's Disease (AD) and healthy controls (HC) is considered for differentiating CJD patients. When fine-tuning the parameters of the global processing system by a supervised learning procedure, the proposed system is able to achieve an average accuracy of 89%, an average sensitivity of 92%, and an average specificity of 89% in differentiating CJD from RPD. Similar results are obtained for CJD versus AD and CJD versus HC.","Title":"Deep Learning Representation from Electroencephalography of Early-Stage Creutzfeldt-Jakob Disease and Features for Differentiation from Rapidly Progressive Dementia"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"3","Author":"","Content":"The concepts and instruments required for the teaching and learning of geometric optics are introduced in the didactic process without a proper didactic transposition. This claim is secured by the ample evidence of both wide-and deep-rooted alternative concepts on the topic. Didactic transposition is a theory that comes from a reflection on the teaching and learning process in mathematics but has been used in other disciplinary fields. It will be used in this work in order to clear up the main obstacles in the teaching-learning process of geometric optics. We proceed to argue that since Newton's approach to optics, in his Book I of Opticks, is independent of the corpuscular or undulatory nature of light, it is the most suitable for a constructivist learning environment. However, Newton's theory must be subject to a proper didactic transposition to help overcome the referred alternative concepts. Then is described our didactic transposition in order to create knowledge to be taught using a dialogical process between students' previous knowledge, history of optics and the desired outcomes on geometrical optics in an elementary pre-service teacher training course. Finally, we use the scheme-facet structure of knowledge both to analyse and discuss our results as well as to illuminate shortcomings that must be addressed in our next stage of the inquiry.","Title":"A Didactic Sequence of Elementary Geometric Optics Informed by History and Philosophy of Science"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"4","Author":"","Content":"We introduce a set of benchmark corpora of conversational English speech derived from the Switchboard-I and Fisher datasets. Traditional automatic speech recognition (ASR) research requires considerable computational resources and has slow experimental turnaround times. Our goal is to introduce these new datasets to researchers in the ASR and machine learning communities in order to facilitate the development of novel speech recognition techniques on smaller but still acoustically rich, diverse, and hence interesting corpora. We select these corpora to maximize an acoustic quality criterion while limiting the vocabulary size (from 10 words up to 10,000 words), where both \"acoustic quality\" and vocabulary size are adeptly measured via various submodular functions. We also survey numerous submodular functions that could be useful to measure both \"acoustic quality\" and \"corpus complexity\" and offer guidelines on when and why a scientist may wish use to one vs. another. The corpora selection process itself is naturally performed using various state-of-the-art submodular function optimization procedures, including submodular level-set constrained submodular optimization (SCSC/SCSK), difference-of-submodular (DS) optimization, and unconstrained submodular minimization (SFM), all of which are fully defined herein. While the focus of this paper is on the resultant speech corpora, and the survey of possible objectives, a consequence of the paper is a thorough empirical comparison of the relative merits of these modern submodular optimization procedures. We provide baseline word recognition results on all of the resultant speech corpora for both Gaussian mixture model (GMM) and deep neural network (DNN)-based systems, and we have released all of the corpora definitions and Kaldi training recipes for free in the public domain. (C) 2016 Elsevier Ltd. All rights reserved.","Title":"SVitchboard-II and FiSVer-I: Crafting high quality and low complexity conversational english speech corpora using submodular function optimization"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"5","Author":"","Content":"Conference Conference: 4th International Conference on Model and Data Engineering (MEDI) Location: Larnaca, CYPRUS Date: SEP 24-26, 2014 Sponsor(s):CNRS; ENSEEIHT Toulouse; IRIT; INP Toulouse; ISAE ENSMA; LIAS; Univ Cyprus","Title":"Modeling user behavior data in systems of engagement"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"6","Author":"","Content":"Spatial features of hyperspectral imagery (HSI) have gained an increasing attention in the latest years. Considering deep convolutional neural network (CNN) can extract a hierarchy of increasingly spatial features, this paper proposes an HSI reconstruction model based on deep CNN to enhance spatial features. The framework proposes a new spatial features-based strategy for band selection to define training label with rich information for the first time. Then, hyperspectral data is trained by deep CNN to build a model with optimized parameters which is suitable for HSI reconstruction. Finally, the reconstructed image is classified by the efficient extreme learning machine (ELM) with a very simple structure. Experimental results indicate that framework built based on CNN and ELM provides competitive performance with small number of training samples. Specifically, by using the reconstructed image, the average accuracy of ELM can be improved as high as 30.04%, while performs tens to hundreds of times faster than those state-of-the-art classifiers.","Title":"Hyperspectral image reconstruction by deep convolutional neural network for classification"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"7","Author":"","Content":"End-to-end learning machines enable a direct mapping from the raw input data to the desired outputs, eliminating the need for hand-crafted features. Despite less engineering effort than the hand-crafted counterparts, these learning machines achieve extremely good results for many computer vision and medical image analysis tasks. Two dominant classes of end-to-end learning machines are massive-training artificial neural networks (MTANNs) and convolutional neural networks (CNNs). Although MTANNs have been actively used for a number of medical image analysis tasks over the past two decades, CNNs have recently gained popularity in the field of medical imaging. In this study, we have compared these two successful learning machines both experimentally and theoretically. For that purpose, we considered two well-studied topics in the field of medical image analysis: detection of lung nodules and distinction between benign and malignant lung nodules in computed tomography (CT). For a thorough analysis, we used 2 optimized MTANN architectures and 4 distinct CNN architectures that have different depths. Our experiments demonstrated that the performance of MTANNs was substantially higher than that of CNN when using only limited training data. With a larger training dataset, the performance gap became less evident even though the margin was still significant. Specifically, for nodule detection, MTANNs generated 2.7 false positives per patient at 100% sensitivity, which was significantly (p < 0.05) lower than the best performing CNN model with 22.7 false positives per patient at the same level of sensitivity. For nodule classification, MTANNs yielded an area under the receiver-operating-characteristic curve (AUC) of 0.8806 (95% CI: 0.8389-0.9223), which was significantly (p < 0.05) greater than the best performing CNN model with an AUC of 0.7755 (95% CI: 0.7120-0.8270). Thus, with limited training data, MTANNs would be a suitable end-to-end machine-learning model for detection and classification of focal lesions that do not require high-level semantic features.","Title":"Comparing two classes of end-to-end machine-learning models in lung nodule detection and classification: MTANNs vs. CNNs"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"8","Author":"","Content":"In this study, we develop a novel nonlinear metric learning method to improve biomarker identification for Alzheimer's Disease (AD) and Mild Cognitive Impairment (MCI). Formulated under a constrained optimization framework, the proposed method learns a smooth nonlinear feature space transformation that makes the mapped data more linearly separable for SVMs. The thin-plate spline (TPS) is chosen as the geometric model due to its remarkable versatility and representation power in generating sophisticated yet smooth deformations. In addition, a deep network based feature fusion strategy through stacked denoising sparse auto-encoder (DSAE) is adopted to integrate cross-sectional and longitudinal features estimated from MR brain images. Using the ADNI dataset, we evaluate the effectiveness of the proposed feature transformation and feature fusion strategies and demonstrate the improvements over the state-of-the-art solutions within the same category.","Title":"Nonlinear feature transformation and deep fusion for Alzheimer's Disease staging analysis"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"9","Author":"","Content":"Recommender system is a specific type of intelligent systems, which exploits historical user ratings on items and/or auxiliary information to make recommendations on items to the users. It plays a critical role in a wide range of online shopping, e-commercial services and social networking applications. Collaborative filtering (CF) is the most popular approaches used for recommender systems, but it suffers from complete cold start (CCS) problem where no rating record are available and incomplete cold start (ICS) problem where only a small number of rating records are available for some new items or users in the system. In this paper, we propose two recommendation models to solve the CCS and ICS problems for new items, which are based on a framework of tightly coupled CF approach and deep learning neural network. A specific deep neural network SADE is used to extract the content features of the items. The state of the art CF model, timeSVD++, which models and utilizes temporal dynamics of user preferences and item features, is modified to take the content features into prediction of ratings for cold start items. Extensive experiments on a large Netflix rating dataset of movies are performed, which show that our proposed recommendation models largely outperform the baseline models for rating prediction of cold start items. The two proposed recommendation models are also evaluated and compared on ICS items, and a flexible scheme of model retraining and switching is proposed to deal with the transition of items from cold start to non-cold start status. The experiment results on Netflix movie recommendation show the tight coupling of CF approach and deep learning neural network is feasible and very effective for cold start item recommendation. The design is general and can be applied to many other recommender systems for online shopping and social networking applications. The solution of cold start item problem can largely improve user experience and trust of recommender systems, and effectively promote cold start items. (C) 2016 Elsevier Ltd. All rights reserved.","Title":"Collaborative filtering and deep learning based recommendation system for cold start items"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"10","Author":"","Content":"A majority of gastrointestinal infectious diseases are caused by food contamination, and prediction of morbidity can be very useful for etiological factor controlling and medical resource utilization. However, an accurate prediction is often very difficult not only because there are various types of food and contaminants, but also because the relationship between the diseases and the contaminants is highly complex and probabilistic. In this study, we use the deep denoising autoencoder (DDAE) to model the effect of food contamination on gastrointestinal infections, and thus provide a valuable tool for morbidity prediction. For effectively training the model with high-dimensional input data, we propose an evolutionary learning algorithm based on ecogeography-based optimization (EBO) in order to avoid premature convergence. Experimental results show that our evolutionary deep learning model obtains a much higher prediction accuracy than the shallow artificial neural network (ANN) model and the DDAE with other learning algorithms on a real-world dataset.","Title":"An evolutionary deep neural network for predicting morbidity of gastrointestinal infections by food contamination"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"11","Author":"","Content":"Feature matters. In this paper, a novel deep neural network framework integrated with low-level features for salient object detection is proposed for complex images. We utilise the advantage of convolutional neural networks to automatically learn the high-level features that capture the structured information and semantic context in the image. In order to better adapt a CNN model into the saliency task, we redesign the network architecture based on typical saliency datasets, which is relatively small-scale compared to ImageNet. Several low-level features are extracted, which can effectively capture contrast and spatial information in the salient regions, and incorporated to compensate with the learned high-level features at the output of the very last fully connected layer. The concatenated feature vector is further fed into a hinge-loss SVM detector in a joint discriminative learning manner and the final saliency score of each region within the bounding box is obtained by the linear combination of the detector's weights. Experiments on three challenging benchmarks demonstrate our algorithm to be effective and superior than most low-level oriented state-of-the-arts in terms of precision recall curves, F-measure and mean absolute errors. Moreover, a series of ablation studies are conducted to verify our algorithm's simplicity and efficiency from different aspects.","Title":"CNN for saliency detection with low-level feature integration"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"12","Author":"","Content":"Fetal viability, gestational age, and complicated image processing have made evaluating placental maturity a tedious and time-consuming task. Despite various developments, automatic placental maturity still remains as a challenging issue. To address this issue, we propose a new method to automatically grade placental maturity from B-mode ultrasound (BUS) and color Doppler energy (CDE) images based on a hybrid learning architecture. We also apply an improved pyramidal shift invariant feature transform (IPSIFT) descriptor using a coarse-to-fine scale representation for visual feature extraction. These local features are then clustered by a generative Gaussian mixture model (GMM) to incorporate high order statistics. Next, the clustering representatives are encoded and aggregated via Fisher vector (FV). Instead of using traditional FV, an end to -end deep training strategy is developed to fine-tune the GMM parameters to boost evaluation performance. A multi-view fusion technique is also developed for feature complementarity exploration. Extensive experimental results demonstrate that our method delivers promising performance in placental maturity evaluation and outperforms competing methods.","Title":"Automatic placental maturity grading via hybrid learning"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"13","Author":"","Content":"This study investigated profiles of autonomous and controlled motivation and their effects in a sample of 188 adult learners from two Portuguese urban areas. Using a person-centered approach, results of cluster analysis and multivariate analysis of covariance revealed four motivational groups with different effects in self-efficacy, engagement, and learning. The study showed that groups of learners who have high autonomous motivation in the beginning of a course score higher in self-efficacy and later on in behavioral engagement and use of deep-learning strategies, whereas those who have controlled motivation alone or low levels of both types of motivation have worse results. Additionally, the study showed motivational differences according to adult learners' gender, educational level, and occupational status. The influence of the Portuguese adult education system on the results and the implications of the study for the practice of adult education are also discussed.","Title":"Motivational Profiles of Adult Learners"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"14","Author":"","Content":"Conference Conference: IEEE International Conference on Internet of People (IoP) Location: Beijing, PEOPLES R CHINA Date: AUG 10-14, 2015 Sponsor(s):IEEE","Title":"AL-DDCNN: a distributed crossing semantic gap learning for person re-identification"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"15","Author":"","Content":"Studies on catches of anglers usually rely on observational data and are thus uncontrolled with respect to angler skill, bait/lure choice and site choice. We performed a controlled fishing experiment targeting northern pike (Esox lucius) in a small (25 ha), weakly eutrophic natural lake situated about 80 km northeast of Berlin (Germany) to understand abiotic, biotic and gear-related factors that relate to catch rates and size of pike captured by angling. The experiment was conducted over two one-week long fishing campaigns where boat-based anglers randomly sampled across 30 pre-determined sites. Sites were systematically exposed to two standardized lure types (soft plastic shad or spoon). We found the catch rates of pike per 15 min to be significantly higher in shallow water and when soft plastic lures were used compared to deeper water and when spoons were used. Catch rates significantly dropped over the course of seven days, suggesting either learning or other reasons moving pike from vulnerable to invulnerable pools (e.g., due to stress caused by capture, sampling and release). Catch rates also varied by season and across anglers and sites as random effects. The variation in size of pike captured exhibited greater stochasticity than variation in catch rate. There was no lure effect on the size of the pike captured, but we found a seasonal effect and a day effect, suggesting larger fish were captured first. Pike captured in sublittoral areas were significantly smaller than those captured in other habitats. Overall, our study documented a novel effect of lure type on the catch rates of pike, but the explanatory power of the predictors was only moderate. Therefore, our results support the idea that the best fishing ingredients are investing time and maximizing encounter probabilities through habitat choice, with only moderate additional effects to be expected from attention to abiotic conditions, day time and choice of type of artificial lure. (C) 2016 Elsevier B.V. All rights reserved.","Title":"Determinants of angling catch of northern pike (Esox lucius) as revealed by a controlled whole-lake catch-and-release angling experiment-The role of abiotic and biotic factors, spatial encounters and lure type"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"16","Author":"","Content":"Engineering students on control courses lack a deep understanding of equilibrium and stability that are crucial concepts in this discipline. Several studies have shown that students find it difficult to understand simple familiar or academic static equilibrium cases as well as dynamic ones from mechanics even if they know the discipline's criteria and formulae. Our aim is to study the impact of a specific and innovative classroom session, containing well-chosen situations that address students' misconceptions. We propose an example of Active Learning experiment based both on the Didactical Engineering methodology and the Conceptual Fields Theory that aims at promoting a conceptual change in students. The chosen methodology allows, at the same time, a proper design of the student learning activities, an accurate monitoring of the students' rational use during the tasks and provides an internal tool for the evaluation of the session's efficiency. Although the expected starting conceptual change was detected, it would require another activity in order to be reinforced.","Title":"Active Learning session based on Didactical Engineering framework for conceptual change in students' equilibrium and stability understanding"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"17","Author":"","Content":"Background: In recent years, the literature associated with healthcare big data has grown rapidly, but few studies have used bibliometrics and a visualization approach to conduct deep mining and reveal a panorama of the healthcare big data field. Methods: To explore the foundational knowledge and research hotspots of big data research in the field of healthcare informatics, this study conducted a series of bibliometric analyses on the related literature, including papers' production trends in the field and the trend of each paper's co-author number, the distribution of core institutions and countries, the core literature distribution, the related information of prolific authors and innovation paths in the field, a keyword co-occurrence analysis, and research hotspots and trends for the future. Results: By conducting a literature content analysis and structure analysis, we found the following: (a) In the early stage, researchers from the United States, the People's Republic of China, the United Kingdom, and Germany made the most contributions to the literature associated with healthcare big data research and the innovation path in this field. (b) The innovation path in healthcare big data consists of three stages: the disease early detection, diagnosis, treatment, and prognosis phase, the life and health promotion phase, and the nursing phase. (c) Research hotspots are mainly concentrated in three dimensions: the disease dimension (e.g., epidemiology, breast cancer, obesity, and diabetes), the technical dimension (e.g., data mining and machine learning), and the health service dimension (e.g., customized service and elderly nursing). Conclusion: This study will provide scholars in the healthcare informatics community with panoramic knowledge of healthcare big data research, as well as research hotspots and future research directions. (C) 2016 Elsevier Ireland Ltd. All rights reserved.","Title":"Visualizing the knowledge structure and evolution of big data research in healthcare informatics"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"18","Author":"","Content":"Objective. Signal classification is an important issue in brain computer interface (BCI) systems. Deep learning approaches have been used successfully in many recent studies to learn features and classify different types of data. However, the number of studies that employ these approaches on BCI applications is very limited. In this study we aim to use deep learning methods to improve classification performance of EEG motor imagery signals. Approach. In this study we investigate convolutional neural networks (CNN) and stacked autoencoders (SAE) to classify EEG Motor Imagery signals. A new form of input is introduced to combine time, frequency and location information extracted from EEG signal and it is used in CNN having one 1D convolutional and one max-pooling layers. We also proposed a new deep network by combining CNN and SAE. In this network, the features that are extracted in CNN are classified through the deep network SAE. Main results. The classification performance obtained by the proposed method on BCI competition IV dataset 2b in terms of kappa value is 0.547. Our approach yields 9% improvement over the winner algorithm of the competition. Significance. Our results show that deep learning methods provide better classification performance compared to other state of art approaches. These methods can be applied successfully to BCI systems where the amount of data is large due to daily recording.","Title":"A novel deep learning approach for classification of EEG motor imagery signals"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"19","Author":"","Content":"We present methods of extractive query-oriented single-document summarization using a deep auto encoder (AE) to compute a feature space from the term-frequency (tf) input. Our experiments explore both local and global vocabularies. We investigate the effect of adding small random noise to local tf as the input representation of AE, and propose an ensemble of such noisy AEs which we call the Ensemble Noisy Auto-Encoder (ENAE). ENAE is a stochastic version of an AE that adds noise to the input text and selects the top sentences from an ensemble of noisy runs. In each individual experiment of the ensemble, a different randomly generated noise is added to the input representation. This architecture changes the application of the AE from a deterministic feed-forward network to a stochastic runtime model. Experiments show that the AE using local vocabularies clearly provide a more discriminative feature space and improves the recall on average 11.2%. The ENAE can make further improvements, particularly in selecting informative sentences. To cover a wide range of topics and structures, we perform experiments on two different publicly available email corpora that are specifically designed for text summarization. We used ROUGE as a fully automatic metric in text summarization and we presented the average ROUGE-2 recall for all experiments. (C) 2016 Elsevier Ltd. All rights reserved.","Title":"Text summarization using unsupervised deep learning"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"20","Author":"","Content":"Local Binary Patterns (LBP) have emerged as one of the most prominent and widely studied local texture descriptors. Truly a large number of LBP variants has been proposed, to the point that it can become overwhelming to grasp their respective strengths and weaknesses, and there is a need for a comprehensive study regarding the prominent LBP-related strategies. New types of descriptors based on multistage convolutional networks and deep learning have also emerged. In different papers the performance comparison of the proposed methods to earlier approaches is mainly done with some well-known texture datasets, with differing classifiers and testing protocols, and often not using the best sets of parameter values and multiple scales for the comparative methods. Very important aspects such as computational complexity and effects of poor image quality are often neglected. In this paper, we provide a systematic review of current LBP variants and propose a taxonomy to more clearly group the prominent alternatives. Merits and demerits of the various LBP features and their underlying connections are also analyzed. We perform a large scale performance evaluation for texture classification, empirically assessing forty texture features including thirty two recent most promising LBP variants and eight non-LBP descriptors based on deep convolutional networks on thirteen widely-used texture datasets. The experiments are designed to measure their robustness against different classification challenges, including changes in rotation, scale, illumination, viewpoint, number of classes, different types of image degradation, and computational complexity. The best overall performance is obtained for the Median Robust Extended Local Binary Pattern (MRELBP) feature. For textures with very large appearance variations, Fisher vector pooling of deep Convolutional Neural Networks is clearly the best, but at the cost of very high computational complexity. The sensitivity to image degradations and computational complexity are among the key problems for most of the methods considered. (C) 2016 Elsevier Ltd. All rights reserved.","Title":"Local binary features for texture classification: Taxonomy and experimental study"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"21","Author":"","Content":"This paper addresses the reconstruction of sparse vectors in the Multiple Measurement Vectors (MMV) problem in compressive sensing, where the sparse vectors are correlated. This problem has so far been studied using model based and Bayesian methods. In this paper, we propose a deep learning approach that relies on a Convolutional Deep Stacking Network (CDSN) to capture the dependency among the different channels. To reconstruct the sparse vectors, we propose a greedy method that exploits the information captured by CDSN. The proposed method encodes the sparse vectors using random measurements (as done usually in compressive sensing). Experiments using a real world image dataset show that the proposed method outperforms the traditional MMV solver, i.e., Simultaneous Orthogonal Matching Pursuit (SOMP), as well as three of the Bayesian methods proposed for solving the MMV compressive sensing problem. We also show that the proposed method is almost as fast as greedy methods. The good performance of the proposed method depends on the availability of training data (as is the case in all deep learning methods). The training data, e.g., different images of the same class or signals with similar sparsity patterns are usually available for many applications. (C) 2016 Elsevier B.V. All rights reserved.","Title":"Convolutional Deep Stacking Networks for distributed compressive sensing"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"22","Author":"","Content":"This paper describes the fabrication and characterization of Lithium Niobate (LiNbO3) memristor devices that have the ability to be tuned to a specific resistance state within a continuous resistance range. This is essential for programming neuromorphic systems based on memristor crossbars in order to achieve best deep learning capability. The memristor devices were formed using a 42 nm layer of LiNbO3 sandwiched between two metal electrodes. I-V curves demonstrate a typical and repeatable memristor characteristic from - 3 V to 3 V. Such devices have a continuous resistance range that has a maximum to minimum resistance ratio of about 100, and the ability to program intermediate resistance states. The results also show the ability to read the device symmetrically with a positive or negative voltage, and strong data retention after the programming phase. (C) 2016 Elsevier B.V. All rights reserved.","Title":"Experimental study of LiNbO3 memristors for use in neuromorphic computing"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"23","Author":"","Content":"Corporate sustainability, which has become essential to most companies in the last decades, stipulates that environmental requirements should be incorporated into diverse business processes. To effectively integrate environmental aspects into product innovation processes, companies might have to significantly change some of the practices and habits of all the stakeholders involved and of the organisation. To complement the extensive literature on the (technical) \"hard side of ecodesign\", this article explores the promising \"soft side\", which considers company culture and human factors, by a multiple step literature review associated with a longitudinal action research in a large cosmetics company. Although a consistent prescriptive change model is still lacking in ecodesign literature, a strong convergence and complementarity is observed between the previous conclusions on ecodesign integration models and the emerging Transition Management approach designed for the sustainability issues faced by organisations. As a result, an \"ecodesign transition framework\" is proposed by combining a three-level systemic approach, considering both top-down planning and bottom-up innovation, with new types of interaction and dynamic cycles of action and learning, with a deep stakeholder management. This new framework was developed and positively applied to the company in a five-year experience to face the complex transition process, thus advancing the knowledge from social science for innovation and sustainability management challenges. Such approach could positively address change management issues and help companies evolve toward a more effective sustainable product innovation process, in the context of evolving business management practices that require progressive change and more human-based strategies. (C) 2016 Elsevier Ltd. All rights reserved.","Title":"Reviews, action and learning on change management for ecodesign transition"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"24","Author":"","Content":"Machine learning methods incorporating deep neural networks have been the subject of recent proposals for new hadronic resonance taggers. These methods require training on a data set produced by an event generator where the true class labels are known. However, this may bias the network towards learning features associated with the approximations to QCD used in that generator which are not present in real data. We therefore investigate the effects of variations in the modeling of the parton shower on the performance of deep neural network taggers using jet images from hadronic W bosons at the LHC, including detector-related effects. By investigating network performance on samples from the Pythia, Herwig and Sherpa generators, we find differences of up to 50% in background rejection for fixed signal efficiency. We also introduce and study a method, which we dub zooming, for implementing scale invariance in neural-network-based taggers. We find that this leads to an improvement in performance across a wide range of jet transverse momenta. Our results emphasize the importance of gaining a detailed understanding of what aspects of jet physics these methods are exploiting.","Title":"Parton shower uncertainties in jet substructure analyses with deep neural networks"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"25","Author":"","Content":"Neuroimaging-based single subject prediction of brain disorders has gained increasing attention in recent years. Using a variety of neuroimaging modalities such as structural, functional and diffusion MRI, along with machine learning techniques, hundreds of studies have been carried out for accurate classification of patients with heterogeneous mental and neurodegenerative disorders such as schizophrenia and Alzheimer's disease. More than 500 studies have been published during the past quarter century on single subject prediction focused on a multiple brain disorders. In the first part of this study, we provide a survey of more than 200 reports in this field with a focus on schizophrenia, mild cognitive impairment (MCI), Alzheimer's disease (AD), depressive disorders, autism spectrum disease (ASD) and attention-deficit hyperactivity disorder (ADHD). Detailed information about those studies such as sample size, type and number of extracted features and reported accuracy are summarized and discussed. To our knowledge, this is by far the most comprehensive review of neuroimaging-based single subject prediction of brain disorders. In the second part, we present our opinion on major pitfalls of those studies from a machine learning point of view. Common biases are discussed and suggestions are provided. Moreover, emerging trends such as decentralized data sharing, multimodal brain imaging, differential diagnosis, disease subtype classification and deep learning are also discussed. Based on this survey, there is extensive evidence showing the great potential of neuroimaging data for single subject prediction of various disorders. However, the main bottleneck of this exciting field is still the limited sample size, which could be potentially addressed by modern data sharing models such as the ones discussed in this paper. Emerging big data technologies and advanced data-intensive machine learning methodologies such as deep learning have coincided with an increasing need for accurate, robust and generalizable single subject prediction of brain disorders during an exciting time. In this report, we survey the past and offer some opinions regarding the road ahead. (C) 2016 Elsevier Inc. All rights reserved.","Title":"Single subject prediction of brain disorders in neuroimaging: Promises and pitfalls"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"26","Author":"","Content":"Hydrologic connectivity has received great attention recently as our conceptual models of watersheds and water quality have evolved in the past several decades. However, the structural complexity of most temperate watersheds (i.e. connections among shallow soils, deep aquifers, the atmosphere and streams) and the dynamic seasonal changes that occur within them (i.e., plant senescence which impacts evapotranspiration) create significant challenges to characterizing or quantifying hydrologic connectivity. The McMurdo Dry Valleys, a polar desert in Antarctica, provide a unique opportunity to study hydrologic connectivity because there is no vegetative cover (and therefore no transpiration), and no deep aquifers connected to surface soils or streams. Glacier melt provides stream flow to well-established channels and closed-basin, ice-covered lakes on the valley floor. Streams are also connected to shallow hyporheic zones along their lengths, which are bounded at similar to 75 cm depth by ice-cemented permafrost. These hydrologic features and connections provide water for and underpin biological communities. Hence, exchange of water among them provides a vector for exchange of energy and dissolved solutes. Connectivity is dynamic on timescales of a day to a flow season (6-12 weeks), as streamflow varies over these timescales. The timescales over which these connections occur is also dynamic. Exchanges between streams and hyporheic zones, for example, have been estimated to be as short as hours to as long as several weeks. These exchanges have significant implications for the biogeochemistry of these systems and the biotic communities in each feature. Here we evaluate the lessons we can learn about hydrologic connectivity in the MDV watersheds that are simplified in the context of processes occurring and water reservoirs included in the landscape, yet are sensitive to climate controls and contain substantial physical heterogeneity. We specifically explore several metrics that are simple and/or commonly employed in hydrologic analyses and interpret them in the context of connectivity between and among hydrologic features. (C) 2016 Elsevier B.V. All rights reserved.","Title":"Hydrologic connectivity and implications for ecosystem processes - Lessons from naked watersheds"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"27","Author":"","Content":"Intracranial self-Stimulation (ICSS) of the medial forebrain bundle is a treatment capable of consistently facilitating acquisition of learning and memory in a wide array of experimental paradigms in rats. However, the evidence supporting this effect on implicit memory comes mainly from classical conditioning and avoidance tasks. The present work aims to determine whether ICSS would also improve the performance of rats in another type of implicit task such as cued simultaneous visual discrimination in the Morris Water Maze. The ICSS treatment was administered immediately after each of the five acquisition sessions and its effects on retention and reversal were evaluated 72 h later. Results showed that ICSS subjects committed fewer errors than Sham subjects and adopted more accurate trajectories during the acquisition of the task. This improvement was maintained until the probe test at 72 h. However, ICSS animals experienced more difficulties than the Sham group during the reversal of the same learning, reflecting an impairment in cognitive flexibility. We conclude that post-training ICSS could also be an effective treatment for improving implicit visual discrimination learning and memory. (C) 2016 Elsevier B.V. All rights reserved.","Title":"Intracranial self-stimulation also facilitates learning in a visual discrimination task in the Morris water maze in rats"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"28","Author":"","Content":"Processing realized in fuzzy rule-based models is associated with matching individual rules with the existing input data and aggregation of the levels of matching and conclusions of the rules. Rules are processed as individual entities. In this study, we introduce an augmentation of fuzzy models by facilitating interaction among the rules leading to more flexible type membership functions of fuzzy sets forming conditions of the rules (thus resulting in substantially advanced topology of the partition of the input space). Different ways of realizing interaction among the rules are studied. In the sequel, we develop a granular fuzzy model implied by the rule-based model showing how granular parameters of the original rule-based model enhance its quality expressed in terms of coverage of experimental data. The two evaluation criteria of the constructed granular model, namely coverage and specificity are studied. Experimental results are reported for a series of publicly available data. (C) 2016 Elsevier B.V. All rights reserved.","Title":"Fuzzy rule-based models with interactive rules and their granular generalization"},{"Overall":"1.0","Date":"June 17, 2005","ReviewID":"29","Author":"","Content":"The primate brain contains a hierarchy of visual areas, dubbed the ventral stream, which rapidly computes object representations that are both specific for object identity and robust against identity-preserving transformations, like depth rotations [1, 2]. Current computational models of object recognition, including recent deep-learning networks, generate these properties through a hierarchy of alternating selectivity-increasing filtering and tolerance-increasing pooling operations, similar to simple-complex cells operations [3-6]. Here, we prove that a class of hierarchical architectures and a broad set of biologically plausible learning rules generate approximate invariance to identity-preserving transformations at the top level of the processing hierarchy. However, all past models tested failed to reproduce the most salient property of an intermediate representation of a three-level face-processing hierarchy in the brain: mirror-symmetric tuning to head orientation [7]. Here, we demonstrate that one specific biologically plausible Hebb-type learning rule generates mirror-symmetric tuning to bilaterally symmetric stimuli, like faces, at intermediate levels of the architecture and show why it does so. Thus, the tuning properties of individual cells inside the visual stream appear to result from group properties of the stimuli they encode and to reflect the learning rules that sculpted the information-processing system within which they reside.","Title":"View-Tolerant Face Recognition and Hebbian Learning Imply Mirror-Symmetric Neural Tuning to Head Orientation"}],"ProductInfo":{"Name":"","Price":"","ProductID":"B00007FHEN","ImgURL":"","Features":""}}